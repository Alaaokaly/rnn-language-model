2025-04-25 23:46:49,063 - INFO - Starting training for 300 epochs
2025-04-25 23:46:49,675 - INFO - Batch: 10/141, Loss: 3.3215, Perplexity: 27.7014, Time: 0.61s
2025-04-25 23:46:50,010 - INFO - Batch: 20/141, Loss: 3.2938, Perplexity: 26.9437, Time: 0.33s
2025-04-25 23:46:50,369 - INFO - Batch: 30/141, Loss: 3.1795, Perplexity: 24.0345, Time: 0.36s
2025-04-25 23:46:50,821 - INFO - Batch: 40/141, Loss: 2.9544, Perplexity: 19.1898, Time: 0.45s
2025-04-25 23:46:51,094 - INFO - Batch: 50/141, Loss: 2.8885, Perplexity: 17.9670, Time: 0.27s
2025-04-25 23:46:51,300 - INFO - Batch: 60/141, Loss: 2.8702, Perplexity: 17.6402, Time: 0.21s
2025-04-25 23:46:51,513 - INFO - Batch: 70/141, Loss: 2.8583, Perplexity: 17.4325, Time: 0.21s
2025-04-25 23:46:51,717 - INFO - Batch: 80/141, Loss: 2.8528, Perplexity: 17.3362, Time: 0.20s
2025-04-25 23:46:51,936 - INFO - Batch: 90/141, Loss: 2.8491, Perplexity: 17.2715, Time: 0.22s
2025-04-25 23:46:52,142 - INFO - Batch: 100/141, Loss: 2.8504, Perplexity: 17.2941, Time: 0.21s
2025-04-25 23:46:52,347 - INFO - Batch: 110/141, Loss: 2.8480, Perplexity: 17.2528, Time: 0.20s
2025-04-25 23:46:52,623 - INFO - Batch: 120/141, Loss: 2.8469, Perplexity: 17.2346, Time: 0.28s
2025-04-25 23:46:52,831 - INFO - Batch: 130/141, Loss: 2.8405, Perplexity: 17.1242, Time: 0.21s
2025-04-25 23:46:53,041 - INFO - Batch: 140/141, Loss: 2.8423, Perplexity: 17.1557, Time: 0.21s
2025-04-25 23:46:53,582 - INFO - Validation  Loss: 2.8657, Perplexity: 17.5621, Time: 0.51s
2025-04-25 23:46:53,582 - INFO - Epoch 1/300 - Train Loss: 2.9630, Val Loss: 2.8657
2025-04-25 23:46:53,861 - INFO - Batch: 10/141, Loss: 2.8471, Perplexity: 17.2376, Time: 0.28s
2025-04-25 23:46:54,067 - INFO - Batch: 20/141, Loss: 2.8324, Perplexity: 16.9868, Time: 0.21s
2025-04-25 23:46:54,273 - INFO - Batch: 30/141, Loss: 2.8407, Perplexity: 17.1280, Time: 0.21s
2025-04-25 23:46:54,484 - INFO - Batch: 40/141, Loss: 2.8345, Perplexity: 17.0221, Time: 0.21s
2025-04-25 23:46:54,691 - INFO - Batch: 50/141, Loss: 2.8339, Perplexity: 17.0117, Time: 0.21s
2025-04-25 23:46:54,900 - INFO - Batch: 60/141, Loss: 2.8246, Perplexity: 16.8536, Time: 0.21s
2025-04-25 23:46:55,122 - INFO - Batch: 70/141, Loss: 2.8317, Perplexity: 16.9750, Time: 0.22s
2025-04-25 23:46:55,393 - INFO - Batch: 80/141, Loss: 2.8252, Perplexity: 16.8647, Time: 0.27s
2025-04-25 23:46:55,615 - INFO - Batch: 90/141, Loss: 2.8253, Perplexity: 16.8663, Time: 0.22s
2025-04-25 23:46:55,825 - INFO - Batch: 100/141, Loss: 2.8138, Perplexity: 16.6734, Time: 0.21s
2025-04-25 23:46:56,075 - INFO - Batch: 110/141, Loss: 2.8083, Perplexity: 16.5824, Time: 0.25s
2025-04-25 23:46:56,288 - INFO - Batch: 120/141, Loss: 2.8079, Perplexity: 16.5756, Time: 0.21s
2025-04-25 23:46:56,497 - INFO - Batch: 130/141, Loss: 2.7928, Perplexity: 16.3272, Time: 0.21s
2025-04-25 23:46:56,707 - INFO - Batch: 140/141, Loss: 2.7788, Perplexity: 16.1000, Time: 0.21s
2025-04-25 23:46:57,332 - INFO - Validation  Loss: 2.8038, Perplexity: 16.5078, Time: 0.59s
2025-04-25 23:46:57,332 - INFO - Epoch 2/300 - Train Loss: 2.8231, Val Loss: 2.8038
2025-04-25 23:46:57,555 - INFO - Batch: 10/141, Loss: 2.7706, Perplexity: 15.9688, Time: 0.22s
2025-04-25 23:46:57,764 - INFO - Batch: 20/141, Loss: 2.7467, Perplexity: 15.5914, Time: 0.21s
2025-04-25 23:46:57,973 - INFO - Batch: 30/141, Loss: 2.7276, Perplexity: 15.2955, Time: 0.21s
2025-04-25 23:46:58,177 - INFO - Batch: 40/141, Loss: 2.7122, Perplexity: 15.0631, Time: 0.20s
2025-04-25 23:46:58,445 - INFO - Batch: 50/141, Loss: 2.6901, Perplexity: 14.7332, Time: 0.27s
2025-04-25 23:46:58,654 - INFO - Batch: 60/141, Loss: 2.6696, Perplexity: 14.4346, Time: 0.21s
2025-04-25 23:46:58,861 - INFO - Batch: 70/141, Loss: 2.6392, Perplexity: 14.0022, Time: 0.21s
2025-04-25 23:46:59,070 - INFO - Batch: 80/141, Loss: 2.6160, Perplexity: 13.6807, Time: 0.21s
2025-04-25 23:46:59,282 - INFO - Batch: 90/141, Loss: 2.5988, Perplexity: 13.4478, Time: 0.21s
2025-04-25 23:46:59,484 - INFO - Batch: 100/141, Loss: 2.5720, Perplexity: 13.0922, Time: 0.20s
2025-04-25 23:46:59,696 - INFO - Batch: 110/141, Loss: 2.5502, Perplexity: 12.8101, Time: 0.21s
2025-04-25 23:46:59,906 - INFO - Batch: 120/141, Loss: 2.5234, Perplexity: 12.4713, Time: 0.21s
2025-04-25 23:47:00,184 - INFO - Batch: 130/141, Loss: 2.5040, Perplexity: 12.2315, Time: 0.28s
2025-04-25 23:47:00,394 - INFO - Batch: 140/141, Loss: 2.4780, Perplexity: 11.9176, Time: 0.21s
2025-04-25 23:47:00,918 - INFO - Validation  Loss: 2.5173, Perplexity: 12.3950, Time: 0.49s
2025-04-25 23:47:00,918 - INFO - Epoch 3/300 - Train Loss: 2.6354, Val Loss: 2.5173
2025-04-25 23:47:01,144 - INFO - Batch: 10/141, Loss: 2.4684, Perplexity: 11.8035, Time: 0.23s
2025-04-25 23:47:01,417 - INFO - Batch: 20/141, Loss: 2.4379, Perplexity: 11.4487, Time: 0.27s
2025-04-25 23:47:01,634 - INFO - Batch: 30/141, Loss: 2.4354, Perplexity: 11.4199, Time: 0.22s
2025-04-25 23:47:01,838 - INFO - Batch: 40/141, Loss: 2.4108, Perplexity: 11.1430, Time: 0.20s
2025-04-25 23:47:02,047 - INFO - Batch: 50/141, Loss: 2.4057, Perplexity: 11.0859, Time: 0.21s
2025-04-25 23:47:02,254 - INFO - Batch: 60/141, Loss: 2.3919, Perplexity: 10.9339, Time: 0.21s
2025-04-25 23:47:02,460 - INFO - Batch: 70/141, Loss: 2.3799, Perplexity: 10.8037, Time: 0.21s
2025-04-25 23:47:02,668 - INFO - Batch: 80/141, Loss: 2.3598, Perplexity: 10.5884, Time: 0.21s
2025-04-25 23:47:02,943 - INFO - Batch: 90/141, Loss: 2.3595, Perplexity: 10.5861, Time: 0.27s
2025-04-25 23:47:03,152 - INFO - Batch: 100/141, Loss: 2.3438, Perplexity: 10.4211, Time: 0.21s
2025-04-25 23:47:03,361 - INFO - Batch: 110/141, Loss: 2.3328, Perplexity: 10.3072, Time: 0.21s
2025-04-25 23:47:03,569 - INFO - Batch: 120/141, Loss: 2.3312, Perplexity: 10.2900, Time: 0.21s
2025-04-25 23:47:03,780 - INFO - Batch: 130/141, Loss: 2.3278, Perplexity: 10.2549, Time: 0.21s
2025-04-25 23:47:03,991 - INFO - Batch: 140/141, Loss: 2.3054, Perplexity: 10.0282, Time: 0.21s
2025-04-25 23:47:04,599 - INFO - Validation  Loss: 2.3602, Perplexity: 10.5934, Time: 0.58s
2025-04-25 23:47:04,600 - INFO - Epoch 4/300 - Train Loss: 2.3791, Val Loss: 2.3602
2025-04-25 23:47:04,826 - INFO - Batch: 10/141, Loss: 2.2898, Perplexity: 9.8727, Time: 0.23s
2025-04-25 23:47:05,037 - INFO - Batch: 20/141, Loss: 2.2971, Perplexity: 9.9455, Time: 0.21s
2025-04-25 23:47:05,255 - INFO - Batch: 30/141, Loss: 2.2761, Perplexity: 9.7387, Time: 0.22s
2025-04-25 23:47:05,461 - INFO - Batch: 40/141, Loss: 2.2907, Perplexity: 9.8820, Time: 0.21s
2025-04-25 23:47:05,675 - INFO - Batch: 50/141, Loss: 2.2738, Perplexity: 9.7165, Time: 0.21s
2025-04-25 23:47:05,948 - INFO - Batch: 60/141, Loss: 2.2772, Perplexity: 9.7493, Time: 0.27s
2025-04-25 23:47:06,155 - INFO - Batch: 70/141, Loss: 2.2572, Perplexity: 9.5565, Time: 0.21s
2025-04-25 23:47:06,361 - INFO - Batch: 80/141, Loss: 2.2475, Perplexity: 9.4643, Time: 0.21s
2025-04-25 23:47:06,566 - INFO - Batch: 90/141, Loss: 2.2484, Perplexity: 9.4725, Time: 0.21s
2025-04-25 23:47:06,775 - INFO - Batch: 100/141, Loss: 2.2266, Perplexity: 9.2680, Time: 0.21s
2025-04-25 23:47:06,985 - INFO - Batch: 110/141, Loss: 2.2176, Perplexity: 9.1855, Time: 0.21s
2025-04-25 23:47:07,197 - INFO - Batch: 120/141, Loss: 2.2243, Perplexity: 9.2474, Time: 0.21s
2025-04-25 23:47:07,401 - INFO - Batch: 130/141, Loss: 2.2263, Perplexity: 9.2651, Time: 0.20s
2025-04-25 23:47:07,677 - INFO - Batch: 140/141, Loss: 2.2230, Perplexity: 9.2349, Time: 0.28s
2025-04-25 23:47:08,217 - INFO - Validation  Loss: 2.2761, Perplexity: 9.7388, Time: 0.51s
2025-04-25 23:47:08,217 - INFO - Epoch 5/300 - Train Loss: 2.2555, Val Loss: 2.2761
2025-04-25 23:47:08,427 - INFO - Batch: 10/141, Loss: 2.2080, Perplexity: 9.0977, Time: 0.21s
2025-04-25 23:47:08,635 - INFO - Batch: 20/141, Loss: 2.1989, Perplexity: 9.0150, Time: 0.21s
2025-04-25 23:47:08,907 - INFO - Batch: 30/141, Loss: 2.1976, Perplexity: 9.0031, Time: 0.27s
2025-04-25 23:47:09,116 - INFO - Batch: 40/141, Loss: 2.1839, Perplexity: 8.8806, Time: 0.21s
2025-04-25 23:47:09,335 - INFO - Batch: 50/141, Loss: 2.1919, Perplexity: 8.9526, Time: 0.22s
2025-04-25 23:47:09,545 - INFO - Batch: 60/141, Loss: 2.1879, Perplexity: 8.9164, Time: 0.21s
2025-04-25 23:47:09,755 - INFO - Batch: 70/141, Loss: 2.1725, Perplexity: 8.7799, Time: 0.21s
2025-04-25 23:47:09,965 - INFO - Batch: 80/141, Loss: 2.1756, Perplexity: 8.8073, Time: 0.21s
2025-04-25 23:47:10,175 - INFO - Batch: 90/141, Loss: 2.1677, Perplexity: 8.7384, Time: 0.21s
2025-04-25 23:47:10,438 - INFO - Batch: 100/141, Loss: 2.1671, Perplexity: 8.7326, Time: 0.26s
2025-04-25 23:47:10,699 - INFO - Batch: 110/141, Loss: 2.1472, Perplexity: 8.5608, Time: 0.26s
2025-04-25 23:47:10,911 - INFO - Batch: 120/141, Loss: 2.1494, Perplexity: 8.5794, Time: 0.21s
2025-04-25 23:47:11,163 - INFO - Batch: 130/141, Loss: 2.1440, Perplexity: 8.5338, Time: 0.25s
2025-04-25 23:47:11,374 - INFO - Batch: 140/141, Loss: 2.1374, Perplexity: 8.4776, Time: 0.21s
2025-04-25 23:47:12,081 - INFO - Validation  Loss: 2.2106, Perplexity: 9.1214, Time: 0.68s
2025-04-25 23:47:12,081 - INFO - Epoch 6/300 - Train Loss: 2.1755, Val Loss: 2.2106
2025-04-25 23:47:12,302 - INFO - Batch: 10/141, Loss: 2.1386, Perplexity: 8.4871, Time: 0.22s
2025-04-25 23:47:12,522 - INFO - Batch: 20/141, Loss: 2.1331, Perplexity: 8.4410, Time: 0.22s
2025-04-25 23:47:12,777 - INFO - Batch: 30/141, Loss: 2.1210, Perplexity: 8.3394, Time: 0.25s
2025-04-25 23:47:13,012 - INFO - Batch: 40/141, Loss: 2.1127, Perplexity: 8.2709, Time: 0.23s
2025-04-25 23:47:13,240 - INFO - Batch: 50/141, Loss: 2.1071, Perplexity: 8.2247, Time: 0.23s
2025-04-25 23:47:13,454 - INFO - Batch: 60/141, Loss: 2.1084, Perplexity: 8.2354, Time: 0.21s
2025-04-25 23:47:13,727 - INFO - Batch: 70/141, Loss: 2.1063, Perplexity: 8.2177, Time: 0.27s
2025-04-25 23:47:13,948 - INFO - Batch: 80/141, Loss: 2.0958, Perplexity: 8.1322, Time: 0.22s
2025-04-25 23:47:14,198 - INFO - Batch: 90/141, Loss: 2.0889, Perplexity: 8.0762, Time: 0.25s
2025-04-25 23:47:14,479 - INFO - Batch: 100/141, Loss: 2.1038, Perplexity: 8.1976, Time: 0.28s
2025-04-25 23:47:14,751 - INFO - Batch: 110/141, Loss: 2.0931, Perplexity: 8.1098, Time: 0.27s
2025-04-25 23:47:15,017 - INFO - Batch: 120/141, Loss: 2.0853, Perplexity: 8.0472, Time: 0.27s
2025-04-25 23:47:15,281 - INFO - Batch: 130/141, Loss: 2.0797, Perplexity: 8.0022, Time: 0.26s
2025-04-25 23:47:15,591 - INFO - Batch: 140/141, Loss: 2.0804, Perplexity: 8.0074, Time: 0.31s
2025-04-25 23:47:16,255 - INFO - Validation  Loss: 2.1512, Perplexity: 8.5950, Time: 0.57s
2025-04-25 23:47:16,256 - INFO - Epoch 7/300 - Train Loss: 2.1081, Val Loss: 2.1512
2025-04-25 23:47:16,479 - INFO - Batch: 10/141, Loss: 2.0790, Perplexity: 7.9961, Time: 0.22s
2025-04-25 23:47:16,713 - INFO - Batch: 20/141, Loss: 2.0632, Perplexity: 7.8708, Time: 0.23s
2025-04-25 23:47:17,032 - INFO - Batch: 30/141, Loss: 2.0803, Perplexity: 8.0069, Time: 0.32s
2025-04-25 23:47:17,333 - INFO - Batch: 40/141, Loss: 2.0709, Perplexity: 7.9317, Time: 0.30s
2025-04-25 23:47:17,606 - INFO - Batch: 50/141, Loss: 2.0665, Perplexity: 7.8968, Time: 0.27s
2025-04-25 23:47:17,821 - INFO - Batch: 60/141, Loss: 2.0721, Perplexity: 7.9412, Time: 0.21s
2025-04-25 23:47:18,031 - INFO - Batch: 70/141, Loss: 2.0562, Perplexity: 7.8159, Time: 0.21s
2025-04-25 23:47:18,239 - INFO - Batch: 80/141, Loss: 2.0526, Perplexity: 7.7879, Time: 0.21s
2025-04-25 23:47:18,438 - INFO - Batch: 90/141, Loss: 2.0509, Perplexity: 7.7746, Time: 0.20s
2025-04-25 23:47:18,654 - INFO - Batch: 100/141, Loss: 2.0531, Perplexity: 7.7923, Time: 0.22s
2025-04-25 23:47:18,922 - INFO - Batch: 110/141, Loss: 2.0304, Perplexity: 7.6174, Time: 0.27s
2025-04-25 23:47:19,126 - INFO - Batch: 120/141, Loss: 2.0474, Perplexity: 7.7481, Time: 0.20s
2025-04-25 23:47:19,368 - INFO - Batch: 130/141, Loss: 2.0444, Perplexity: 7.7247, Time: 0.24s
2025-04-25 23:47:19,586 - INFO - Batch: 140/141, Loss: 2.0247, Perplexity: 7.5741, Time: 0.22s
2025-04-25 23:47:20,192 - INFO - Validation  Loss: 2.1065, Perplexity: 8.2194, Time: 0.58s
2025-04-25 23:47:20,192 - INFO - Epoch 8/300 - Train Loss: 2.0543, Val Loss: 2.1065
2025-04-25 23:47:20,405 - INFO - Batch: 10/141, Loss: 2.0218, Perplexity: 7.5519, Time: 0.21s
2025-04-25 23:47:20,607 - INFO - Batch: 20/141, Loss: 2.0257, Perplexity: 7.5816, Time: 0.20s
2025-04-25 23:47:20,812 - INFO - Batch: 30/141, Loss: 2.0193, Perplexity: 7.5331, Time: 0.20s
2025-04-25 23:47:21,031 - INFO - Batch: 40/141, Loss: 2.0107, Perplexity: 7.4683, Time: 0.22s
2025-04-25 23:47:21,246 - INFO - Batch: 50/141, Loss: 2.0132, Perplexity: 7.4874, Time: 0.21s
2025-04-25 23:47:21,451 - INFO - Batch: 60/141, Loss: 2.0143, Perplexity: 7.4954, Time: 0.21s
2025-04-25 23:47:21,656 - INFO - Batch: 70/141, Loss: 2.0146, Perplexity: 7.4979, Time: 0.20s
2025-04-25 23:47:21,917 - INFO - Batch: 80/141, Loss: 2.0096, Perplexity: 7.4603, Time: 0.26s
2025-04-25 23:47:22,113 - INFO - Batch: 90/141, Loss: 2.0046, Perplexity: 7.4234, Time: 0.20s
2025-04-25 23:47:22,310 - INFO - Batch: 100/141, Loss: 2.0037, Perplexity: 7.4162, Time: 0.20s
2025-04-25 23:47:22,508 - INFO - Batch: 110/141, Loss: 2.0129, Perplexity: 7.4853, Time: 0.20s
2025-04-25 23:47:22,712 - INFO - Batch: 120/141, Loss: 1.9966, Perplexity: 7.3639, Time: 0.20s
2025-04-25 23:47:22,912 - INFO - Batch: 130/141, Loss: 1.9879, Perplexity: 7.3001, Time: 0.20s
2025-04-25 23:47:23,118 - INFO - Batch: 140/141, Loss: 2.0043, Perplexity: 7.4206, Time: 0.21s
2025-04-25 23:47:23,738 - INFO - Validation  Loss: 2.0741, Perplexity: 7.9570, Time: 0.59s
2025-04-25 23:47:23,738 - INFO - Epoch 9/300 - Train Loss: 2.0134, Val Loss: 2.0741
2025-04-25 23:47:23,948 - INFO - Batch: 10/141, Loss: 2.0004, Perplexity: 7.3918, Time: 0.21s
2025-04-25 23:47:24,174 - INFO - Batch: 20/141, Loss: 1.9913, Perplexity: 7.3249, Time: 0.23s
2025-04-25 23:47:24,388 - INFO - Batch: 30/141, Loss: 1.9939, Perplexity: 7.3442, Time: 0.21s
2025-04-25 23:47:24,664 - INFO - Batch: 40/141, Loss: 1.9990, Perplexity: 7.3817, Time: 0.28s
2025-04-25 23:47:24,868 - INFO - Batch: 50/141, Loss: 1.9906, Perplexity: 7.3200, Time: 0.20s
2025-04-25 23:47:25,073 - INFO - Batch: 60/141, Loss: 1.9760, Perplexity: 7.2137, Time: 0.20s
2025-04-25 23:47:25,286 - INFO - Batch: 70/141, Loss: 1.9745, Perplexity: 7.2031, Time: 0.21s
2025-04-25 23:47:25,504 - INFO - Batch: 80/141, Loss: 1.9808, Perplexity: 7.2485, Time: 0.22s
2025-04-25 23:47:25,745 - INFO - Batch: 90/141, Loss: 1.9900, Perplexity: 7.3155, Time: 0.24s
2025-04-25 23:47:25,953 - INFO - Batch: 100/141, Loss: 1.9681, Perplexity: 7.1568, Time: 0.21s
2025-04-25 23:47:26,159 - INFO - Batch: 110/141, Loss: 1.9722, Perplexity: 7.1864, Time: 0.21s
2025-04-25 23:47:26,425 - INFO - Batch: 120/141, Loss: 1.9683, Perplexity: 7.1583, Time: 0.27s
2025-04-25 23:47:26,628 - INFO - Batch: 130/141, Loss: 1.9535, Perplexity: 7.0532, Time: 0.20s
2025-04-25 23:47:26,834 - INFO - Batch: 140/141, Loss: 1.9669, Perplexity: 7.1487, Time: 0.21s
2025-04-25 23:47:27,376 - INFO - Validation  Loss: 2.0482, Perplexity: 7.7540, Time: 0.52s
2025-04-25 23:47:27,376 - INFO - Epoch 10/300 - Train Loss: 1.9803, Val Loss: 2.0482
2025-04-25 23:47:27,664 - INFO - Batch: 10/141, Loss: 1.9523, Perplexity: 7.0450, Time: 0.29s
2025-04-25 23:47:27,891 - INFO - Batch: 20/141, Loss: 1.9665, Perplexity: 7.1455, Time: 0.23s
2025-04-25 23:47:28,111 - INFO - Batch: 30/141, Loss: 1.9674, Perplexity: 7.1520, Time: 0.22s
2025-04-25 23:47:28,314 - INFO - Batch: 40/141, Loss: 1.9621, Perplexity: 7.1139, Time: 0.20s
2025-04-25 23:47:28,517 - INFO - Batch: 50/141, Loss: 1.9602, Perplexity: 7.1007, Time: 0.20s
2025-04-25 23:47:28,785 - INFO - Batch: 60/141, Loss: 1.9630, Perplexity: 7.1208, Time: 0.27s
2025-04-25 23:47:29,032 - INFO - Batch: 70/141, Loss: 1.9460, Perplexity: 7.0009, Time: 0.25s
2025-04-25 23:47:29,256 - INFO - Batch: 80/141, Loss: 1.9453, Perplexity: 6.9957, Time: 0.22s
2025-04-25 23:47:29,528 - INFO - Batch: 90/141, Loss: 1.9397, Perplexity: 6.9565, Time: 0.27s
2025-04-25 23:47:29,781 - INFO - Batch: 100/141, Loss: 1.9522, Perplexity: 7.0444, Time: 0.25s
2025-04-25 23:47:30,036 - INFO - Batch: 110/141, Loss: 1.9459, Perplexity: 7.0002, Time: 0.26s
2025-04-25 23:47:30,250 - INFO - Batch: 120/141, Loss: 1.9443, Perplexity: 6.9885, Time: 0.21s
2025-04-25 23:47:30,455 - INFO - Batch: 130/141, Loss: 1.9479, Perplexity: 7.0140, Time: 0.20s
2025-04-25 23:47:30,662 - INFO - Batch: 140/141, Loss: 1.9477, Perplexity: 7.0122, Time: 0.21s
2025-04-25 23:47:31,290 - INFO - Validation  Loss: 2.0268, Perplexity: 7.5897, Time: 0.60s
2025-04-25 23:47:31,290 - INFO - Epoch 11/300 - Train Loss: 1.9523, Val Loss: 2.0268
2025-04-25 23:47:31,497 - INFO - Batch: 10/141, Loss: 1.9373, Perplexity: 6.9400, Time: 0.21s
2025-04-25 23:47:31,707 - INFO - Batch: 20/141, Loss: 1.9523, Perplexity: 7.0447, Time: 0.21s
2025-04-25 23:47:31,909 - INFO - Batch: 30/141, Loss: 1.9408, Perplexity: 6.9641, Time: 0.20s
2025-04-25 23:47:32,117 - INFO - Batch: 40/141, Loss: 1.9385, Perplexity: 6.9485, Time: 0.21s
2025-04-25 23:47:32,381 - INFO - Batch: 50/141, Loss: 1.9305, Perplexity: 6.8931, Time: 0.26s
2025-04-25 23:47:32,586 - INFO - Batch: 60/141, Loss: 1.9105, Perplexity: 6.7561, Time: 0.20s
2025-04-25 23:47:32,794 - INFO - Batch: 70/141, Loss: 1.9211, Perplexity: 6.8283, Time: 0.21s
2025-04-25 23:47:33,016 - INFO - Batch: 80/141, Loss: 1.9203, Perplexity: 6.8232, Time: 0.22s
2025-04-25 23:47:33,280 - INFO - Batch: 90/141, Loss: 1.9256, Perplexity: 6.8589, Time: 0.26s
2025-04-25 23:47:33,492 - INFO - Batch: 100/141, Loss: 1.9166, Perplexity: 6.7979, Time: 0.21s
2025-04-25 23:47:33,702 - INFO - Batch: 110/141, Loss: 1.9297, Perplexity: 6.8873, Time: 0.21s
2025-04-25 23:47:33,905 - INFO - Batch: 120/141, Loss: 1.9147, Perplexity: 6.7852, Time: 0.20s
2025-04-25 23:47:34,204 - INFO - Batch: 130/141, Loss: 1.9207, Perplexity: 6.8257, Time: 0.30s
2025-04-25 23:47:34,407 - INFO - Batch: 140/141, Loss: 1.9117, Perplexity: 6.7645, Time: 0.20s
2025-04-25 23:47:35,003 - INFO - Validation  Loss: 2.0097, Perplexity: 7.4612, Time: 0.57s
2025-04-25 23:47:35,004 - INFO - Epoch 12/300 - Train Loss: 1.9284, Val Loss: 2.0097
2025-04-25 23:47:35,219 - INFO - Batch: 10/141, Loss: 1.9204, Perplexity: 6.8240, Time: 0.22s
2025-04-25 23:47:35,487 - INFO - Batch: 20/141, Loss: 1.9200, Perplexity: 6.8210, Time: 0.27s
2025-04-25 23:47:35,711 - INFO - Batch: 30/141, Loss: 1.9099, Perplexity: 6.7526, Time: 0.22s
2025-04-25 23:47:35,919 - INFO - Batch: 40/141, Loss: 1.9242, Perplexity: 6.8495, Time: 0.21s
2025-04-25 23:47:36,126 - INFO - Batch: 50/141, Loss: 1.9019, Perplexity: 6.6986, Time: 0.21s
2025-04-25 23:47:36,328 - INFO - Batch: 60/141, Loss: 1.9107, Perplexity: 6.7581, Time: 0.20s
2025-04-25 23:47:36,547 - INFO - Batch: 70/141, Loss: 1.9120, Perplexity: 6.7667, Time: 0.22s
2025-04-25 23:47:36,781 - INFO - Batch: 80/141, Loss: 1.9017, Perplexity: 6.6972, Time: 0.23s
2025-04-25 23:47:36,989 - INFO - Batch: 90/141, Loss: 1.9086, Perplexity: 6.7435, Time: 0.21s
2025-04-25 23:47:37,249 - INFO - Batch: 100/141, Loss: 1.9099, Perplexity: 6.7525, Time: 0.26s
2025-04-25 23:47:37,462 - INFO - Batch: 110/141, Loss: 1.9070, Perplexity: 6.7330, Time: 0.21s
2025-04-25 23:47:37,721 - INFO - Batch: 120/141, Loss: 1.8914, Perplexity: 6.6288, Time: 0.26s
2025-04-25 23:47:37,977 - INFO - Batch: 130/141, Loss: 1.9001, Perplexity: 6.6863, Time: 0.26s
2025-04-25 23:47:38,230 - INFO - Batch: 140/141, Loss: 1.8971, Perplexity: 6.6662, Time: 0.25s
2025-04-25 23:47:38,939 - INFO - Validation  Loss: 1.9973, Perplexity: 7.3695, Time: 0.68s
2025-04-25 23:47:38,939 - INFO - Epoch 13/300 - Train Loss: 1.9085, Val Loss: 1.9973
2025-04-25 23:47:39,159 - INFO - Batch: 10/141, Loss: 1.9028, Perplexity: 6.7049, Time: 0.22s
2025-04-25 23:47:39,407 - INFO - Batch: 20/141, Loss: 1.9168, Perplexity: 6.7989, Time: 0.25s
2025-04-25 23:47:39,636 - INFO - Batch: 30/141, Loss: 1.9023, Perplexity: 6.7014, Time: 0.23s
2025-04-25 23:47:39,871 - INFO - Batch: 40/141, Loss: 1.8884, Perplexity: 6.6085, Time: 0.23s
2025-04-25 23:47:40,146 - INFO - Batch: 50/141, Loss: 1.8960, Perplexity: 6.6590, Time: 0.27s
2025-04-25 23:47:40,465 - INFO - Batch: 60/141, Loss: 1.8943, Perplexity: 6.6480, Time: 0.32s
2025-04-25 23:47:40,703 - INFO - Batch: 70/141, Loss: 1.9021, Perplexity: 6.6997, Time: 0.24s
2025-04-25 23:47:40,938 - INFO - Batch: 80/141, Loss: 1.8924, Perplexity: 6.6355, Time: 0.23s
2025-04-25 23:47:41,180 - INFO - Batch: 90/141, Loss: 1.9019, Perplexity: 6.6988, Time: 0.24s
2025-04-25 23:47:41,399 - INFO - Batch: 100/141, Loss: 1.8792, Perplexity: 6.5484, Time: 0.22s
2025-04-25 23:47:41,622 - INFO - Batch: 110/141, Loss: 1.8803, Perplexity: 6.5557, Time: 0.22s
2025-04-25 23:47:41,879 - INFO - Batch: 120/141, Loss: 1.8879, Perplexity: 6.6053, Time: 0.26s
2025-04-25 23:47:42,094 - INFO - Batch: 130/141, Loss: 1.8750, Perplexity: 6.5207, Time: 0.22s
2025-04-25 23:47:42,405 - INFO - Batch: 140/141, Loss: 1.8927, Perplexity: 6.6375, Time: 0.31s
2025-04-25 23:47:43,013 - INFO - Validation  Loss: 1.9861, Perplexity: 7.2871, Time: 0.58s
2025-04-25 23:47:43,013 - INFO - Epoch 14/300 - Train Loss: 1.8917, Val Loss: 1.9861
2025-04-25 23:47:43,224 - INFO - Batch: 10/141, Loss: 1.8761, Perplexity: 6.5279, Time: 0.21s
2025-04-25 23:47:43,436 - INFO - Batch: 20/141, Loss: 1.8845, Perplexity: 6.5831, Time: 0.21s
2025-04-25 23:47:43,715 - INFO - Batch: 30/141, Loss: 1.8761, Perplexity: 6.5283, Time: 0.28s
2025-04-25 23:47:43,925 - INFO - Batch: 40/141, Loss: 1.8651, Perplexity: 6.4566, Time: 0.21s
2025-04-25 23:47:44,139 - INFO - Batch: 50/141, Loss: 1.8607, Perplexity: 6.4285, Time: 0.21s
2025-04-25 23:47:44,355 - INFO - Batch: 60/141, Loss: 1.8716, Perplexity: 6.4984, Time: 0.22s
2025-04-25 23:47:44,569 - INFO - Batch: 70/141, Loss: 1.8866, Perplexity: 6.5969, Time: 0.21s
2025-04-25 23:47:44,788 - INFO - Batch: 80/141, Loss: 1.8928, Perplexity: 6.6377, Time: 0.22s
2025-04-25 23:47:45,004 - INFO - Batch: 90/141, Loss: 1.8777, Perplexity: 6.5386, Time: 0.22s
2025-04-25 23:47:45,316 - INFO - Batch: 100/141, Loss: 1.8571, Perplexity: 6.4051, Time: 0.31s
2025-04-25 23:47:45,523 - INFO - Batch: 110/141, Loss: 1.8732, Perplexity: 6.5094, Time: 0.21s
2025-04-25 23:47:45,749 - INFO - Batch: 120/141, Loss: 1.8777, Perplexity: 6.5383, Time: 0.23s
2025-04-25 23:47:45,981 - INFO - Batch: 130/141, Loss: 1.8707, Perplexity: 6.4925, Time: 0.23s
2025-04-25 23:47:46,203 - INFO - Batch: 140/141, Loss: 1.8752, Perplexity: 6.5221, Time: 0.22s
2025-04-25 23:47:46,849 - INFO - Validation  Loss: 1.9749, Perplexity: 7.2058, Time: 0.62s
2025-04-25 23:47:46,849 - INFO - Epoch 15/300 - Train Loss: 1.8773, Val Loss: 1.9749
2025-04-25 23:47:47,060 - INFO - Batch: 10/141, Loss: 1.8782, Perplexity: 6.5417, Time: 0.21s
2025-04-25 23:47:47,272 - INFO - Batch: 20/141, Loss: 1.8833, Perplexity: 6.5753, Time: 0.21s
2025-04-25 23:47:47,484 - INFO - Batch: 30/141, Loss: 1.8735, Perplexity: 6.5109, Time: 0.21s
2025-04-25 23:47:47,701 - INFO - Batch: 40/141, Loss: 1.8768, Perplexity: 6.5328, Time: 0.22s
2025-04-25 23:47:47,918 - INFO - Batch: 50/141, Loss: 1.8819, Perplexity: 6.5661, Time: 0.22s
2025-04-25 23:47:48,132 - INFO - Batch: 60/141, Loss: 1.8633, Perplexity: 6.4453, Time: 0.21s
2025-04-25 23:47:48,407 - INFO - Batch: 70/141, Loss: 1.8630, Perplexity: 6.4433, Time: 0.28s
2025-04-25 23:47:48,618 - INFO - Batch: 80/141, Loss: 1.8612, Perplexity: 6.4315, Time: 0.21s
2025-04-25 23:47:48,835 - INFO - Batch: 90/141, Loss: 1.8416, Perplexity: 6.3069, Time: 0.22s
2025-04-25 23:47:49,053 - INFO - Batch: 100/141, Loss: 1.8752, Perplexity: 6.5221, Time: 0.22s
2025-04-25 23:47:49,263 - INFO - Batch: 110/141, Loss: 1.8742, Perplexity: 6.5158, Time: 0.21s
2025-04-25 23:47:49,478 - INFO - Batch: 120/141, Loss: 1.8589, Perplexity: 6.4169, Time: 0.21s
2025-04-25 23:47:49,690 - INFO - Batch: 130/141, Loss: 1.8559, Perplexity: 6.3973, Time: 0.21s
2025-04-25 23:47:49,907 - INFO - Batch: 140/141, Loss: 1.8619, Perplexity: 6.4363, Time: 0.22s
2025-04-25 23:47:50,528 - INFO - Validation  Loss: 1.9659, Perplexity: 7.1413, Time: 0.60s
2025-04-25 23:47:50,528 - INFO - Epoch 16/300 - Train Loss: 1.8647, Val Loss: 1.9659
2025-04-25 23:47:50,744 - INFO - Batch: 10/141, Loss: 1.8531, Perplexity: 6.3794, Time: 0.22s
2025-04-25 23:47:50,955 - INFO - Batch: 20/141, Loss: 1.8868, Perplexity: 6.5983, Time: 0.21s
2025-04-25 23:47:51,167 - INFO - Batch: 30/141, Loss: 1.8620, Perplexity: 6.4365, Time: 0.21s
2025-04-25 23:47:51,435 - INFO - Batch: 40/141, Loss: 1.8589, Perplexity: 6.4166, Time: 0.27s
2025-04-25 23:47:51,645 - INFO - Batch: 50/141, Loss: 1.8531, Perplexity: 6.3793, Time: 0.21s
2025-04-25 23:47:51,865 - INFO - Batch: 60/141, Loss: 1.8457, Perplexity: 6.3323, Time: 0.22s
2025-04-25 23:47:52,077 - INFO - Batch: 70/141, Loss: 1.8570, Perplexity: 6.4044, Time: 0.21s
2025-04-25 23:47:52,295 - INFO - Batch: 80/141, Loss: 1.8595, Perplexity: 6.4206, Time: 0.22s
2025-04-25 23:47:52,504 - INFO - Batch: 90/141, Loss: 1.8480, Perplexity: 6.3471, Time: 0.21s
2025-04-25 23:47:52,715 - INFO - Batch: 100/141, Loss: 1.8508, Perplexity: 6.3650, Time: 0.21s
2025-04-25 23:47:52,993 - INFO - Batch: 110/141, Loss: 1.8508, Perplexity: 6.3648, Time: 0.28s
2025-04-25 23:47:53,200 - INFO - Batch: 120/141, Loss: 1.8501, Perplexity: 6.3605, Time: 0.21s
2025-04-25 23:47:53,406 - INFO - Batch: 130/141, Loss: 1.8526, Perplexity: 6.3766, Time: 0.21s
2025-04-25 23:47:53,613 - INFO - Batch: 140/141, Loss: 1.8467, Perplexity: 6.3387, Time: 0.21s
2025-04-25 23:47:54,238 - INFO - Validation  Loss: 1.9581, Perplexity: 7.0858, Time: 0.60s
2025-04-25 23:47:54,238 - INFO - Epoch 17/300 - Train Loss: 1.8536, Val Loss: 1.9581
2025-04-25 23:47:54,449 - INFO - Batch: 10/141, Loss: 1.8421, Perplexity: 6.3101, Time: 0.21s
2025-04-25 23:47:54,656 - INFO - Batch: 20/141, Loss: 1.8494, Perplexity: 6.3560, Time: 0.21s
2025-04-25 23:47:54,872 - INFO - Batch: 30/141, Loss: 1.8655, Perplexity: 6.4591, Time: 0.22s
2025-04-25 23:47:55,092 - INFO - Batch: 40/141, Loss: 1.8489, Perplexity: 6.3529, Time: 0.22s
2025-04-25 23:47:55,307 - INFO - Batch: 50/141, Loss: 1.8580, Perplexity: 6.4107, Time: 0.22s
2025-04-25 23:47:55,522 - INFO - Batch: 60/141, Loss: 1.8504, Perplexity: 6.3623, Time: 0.21s
2025-04-25 23:47:55,737 - INFO - Batch: 70/141, Loss: 1.8401, Perplexity: 6.2972, Time: 0.21s
2025-04-25 23:47:56,018 - INFO - Batch: 80/141, Loss: 1.8505, Perplexity: 6.3629, Time: 0.28s
2025-04-25 23:47:56,227 - INFO - Batch: 90/141, Loss: 1.8386, Perplexity: 6.2878, Time: 0.21s
2025-04-25 23:47:56,446 - INFO - Batch: 100/141, Loss: 1.8311, Perplexity: 6.2406, Time: 0.22s
2025-04-25 23:47:56,663 - INFO - Batch: 110/141, Loss: 1.8525, Perplexity: 6.3759, Time: 0.22s
2025-04-25 23:47:56,916 - INFO - Batch: 120/141, Loss: 1.8429, Perplexity: 6.3150, Time: 0.25s
2025-04-25 23:47:57,135 - INFO - Batch: 130/141, Loss: 1.8315, Perplexity: 6.2435, Time: 0.22s
2025-04-25 23:47:57,352 - INFO - Batch: 140/141, Loss: 1.8450, Perplexity: 6.3283, Time: 0.22s
2025-04-25 23:47:58,007 - INFO - Validation  Loss: 1.9530, Perplexity: 7.0497, Time: 0.63s
2025-04-25 23:47:58,007 - INFO - Epoch 18/300 - Train Loss: 1.8436, Val Loss: 1.9530
2025-04-25 23:47:58,217 - INFO - Batch: 10/141, Loss: 1.8419, Perplexity: 6.3087, Time: 0.21s
2025-04-25 23:47:58,434 - INFO - Batch: 20/141, Loss: 1.8316, Perplexity: 6.2441, Time: 0.22s
2025-04-25 23:47:58,647 - INFO - Batch: 30/141, Loss: 1.8174, Perplexity: 6.1559, Time: 0.21s
2025-04-25 23:47:58,868 - INFO - Batch: 40/141, Loss: 1.8486, Perplexity: 6.3507, Time: 0.22s
2025-04-25 23:47:59,132 - INFO - Batch: 50/141, Loss: 1.8377, Perplexity: 6.2822, Time: 0.26s
2025-04-25 23:47:59,345 - INFO - Batch: 60/141, Loss: 1.8431, Perplexity: 6.3160, Time: 0.21s
2025-04-25 23:47:59,561 - INFO - Batch: 70/141, Loss: 1.8409, Perplexity: 6.3024, Time: 0.22s
2025-04-25 23:47:59,777 - INFO - Batch: 80/141, Loss: 1.8318, Perplexity: 6.2454, Time: 0.22s
2025-04-25 23:47:59,995 - INFO - Batch: 90/141, Loss: 1.8300, Perplexity: 6.2336, Time: 0.22s
2025-04-25 23:48:00,218 - INFO - Batch: 100/141, Loss: 1.8458, Perplexity: 6.3329, Time: 0.22s
2025-04-25 23:48:00,435 - INFO - Batch: 110/141, Loss: 1.8329, Perplexity: 6.2522, Time: 0.22s
2025-04-25 23:48:00,707 - INFO - Batch: 120/141, Loss: 1.8176, Perplexity: 6.1571, Time: 0.27s
2025-04-25 23:48:00,914 - INFO - Batch: 130/141, Loss: 1.8243, Perplexity: 6.1985, Time: 0.21s
2025-04-25 23:48:01,125 - INFO - Batch: 140/141, Loss: 1.8354, Perplexity: 6.2675, Time: 0.21s
2025-04-25 23:48:01,680 - INFO - Validation  Loss: 1.9482, Perplexity: 7.0162, Time: 0.53s
2025-04-25 23:48:01,680 - INFO - Epoch 19/300 - Train Loss: 1.8348, Val Loss: 1.9482
2025-04-25 23:48:01,950 - INFO - Batch: 10/141, Loss: 1.8294, Perplexity: 6.2303, Time: 0.27s
2025-04-25 23:48:02,158 - INFO - Batch: 20/141, Loss: 1.8210, Perplexity: 6.1780, Time: 0.21s
2025-04-25 23:48:02,368 - INFO - Batch: 30/141, Loss: 1.8203, Perplexity: 6.1736, Time: 0.21s
2025-04-25 23:48:02,584 - INFO - Batch: 40/141, Loss: 1.8298, Perplexity: 6.2325, Time: 0.22s
2025-04-25 23:48:02,801 - INFO - Batch: 50/141, Loss: 1.8306, Perplexity: 6.2379, Time: 0.22s
2025-04-25 23:48:03,016 - INFO - Batch: 60/141, Loss: 1.8279, Perplexity: 6.2209, Time: 0.21s
2025-04-25 23:48:03,230 - INFO - Batch: 70/141, Loss: 1.8179, Perplexity: 6.1592, Time: 0.21s
2025-04-25 23:48:03,442 - INFO - Batch: 80/141, Loss: 1.8229, Perplexity: 6.1896, Time: 0.21s
2025-04-25 23:48:03,713 - INFO - Batch: 90/141, Loss: 1.8216, Perplexity: 6.1820, Time: 0.27s
2025-04-25 23:48:03,926 - INFO - Batch: 100/141, Loss: 1.8320, Perplexity: 6.2464, Time: 0.21s
2025-04-25 23:48:04,143 - INFO - Batch: 110/141, Loss: 1.8238, Perplexity: 6.1951, Time: 0.22s
2025-04-25 23:48:04,355 - INFO - Batch: 120/141, Loss: 1.8352, Perplexity: 6.2662, Time: 0.21s
2025-04-25 23:48:04,571 - INFO - Batch: 130/141, Loss: 1.8243, Perplexity: 6.1986, Time: 0.22s
2025-04-25 23:48:04,795 - INFO - Batch: 140/141, Loss: 1.8209, Perplexity: 6.1772, Time: 0.22s
2025-04-25 23:48:05,421 - INFO - Validation  Loss: 1.9441, Perplexity: 6.9874, Time: 0.60s
2025-04-25 23:48:05,421 - INFO - Epoch 20/300 - Train Loss: 1.8268, Val Loss: 1.9441
2025-04-25 23:48:05,631 - INFO - Batch: 10/141, Loss: 1.8097, Perplexity: 6.1085, Time: 0.21s
2025-04-25 23:48:05,843 - INFO - Batch: 20/141, Loss: 1.8156, Perplexity: 6.1449, Time: 0.21s
2025-04-25 23:48:06,055 - INFO - Batch: 30/141, Loss: 1.8229, Perplexity: 6.1900, Time: 0.21s
2025-04-25 23:48:06,271 - INFO - Batch: 40/141, Loss: 1.8212, Perplexity: 6.1795, Time: 0.22s
2025-04-25 23:48:06,554 - INFO - Batch: 50/141, Loss: 1.8214, Perplexity: 6.1804, Time: 0.28s
2025-04-25 23:48:06,761 - INFO - Batch: 60/141, Loss: 1.8391, Perplexity: 6.2909, Time: 0.21s
2025-04-25 23:48:06,983 - INFO - Batch: 70/141, Loss: 1.8141, Perplexity: 6.1353, Time: 0.22s
2025-04-25 23:48:07,197 - INFO - Batch: 80/141, Loss: 1.8125, Perplexity: 6.1254, Time: 0.21s
2025-04-25 23:48:07,412 - INFO - Batch: 90/141, Loss: 1.8270, Perplexity: 6.2154, Time: 0.21s
2025-04-25 23:48:07,624 - INFO - Batch: 100/141, Loss: 1.8166, Perplexity: 6.1511, Time: 0.21s
2025-04-25 23:48:07,843 - INFO - Batch: 110/141, Loss: 1.8160, Perplexity: 6.1472, Time: 0.22s
2025-04-25 23:48:08,054 - INFO - Batch: 120/141, Loss: 1.8061, Perplexity: 6.0868, Time: 0.21s
2025-04-25 23:48:08,323 - INFO - Batch: 130/141, Loss: 1.8094, Perplexity: 6.1068, Time: 0.27s
2025-04-25 23:48:08,535 - INFO - Batch: 140/141, Loss: 1.8172, Perplexity: 6.1544, Time: 0.21s
2025-04-25 23:48:09,092 - INFO - Validation  Loss: 1.9404, Perplexity: 6.9615, Time: 0.53s
2025-04-25 23:48:09,092 - INFO - Epoch 21/300 - Train Loss: 1.8195, Val Loss: 1.9404
2025-04-25 23:48:09,299 - INFO - Batch: 10/141, Loss: 1.8080, Perplexity: 6.0981, Time: 0.21s
2025-04-25 23:48:09,570 - INFO - Batch: 20/141, Loss: 1.8188, Perplexity: 6.1647, Time: 0.27s
2025-04-25 23:48:09,778 - INFO - Batch: 30/141, Loss: 1.8118, Perplexity: 6.1213, Time: 0.21s
2025-04-25 23:48:09,993 - INFO - Batch: 40/141, Loss: 1.8194, Perplexity: 6.1680, Time: 0.22s
2025-04-25 23:48:10,255 - INFO - Batch: 50/141, Loss: 1.8172, Perplexity: 6.1548, Time: 0.26s
2025-04-25 23:48:10,471 - INFO - Batch: 60/141, Loss: 1.8100, Perplexity: 6.1107, Time: 0.22s
2025-04-25 23:48:10,691 - INFO - Batch: 70/141, Loss: 1.8081, Perplexity: 6.0988, Time: 0.22s
2025-04-25 23:48:10,910 - INFO - Batch: 80/141, Loss: 1.8109, Perplexity: 6.1159, Time: 0.22s
2025-04-25 23:48:11,128 - INFO - Batch: 90/141, Loss: 1.8279, Perplexity: 6.2209, Time: 0.22s
2025-04-25 23:48:11,401 - INFO - Batch: 100/141, Loss: 1.8061, Perplexity: 6.0866, Time: 0.27s
2025-04-25 23:48:11,611 - INFO - Batch: 110/141, Loss: 1.8285, Perplexity: 6.2245, Time: 0.21s
2025-04-25 23:48:11,832 - INFO - Batch: 120/141, Loss: 1.8190, Perplexity: 6.1660, Time: 0.22s
2025-04-25 23:48:12,045 - INFO - Batch: 130/141, Loss: 1.8064, Perplexity: 6.0885, Time: 0.21s
2025-04-25 23:48:12,261 - INFO - Batch: 140/141, Loss: 1.8093, Perplexity: 6.1061, Time: 0.22s
2025-04-25 23:48:12,881 - INFO - Validation  Loss: 1.9353, Perplexity: 6.9259, Time: 0.60s
2025-04-25 23:48:12,881 - INFO - Epoch 22/300 - Train Loss: 1.8126, Val Loss: 1.9353
2025-04-25 23:48:13,085 - INFO - Batch: 10/141, Loss: 1.8164, Perplexity: 6.1499, Time: 0.20s
2025-04-25 23:48:13,290 - INFO - Batch: 20/141, Loss: 1.8101, Perplexity: 6.1113, Time: 0.20s
2025-04-25 23:48:13,497 - INFO - Batch: 30/141, Loss: 1.8067, Perplexity: 6.0906, Time: 0.21s
2025-04-25 23:48:13,708 - INFO - Batch: 40/141, Loss: 1.7983, Perplexity: 6.0392, Time: 0.21s
2025-04-25 23:48:13,923 - INFO - Batch: 50/141, Loss: 1.8055, Perplexity: 6.0827, Time: 0.21s
2025-04-25 23:48:14,195 - INFO - Batch: 60/141, Loss: 1.8000, Perplexity: 6.0494, Time: 0.27s
2025-04-25 23:48:14,408 - INFO - Batch: 70/141, Loss: 1.8041, Perplexity: 6.0743, Time: 0.21s
2025-04-25 23:48:14,626 - INFO - Batch: 80/141, Loss: 1.7931, Perplexity: 6.0083, Time: 0.22s
2025-04-25 23:48:14,842 - INFO - Batch: 90/141, Loss: 1.8041, Perplexity: 6.0747, Time: 0.22s
2025-04-25 23:48:15,056 - INFO - Batch: 100/141, Loss: 1.8113, Perplexity: 6.1183, Time: 0.21s
2025-04-25 23:48:15,282 - INFO - Batch: 110/141, Loss: 1.8154, Perplexity: 6.1433, Time: 0.23s
2025-04-25 23:48:15,490 - INFO - Batch: 120/141, Loss: 1.8043, Perplexity: 6.0758, Time: 0.21s
2025-04-25 23:48:15,715 - INFO - Batch: 130/141, Loss: 1.8015, Perplexity: 6.0590, Time: 0.22s
2025-04-25 23:48:16,013 - INFO - Batch: 140/141, Loss: 1.8093, Perplexity: 6.1063, Time: 0.30s
2025-04-25 23:48:16,561 - INFO - Validation  Loss: 1.9322, Perplexity: 6.9049, Time: 0.52s
2025-04-25 23:48:16,562 - INFO - Epoch 23/300 - Train Loss: 1.8064, Val Loss: 1.9322
2025-04-25 23:48:16,771 - INFO - Batch: 10/141, Loss: 1.7936, Perplexity: 6.0113, Time: 0.21s
2025-04-25 23:48:16,987 - INFO - Batch: 20/141, Loss: 1.8022, Perplexity: 6.0630, Time: 0.22s
2025-04-25 23:48:17,253 - INFO - Batch: 30/141, Loss: 1.7795, Perplexity: 5.9268, Time: 0.27s
2025-04-25 23:48:17,461 - INFO - Batch: 40/141, Loss: 1.8050, Perplexity: 6.0797, Time: 0.21s
2025-04-25 23:48:17,669 - INFO - Batch: 50/141, Loss: 1.7983, Perplexity: 6.0393, Time: 0.21s
2025-04-25 23:48:17,885 - INFO - Batch: 60/141, Loss: 1.8217, Perplexity: 6.1824, Time: 0.22s
2025-04-25 23:48:18,099 - INFO - Batch: 70/141, Loss: 1.7969, Perplexity: 6.0310, Time: 0.21s
2025-04-25 23:48:18,315 - INFO - Batch: 80/141, Loss: 1.7875, Perplexity: 5.9742, Time: 0.22s
2025-04-25 23:48:18,529 - INFO - Batch: 90/141, Loss: 1.8044, Perplexity: 6.0765, Time: 0.21s
2025-04-25 23:48:18,750 - INFO - Batch: 100/141, Loss: 1.7866, Perplexity: 5.9689, Time: 0.22s
2025-04-25 23:48:19,025 - INFO - Batch: 110/141, Loss: 1.8003, Perplexity: 6.0517, Time: 0.28s
2025-04-25 23:48:19,236 - INFO - Batch: 120/141, Loss: 1.7824, Perplexity: 5.9440, Time: 0.21s
2025-04-25 23:48:19,443 - INFO - Batch: 130/141, Loss: 1.7990, Perplexity: 6.0434, Time: 0.21s
2025-04-25 23:48:19,653 - INFO - Batch: 140/141, Loss: 1.7977, Perplexity: 6.0358, Time: 0.21s
2025-04-25 23:48:20,275 - INFO - Validation  Loss: 1.9288, Perplexity: 6.8815, Time: 0.60s
2025-04-25 23:48:20,276 - INFO - Epoch 24/300 - Train Loss: 1.8005, Val Loss: 1.9288
2025-04-25 23:48:20,481 - INFO - Batch: 10/141, Loss: 1.8009, Perplexity: 6.0554, Time: 0.21s
2025-04-25 23:48:20,693 - INFO - Batch: 20/141, Loss: 1.7940, Perplexity: 6.0132, Time: 0.21s
2025-04-25 23:48:20,909 - INFO - Batch: 30/141, Loss: 1.7952, Perplexity: 6.0205, Time: 0.22s
2025-04-25 23:48:21,124 - INFO - Batch: 40/141, Loss: 1.7901, Perplexity: 5.9900, Time: 0.21s
2025-04-25 23:48:21,335 - INFO - Batch: 50/141, Loss: 1.8037, Perplexity: 6.0718, Time: 0.21s
2025-04-25 23:48:21,543 - INFO - Batch: 60/141, Loss: 1.7921, Perplexity: 6.0019, Time: 0.21s
2025-04-25 23:48:21,812 - INFO - Batch: 70/141, Loss: 1.7724, Perplexity: 5.8850, Time: 0.27s
2025-04-25 23:48:22,022 - INFO - Batch: 80/141, Loss: 1.7986, Perplexity: 6.0413, Time: 0.21s
2025-04-25 23:48:22,228 - INFO - Batch: 90/141, Loss: 1.7955, Perplexity: 6.0228, Time: 0.21s
2025-04-25 23:48:22,442 - INFO - Batch: 100/141, Loss: 1.7946, Perplexity: 6.0170, Time: 0.21s
2025-04-25 23:48:22,660 - INFO - Batch: 110/141, Loss: 1.7921, Perplexity: 6.0020, Time: 0.22s
2025-04-25 23:48:22,875 - INFO - Batch: 120/141, Loss: 1.7914, Perplexity: 5.9976, Time: 0.22s
2025-04-25 23:48:23,087 - INFO - Batch: 130/141, Loss: 1.8115, Perplexity: 6.1197, Time: 0.21s
2025-04-25 23:48:23,299 - INFO - Batch: 140/141, Loss: 1.8034, Perplexity: 6.0701, Time: 0.21s
2025-04-25 23:48:23,907 - INFO - Validation  Loss: 1.9270, Perplexity: 6.8688, Time: 0.58s
2025-04-25 23:48:23,907 - INFO - Epoch 25/300 - Train Loss: 1.7952, Val Loss: 1.9270
2025-04-25 23:48:24,114 - INFO - Batch: 10/141, Loss: 1.7950, Perplexity: 6.0197, Time: 0.21s
2025-04-25 23:48:24,321 - INFO - Batch: 20/141, Loss: 1.7891, Perplexity: 5.9842, Time: 0.21s
2025-04-25 23:48:24,539 - INFO - Batch: 30/141, Loss: 1.7959, Perplexity: 6.0248, Time: 0.22s
2025-04-25 23:48:24,813 - INFO - Batch: 40/141, Loss: 1.7986, Perplexity: 6.0410, Time: 0.27s
2025-04-25 23:48:25,023 - INFO - Batch: 50/141, Loss: 1.8059, Perplexity: 6.0855, Time: 0.21s
2025-04-25 23:48:25,238 - INFO - Batch: 60/141, Loss: 1.7973, Perplexity: 6.0332, Time: 0.21s
2025-04-25 23:48:25,447 - INFO - Batch: 70/141, Loss: 1.8001, Perplexity: 6.0502, Time: 0.21s
2025-04-25 23:48:25,658 - INFO - Batch: 80/141, Loss: 1.7808, Perplexity: 5.9345, Time: 0.21s
2025-04-25 23:48:25,888 - INFO - Batch: 90/141, Loss: 1.7835, Perplexity: 5.9509, Time: 0.23s
2025-04-25 23:48:26,130 - INFO - Batch: 100/141, Loss: 1.7696, Perplexity: 5.8687, Time: 0.24s
2025-04-25 23:48:26,336 - INFO - Batch: 110/141, Loss: 1.7861, Perplexity: 5.9664, Time: 0.21s
2025-04-25 23:48:26,605 - INFO - Batch: 120/141, Loss: 1.7842, Perplexity: 5.9550, Time: 0.27s
2025-04-25 23:48:26,815 - INFO - Batch: 130/141, Loss: 1.7918, Perplexity: 6.0004, Time: 0.21s
2025-04-25 23:48:27,030 - INFO - Batch: 140/141, Loss: 1.7833, Perplexity: 5.9493, Time: 0.21s
2025-04-25 23:48:27,645 - INFO - Validation  Loss: 1.9239, Perplexity: 6.8479, Time: 0.59s
2025-04-25 23:48:27,646 - INFO - Epoch 26/300 - Train Loss: 1.7905, Val Loss: 1.9239
2025-04-25 23:48:27,850 - INFO - Batch: 10/141, Loss: 1.7879, Perplexity: 5.9770, Time: 0.20s
2025-04-25 23:48:28,059 - INFO - Batch: 20/141, Loss: 1.7879, Perplexity: 5.9768, Time: 0.21s
2025-04-25 23:48:28,265 - INFO - Batch: 30/141, Loss: 1.7865, Perplexity: 5.9682, Time: 0.21s
2025-04-25 23:48:28,479 - INFO - Batch: 40/141, Loss: 1.7984, Perplexity: 6.0398, Time: 0.21s
2025-04-25 23:48:28,692 - INFO - Batch: 50/141, Loss: 1.7877, Perplexity: 5.9759, Time: 0.21s
2025-04-25 23:48:28,908 - INFO - Batch: 60/141, Loss: 1.7771, Perplexity: 5.9124, Time: 0.22s
2025-04-25 23:48:29,121 - INFO - Batch: 70/141, Loss: 1.7854, Perplexity: 5.9618, Time: 0.21s
2025-04-25 23:48:29,388 - INFO - Batch: 80/141, Loss: 1.7944, Perplexity: 6.0160, Time: 0.27s
2025-04-25 23:48:29,594 - INFO - Batch: 90/141, Loss: 1.7752, Perplexity: 5.9015, Time: 0.21s
2025-04-25 23:48:29,803 - INFO - Batch: 100/141, Loss: 1.7896, Perplexity: 5.9872, Time: 0.21s
2025-04-25 23:48:30,018 - INFO - Batch: 110/141, Loss: 1.8020, Perplexity: 6.0620, Time: 0.22s
2025-04-25 23:48:30,238 - INFO - Batch: 120/141, Loss: 1.7910, Perplexity: 5.9953, Time: 0.22s
2025-04-25 23:48:30,455 - INFO - Batch: 130/141, Loss: 1.7729, Perplexity: 5.8881, Time: 0.22s
2025-04-25 23:48:30,664 - INFO - Batch: 140/141, Loss: 1.8071, Perplexity: 6.0925, Time: 0.21s
2025-04-25 23:48:31,284 - INFO - Validation  Loss: 1.9212, Perplexity: 6.8290, Time: 0.59s
2025-04-25 23:48:31,284 - INFO - Epoch 27/300 - Train Loss: 1.7860, Val Loss: 1.9212
2025-04-25 23:48:31,493 - INFO - Batch: 10/141, Loss: 1.7712, Perplexity: 5.8782, Time: 0.21s
2025-04-25 23:48:31,705 - INFO - Batch: 20/141, Loss: 1.7790, Perplexity: 5.9237, Time: 0.21s
2025-04-25 23:48:31,919 - INFO - Batch: 30/141, Loss: 1.7820, Perplexity: 5.9415, Time: 0.21s
2025-04-25 23:48:32,131 - INFO - Batch: 40/141, Loss: 1.7749, Perplexity: 5.8999, Time: 0.21s
2025-04-25 23:48:32,395 - INFO - Batch: 50/141, Loss: 1.7890, Perplexity: 5.9835, Time: 0.26s
2025-04-25 23:48:32,605 - INFO - Batch: 60/141, Loss: 1.7941, Perplexity: 6.0142, Time: 0.21s
2025-04-25 23:48:32,822 - INFO - Batch: 70/141, Loss: 1.7939, Perplexity: 6.0129, Time: 0.22s
2025-04-25 23:48:33,036 - INFO - Batch: 80/141, Loss: 1.7851, Perplexity: 5.9602, Time: 0.21s
2025-04-25 23:48:33,247 - INFO - Batch: 90/141, Loss: 1.7815, Perplexity: 5.9389, Time: 0.21s
2025-04-25 23:48:33,460 - INFO - Batch: 100/141, Loss: 1.7944, Perplexity: 6.0159, Time: 0.21s
2025-04-25 23:48:33,675 - INFO - Batch: 110/141, Loss: 1.7673, Perplexity: 5.8548, Time: 0.22s
2025-04-25 23:48:33,959 - INFO - Batch: 120/141, Loss: 1.7695, Perplexity: 5.8679, Time: 0.28s
2025-04-25 23:48:34,164 - INFO - Batch: 130/141, Loss: 1.7835, Perplexity: 5.9507, Time: 0.21s
2025-04-25 23:48:34,375 - INFO - Batch: 140/141, Loss: 1.7737, Perplexity: 5.8926, Time: 0.21s
2025-04-25 23:48:34,943 - INFO - Validation  Loss: 1.9201, Perplexity: 6.8217, Time: 0.54s
2025-04-25 23:48:34,943 - INFO - Epoch 28/300 - Train Loss: 1.7820, Val Loss: 1.9201
2025-04-25 23:48:35,218 - INFO - Batch: 10/141, Loss: 1.7723, Perplexity: 5.8843, Time: 0.27s
2025-04-25 23:48:35,424 - INFO - Batch: 20/141, Loss: 1.7802, Perplexity: 5.9310, Time: 0.21s
2025-04-25 23:48:35,632 - INFO - Batch: 30/141, Loss: 1.7770, Perplexity: 5.9120, Time: 0.21s
2025-04-25 23:48:35,843 - INFO - Batch: 40/141, Loss: 1.7803, Perplexity: 5.9314, Time: 0.21s
2025-04-25 23:48:36,061 - INFO - Batch: 50/141, Loss: 1.7897, Perplexity: 5.9877, Time: 0.22s
2025-04-25 23:48:36,273 - INFO - Batch: 60/141, Loss: 1.7834, Perplexity: 5.9499, Time: 0.21s
2025-04-25 23:48:36,488 - INFO - Batch: 70/141, Loss: 1.7884, Perplexity: 5.9800, Time: 0.21s
2025-04-25 23:48:36,704 - INFO - Batch: 80/141, Loss: 1.7784, Perplexity: 5.9202, Time: 0.22s
2025-04-25 23:48:36,983 - INFO - Batch: 90/141, Loss: 1.7746, Perplexity: 5.8979, Time: 0.28s
2025-04-25 23:48:37,193 - INFO - Batch: 100/141, Loss: 1.7766, Perplexity: 5.9097, Time: 0.21s
2025-04-25 23:48:37,402 - INFO - Batch: 110/141, Loss: 1.7721, Perplexity: 5.8833, Time: 0.21s
2025-04-25 23:48:37,614 - INFO - Batch: 120/141, Loss: 1.7837, Perplexity: 5.9518, Time: 0.21s
2025-04-25 23:48:37,827 - INFO - Batch: 130/141, Loss: 1.7983, Perplexity: 6.0391, Time: 0.21s
2025-04-25 23:48:38,048 - INFO - Batch: 140/141, Loss: 1.7783, Perplexity: 5.9200, Time: 0.22s
2025-04-25 23:48:38,679 - INFO - Validation  Loss: 1.9181, Perplexity: 6.8079, Time: 0.61s
2025-04-25 23:48:38,679 - INFO - Epoch 29/300 - Train Loss: 1.7783, Val Loss: 1.9181
2025-04-25 23:48:38,898 - INFO - Batch: 10/141, Loss: 1.7721, Perplexity: 5.8832, Time: 0.22s
2025-04-25 23:48:39,107 - INFO - Batch: 20/141, Loss: 1.7784, Perplexity: 5.9202, Time: 0.21s
2025-04-25 23:48:39,320 - INFO - Batch: 30/141, Loss: 1.7787, Perplexity: 5.9221, Time: 0.21s
2025-04-25 23:48:39,529 - INFO - Batch: 40/141, Loss: 1.7824, Perplexity: 5.9439, Time: 0.21s
2025-04-25 23:48:39,740 - INFO - Batch: 50/141, Loss: 1.7871, Perplexity: 5.9719, Time: 0.21s
2025-04-25 23:48:40,012 - INFO - Batch: 60/141, Loss: 1.7878, Perplexity: 5.9766, Time: 0.27s
2025-04-25 23:48:40,228 - INFO - Batch: 70/141, Loss: 1.7687, Perplexity: 5.8630, Time: 0.22s
2025-04-25 23:48:40,441 - INFO - Batch: 80/141, Loss: 1.7722, Perplexity: 5.8839, Time: 0.21s
2025-04-25 23:48:40,657 - INFO - Batch: 90/141, Loss: 1.7723, Perplexity: 5.8846, Time: 0.22s
2025-04-25 23:48:40,874 - INFO - Batch: 100/141, Loss: 1.7708, Perplexity: 5.8758, Time: 0.22s
2025-04-25 23:48:41,093 - INFO - Batch: 110/141, Loss: 1.7686, Perplexity: 5.8625, Time: 0.22s
2025-04-25 23:48:41,310 - INFO - Batch: 120/141, Loss: 1.7774, Perplexity: 5.9146, Time: 0.22s
2025-04-25 23:48:41,586 - INFO - Batch: 130/141, Loss: 1.7865, Perplexity: 5.9684, Time: 0.28s
2025-04-25 23:48:41,797 - INFO - Batch: 140/141, Loss: 1.7724, Perplexity: 5.8849, Time: 0.21s
2025-04-25 23:48:42,393 - INFO - Validation  Loss: 1.9159, Perplexity: 6.7931, Time: 0.57s
2025-04-25 23:48:42,393 - INFO - Epoch 30/300 - Train Loss: 1.7749, Val Loss: 1.9159
2025-04-25 23:48:42,602 - INFO - Batch: 10/141, Loss: 1.7757, Perplexity: 5.9045, Time: 0.21s
2025-04-25 23:48:42,880 - INFO - Batch: 20/141, Loss: 1.7748, Perplexity: 5.8990, Time: 0.28s
2025-04-25 23:48:43,090 - INFO - Batch: 30/141, Loss: 1.7760, Perplexity: 5.9063, Time: 0.21s
2025-04-25 23:48:43,297 - INFO - Batch: 40/141, Loss: 1.7684, Perplexity: 5.8615, Time: 0.21s
2025-04-25 23:48:43,510 - INFO - Batch: 50/141, Loss: 1.7798, Perplexity: 5.9286, Time: 0.21s
2025-04-25 23:48:43,721 - INFO - Batch: 60/141, Loss: 1.7645, Perplexity: 5.8386, Time: 0.21s
2025-04-25 23:48:43,935 - INFO - Batch: 70/141, Loss: 1.7666, Perplexity: 5.8511, Time: 0.21s
2025-04-25 23:48:44,146 - INFO - Batch: 80/141, Loss: 1.7729, Perplexity: 5.8880, Time: 0.21s
2025-04-25 23:48:44,361 - INFO - Batch: 90/141, Loss: 1.7723, Perplexity: 5.8845, Time: 0.21s
2025-04-25 23:48:44,637 - INFO - Batch: 100/141, Loss: 1.7764, Perplexity: 5.9083, Time: 0.28s
2025-04-25 23:48:44,844 - INFO - Batch: 110/141, Loss: 1.7722, Perplexity: 5.8837, Time: 0.21s
2025-04-25 23:48:45,062 - INFO - Batch: 120/141, Loss: 1.7595, Perplexity: 5.8094, Time: 0.22s
2025-04-25 23:48:45,285 - INFO - Batch: 130/141, Loss: 1.7562, Perplexity: 5.7902, Time: 0.22s
2025-04-25 23:48:45,495 - INFO - Batch: 140/141, Loss: 1.7540, Perplexity: 5.7777, Time: 0.21s
2025-04-25 23:48:46,111 - INFO - Validation  Loss: 1.9149, Perplexity: 6.7864, Time: 0.59s
2025-04-25 23:48:46,111 - INFO - Epoch 31/300 - Train Loss: 1.7718, Val Loss: 1.9149
2025-04-25 23:48:46,315 - INFO - Batch: 10/141, Loss: 1.7706, Perplexity: 5.8744, Time: 0.20s
2025-04-25 23:48:46,523 - INFO - Batch: 20/141, Loss: 1.7586, Perplexity: 5.8045, Time: 0.21s
2025-04-25 23:48:46,743 - INFO - Batch: 30/141, Loss: 1.7621, Perplexity: 5.8248, Time: 0.22s
2025-04-25 23:48:46,982 - INFO - Batch: 40/141, Loss: 1.7563, Perplexity: 5.7911, Time: 0.24s
2025-04-25 23:48:47,196 - INFO - Batch: 50/141, Loss: 1.7657, Perplexity: 5.8457, Time: 0.21s
2025-04-25 23:48:47,406 - INFO - Batch: 60/141, Loss: 1.7693, Perplexity: 5.8667, Time: 0.21s
2025-04-25 23:48:47,677 - INFO - Batch: 70/141, Loss: 1.7727, Perplexity: 5.8870, Time: 0.27s
2025-04-25 23:48:47,888 - INFO - Batch: 80/141, Loss: 1.7630, Perplexity: 5.8297, Time: 0.21s
2025-04-25 23:48:48,104 - INFO - Batch: 90/141, Loss: 1.7618, Perplexity: 5.8227, Time: 0.22s
2025-04-25 23:48:48,314 - INFO - Batch: 100/141, Loss: 1.7558, Perplexity: 5.7880, Time: 0.21s
2025-04-25 23:48:48,526 - INFO - Batch: 110/141, Loss: 1.7786, Perplexity: 5.9215, Time: 0.21s
2025-04-25 23:48:48,747 - INFO - Batch: 120/141, Loss: 1.7714, Perplexity: 5.8790, Time: 0.22s
2025-04-25 23:48:48,967 - INFO - Batch: 130/141, Loss: 1.7661, Perplexity: 5.8482, Time: 0.22s
2025-04-25 23:48:49,237 - INFO - Batch: 140/141, Loss: 1.7658, Perplexity: 5.8462, Time: 0.27s
2025-04-25 23:48:49,784 - INFO - Validation  Loss: 1.9128, Perplexity: 6.7717, Time: 0.52s
2025-04-25 23:48:49,784 - INFO - Epoch 32/300 - Train Loss: 1.7690, Val Loss: 1.9128
2025-04-25 23:48:49,992 - INFO - Batch: 10/141, Loss: 1.7760, Perplexity: 5.9060, Time: 0.21s
2025-04-25 23:48:50,213 - INFO - Batch: 20/141, Loss: 1.7639, Perplexity: 5.8351, Time: 0.22s
2025-04-25 23:48:50,510 - INFO - Batch: 30/141, Loss: 1.7507, Perplexity: 5.7584, Time: 0.30s
2025-04-25 23:48:50,724 - INFO - Batch: 40/141, Loss: 1.7666, Perplexity: 5.8511, Time: 0.21s
2025-04-25 23:48:50,938 - INFO - Batch: 50/141, Loss: 1.7555, Perplexity: 5.7862, Time: 0.21s
2025-04-25 23:48:51,158 - INFO - Batch: 60/141, Loss: 1.7697, Perplexity: 5.8693, Time: 0.22s
2025-04-25 23:48:51,366 - INFO - Batch: 70/141, Loss: 1.7518, Perplexity: 5.7649, Time: 0.21s
2025-04-25 23:48:51,576 - INFO - Batch: 80/141, Loss: 1.7551, Perplexity: 5.7839, Time: 0.21s
2025-04-25 23:48:51,791 - INFO - Batch: 90/141, Loss: 1.7716, Perplexity: 5.8801, Time: 0.22s
2025-04-25 23:48:52,006 - INFO - Batch: 100/141, Loss: 1.7629, Perplexity: 5.8293, Time: 0.21s
2025-04-25 23:48:52,275 - INFO - Batch: 110/141, Loss: 1.7768, Perplexity: 5.9111, Time: 0.27s
2025-04-25 23:48:52,481 - INFO - Batch: 120/141, Loss: 1.7618, Perplexity: 5.8228, Time: 0.21s
2025-04-25 23:48:52,690 - INFO - Batch: 130/141, Loss: 1.7603, Perplexity: 5.8145, Time: 0.21s
2025-04-25 23:48:52,900 - INFO - Batch: 140/141, Loss: 1.7701, Perplexity: 5.8713, Time: 0.21s
2025-04-25 23:48:53,519 - INFO - Validation  Loss: 1.9120, Perplexity: 6.7666, Time: 0.59s
2025-04-25 23:48:53,519 - INFO - Epoch 33/300 - Train Loss: 1.7663, Val Loss: 1.9120
2025-04-25 23:48:53,723 - INFO - Batch: 10/141, Loss: 1.7588, Perplexity: 5.8055, Time: 0.20s
2025-04-25 23:48:53,935 - INFO - Batch: 20/141, Loss: 1.7557, Perplexity: 5.7877, Time: 0.21s
2025-04-25 23:48:54,141 - INFO - Batch: 30/141, Loss: 1.7561, Perplexity: 5.7895, Time: 0.21s
2025-04-25 23:48:54,352 - INFO - Batch: 40/141, Loss: 1.7610, Perplexity: 5.8180, Time: 0.21s
2025-04-25 23:48:54,563 - INFO - Batch: 50/141, Loss: 1.7768, Perplexity: 5.9107, Time: 0.21s
2025-04-25 23:48:54,780 - INFO - Batch: 60/141, Loss: 1.7642, Perplexity: 5.8370, Time: 0.22s
2025-04-25 23:48:55,058 - INFO - Batch: 70/141, Loss: 1.7623, Perplexity: 5.8260, Time: 0.28s
2025-04-25 23:48:55,272 - INFO - Batch: 80/141, Loss: 1.7774, Perplexity: 5.9146, Time: 0.21s
2025-04-25 23:48:55,480 - INFO - Batch: 90/141, Loss: 1.7697, Perplexity: 5.8689, Time: 0.21s
2025-04-25 23:48:55,689 - INFO - Batch: 100/141, Loss: 1.7674, Perplexity: 5.8553, Time: 0.21s
2025-04-25 23:48:55,898 - INFO - Batch: 110/141, Loss: 1.7614, Perplexity: 5.8204, Time: 0.21s
2025-04-25 23:48:56,115 - INFO - Batch: 120/141, Loss: 1.7632, Perplexity: 5.8308, Time: 0.22s
2025-04-25 23:48:56,326 - INFO - Batch: 130/141, Loss: 1.7584, Perplexity: 5.8034, Time: 0.21s
2025-04-25 23:48:56,541 - INFO - Batch: 140/141, Loss: 1.7745, Perplexity: 5.8973, Time: 0.21s
2025-04-25 23:48:57,163 - INFO - Validation  Loss: 1.9111, Perplexity: 6.7602, Time: 0.60s
2025-04-25 23:48:57,164 - INFO - Epoch 34/300 - Train Loss: 1.7638, Val Loss: 1.9111
2025-04-25 23:48:57,370 - INFO - Batch: 10/141, Loss: 1.7579, Perplexity: 5.8002, Time: 0.21s
2025-04-25 23:48:57,576 - INFO - Batch: 20/141, Loss: 1.7643, Perplexity: 5.8374, Time: 0.21s
2025-04-25 23:48:57,785 - INFO - Batch: 30/141, Loss: 1.7623, Perplexity: 5.8255, Time: 0.21s
2025-04-25 23:48:58,096 - INFO - Batch: 40/141, Loss: 1.7629, Perplexity: 5.8291, Time: 0.31s
2025-04-25 23:48:58,306 - INFO - Batch: 50/141, Loss: 1.7673, Perplexity: 5.8551, Time: 0.21s
2025-04-25 23:48:58,518 - INFO - Batch: 60/141, Loss: 1.7629, Perplexity: 5.8296, Time: 0.21s
2025-04-25 23:48:58,732 - INFO - Batch: 70/141, Loss: 1.7728, Perplexity: 5.8875, Time: 0.21s
2025-04-25 23:48:58,951 - INFO - Batch: 80/141, Loss: 1.7375, Perplexity: 5.6831, Time: 0.22s
2025-04-25 23:48:59,179 - INFO - Batch: 90/141, Loss: 1.7694, Perplexity: 5.8674, Time: 0.23s
2025-04-25 23:48:59,392 - INFO - Batch: 100/141, Loss: 1.7607, Perplexity: 5.8167, Time: 0.21s
2025-04-25 23:48:59,606 - INFO - Batch: 110/141, Loss: 1.7533, Perplexity: 5.7737, Time: 0.21s
2025-04-25 23:48:59,874 - INFO - Batch: 120/141, Loss: 1.7635, Perplexity: 5.8330, Time: 0.27s
2025-04-25 23:49:00,094 - INFO - Batch: 130/141, Loss: 1.7537, Perplexity: 5.7758, Time: 0.22s
2025-04-25 23:49:00,307 - INFO - Batch: 140/141, Loss: 1.7595, Perplexity: 5.8098, Time: 0.21s
2025-04-25 23:49:00,868 - INFO - Validation  Loss: 1.9102, Perplexity: 6.7546, Time: 0.54s
2025-04-25 23:49:00,868 - INFO - Epoch 35/300 - Train Loss: 1.7614, Val Loss: 1.9102
2025-04-25 23:49:01,141 - INFO - Batch: 10/141, Loss: 1.7585, Perplexity: 5.8035, Time: 0.27s
2025-04-25 23:49:01,356 - INFO - Batch: 20/141, Loss: 1.7662, Perplexity: 5.8486, Time: 0.21s
2025-04-25 23:49:01,565 - INFO - Batch: 30/141, Loss: 1.7581, Perplexity: 5.8015, Time: 0.21s
2025-04-25 23:49:01,774 - INFO - Batch: 40/141, Loss: 1.7672, Perplexity: 5.8547, Time: 0.21s
2025-04-25 23:49:01,990 - INFO - Batch: 50/141, Loss: 1.7498, Perplexity: 5.7535, Time: 0.22s
2025-04-25 23:49:02,201 - INFO - Batch: 60/141, Loss: 1.7630, Perplexity: 5.8297, Time: 0.21s
2025-04-25 23:49:02,414 - INFO - Batch: 70/141, Loss: 1.7548, Perplexity: 5.7822, Time: 0.21s
2025-04-25 23:49:02,686 - INFO - Batch: 80/141, Loss: 1.7526, Perplexity: 5.7693, Time: 0.27s
2025-04-25 23:49:02,893 - INFO - Batch: 90/141, Loss: 1.7608, Perplexity: 5.8173, Time: 0.21s
2025-04-25 23:49:03,103 - INFO - Batch: 100/141, Loss: 1.7524, Perplexity: 5.7682, Time: 0.21s
2025-04-25 23:49:03,323 - INFO - Batch: 110/141, Loss: 1.7548, Perplexity: 5.7824, Time: 0.22s
2025-04-25 23:49:03,541 - INFO - Batch: 120/141, Loss: 1.7614, Perplexity: 5.8204, Time: 0.22s
2025-04-25 23:49:03,758 - INFO - Batch: 130/141, Loss: 1.7601, Perplexity: 5.8129, Time: 0.22s
2025-04-25 23:49:03,973 - INFO - Batch: 140/141, Loss: 1.7492, Perplexity: 5.7501, Time: 0.21s
2025-04-25 23:49:04,598 - INFO - Validation  Loss: 1.9086, Perplexity: 6.7438, Time: 0.60s
2025-04-25 23:49:04,598 - INFO - Epoch 36/300 - Train Loss: 1.7592, Val Loss: 1.9086
2025-04-25 23:49:04,807 - INFO - Batch: 10/141, Loss: 1.7851, Perplexity: 5.9602, Time: 0.21s
2025-04-25 23:49:05,019 - INFO - Batch: 20/141, Loss: 1.7491, Perplexity: 5.7496, Time: 0.21s
2025-04-25 23:49:05,239 - INFO - Batch: 30/141, Loss: 1.7522, Perplexity: 5.7673, Time: 0.22s
2025-04-25 23:49:05,456 - INFO - Batch: 40/141, Loss: 1.7623, Perplexity: 5.8257, Time: 0.22s
2025-04-25 23:49:05,727 - INFO - Batch: 50/141, Loss: 1.7536, Perplexity: 5.7756, Time: 0.27s
2025-04-25 23:49:05,935 - INFO - Batch: 60/141, Loss: 1.7570, Perplexity: 5.7952, Time: 0.21s
2025-04-25 23:49:06,155 - INFO - Batch: 70/141, Loss: 1.7525, Perplexity: 5.7691, Time: 0.22s
2025-04-25 23:49:06,365 - INFO - Batch: 80/141, Loss: 1.7615, Perplexity: 5.8209, Time: 0.21s
2025-04-25 23:49:06,581 - INFO - Batch: 90/141, Loss: 1.7635, Perplexity: 5.8325, Time: 0.22s
2025-04-25 23:49:06,792 - INFO - Batch: 100/141, Loss: 1.7616, Perplexity: 5.8215, Time: 0.21s
2025-04-25 23:49:07,010 - INFO - Batch: 110/141, Loss: 1.7701, Perplexity: 5.8712, Time: 0.22s
2025-04-25 23:49:07,226 - INFO - Batch: 120/141, Loss: 1.7458, Perplexity: 5.7302, Time: 0.22s
2025-04-25 23:49:07,493 - INFO - Batch: 130/141, Loss: 1.7499, Perplexity: 5.7541, Time: 0.27s
2025-04-25 23:49:07,704 - INFO - Batch: 140/141, Loss: 1.7499, Perplexity: 5.7540, Time: 0.21s
2025-04-25 23:49:08,251 - INFO - Validation  Loss: 1.9071, Perplexity: 6.7335, Time: 0.52s
2025-04-25 23:49:08,251 - INFO - Epoch 37/300 - Train Loss: 1.7571, Val Loss: 1.9071
2025-04-25 23:49:08,454 - INFO - Batch: 10/141, Loss: 1.7426, Perplexity: 5.7120, Time: 0.20s
2025-04-25 23:49:08,716 - INFO - Batch: 20/141, Loss: 1.7657, Perplexity: 5.8457, Time: 0.26s
2025-04-25 23:49:08,931 - INFO - Batch: 30/141, Loss: 1.7496, Perplexity: 5.7524, Time: 0.21s
2025-04-25 23:49:09,146 - INFO - Batch: 40/141, Loss: 1.7496, Perplexity: 5.7522, Time: 0.22s
2025-04-25 23:49:09,401 - INFO - Batch: 50/141, Loss: 1.7567, Perplexity: 5.7933, Time: 0.25s
2025-04-25 23:49:09,617 - INFO - Batch: 60/141, Loss: 1.7614, Perplexity: 5.8207, Time: 0.22s
2025-04-25 23:49:09,826 - INFO - Batch: 70/141, Loss: 1.7557, Perplexity: 5.7876, Time: 0.21s
2025-04-25 23:49:10,043 - INFO - Batch: 80/141, Loss: 1.7566, Perplexity: 5.7926, Time: 0.22s
2025-04-25 23:49:10,319 - INFO - Batch: 90/141, Loss: 1.7625, Perplexity: 5.8271, Time: 0.28s
2025-04-25 23:49:10,531 - INFO - Batch: 100/141, Loss: 1.7498, Perplexity: 5.7535, Time: 0.21s
2025-04-25 23:49:10,741 - INFO - Batch: 110/141, Loss: 1.7413, Perplexity: 5.7046, Time: 0.21s
2025-04-25 23:49:10,954 - INFO - Batch: 120/141, Loss: 1.7629, Perplexity: 5.8291, Time: 0.21s
2025-04-25 23:49:11,168 - INFO - Batch: 130/141, Loss: 1.7546, Perplexity: 5.7810, Time: 0.21s
2025-04-25 23:49:11,389 - INFO - Batch: 140/141, Loss: 1.7490, Perplexity: 5.7489, Time: 0.22s
2025-04-25 23:49:12,007 - INFO - Validation  Loss: 1.9052, Perplexity: 6.7209, Time: 0.59s
2025-04-25 23:49:12,007 - INFO - Epoch 38/300 - Train Loss: 1.7550, Val Loss: 1.9052
2025-04-25 23:49:12,210 - INFO - Batch: 10/141, Loss: 1.7555, Perplexity: 5.7864, Time: 0.20s
2025-04-25 23:49:12,422 - INFO - Batch: 20/141, Loss: 1.7730, Perplexity: 5.8882, Time: 0.21s
2025-04-25 23:49:12,632 - INFO - Batch: 30/141, Loss: 1.7596, Perplexity: 5.8098, Time: 0.21s
2025-04-25 23:49:12,841 - INFO - Batch: 40/141, Loss: 1.7568, Perplexity: 5.7941, Time: 0.21s
2025-04-25 23:49:13,086 - INFO - Batch: 50/141, Loss: 1.7703, Perplexity: 5.8729, Time: 0.24s
2025-04-25 23:49:13,370 - INFO - Batch: 60/141, Loss: 1.7578, Perplexity: 5.7999, Time: 0.28s
2025-04-25 23:49:13,579 - INFO - Batch: 70/141, Loss: 1.7436, Perplexity: 5.7178, Time: 0.21s
2025-04-25 23:49:13,791 - INFO - Batch: 80/141, Loss: 1.7739, Perplexity: 5.8937, Time: 0.21s
2025-04-25 23:49:14,001 - INFO - Batch: 90/141, Loss: 1.7407, Perplexity: 5.7013, Time: 0.21s
2025-04-25 23:49:14,214 - INFO - Batch: 100/141, Loss: 1.7601, Perplexity: 5.8132, Time: 0.21s
2025-04-25 23:49:14,428 - INFO - Batch: 110/141, Loss: 1.7454, Perplexity: 5.7284, Time: 0.21s
2025-04-25 23:49:14,637 - INFO - Batch: 120/141, Loss: 1.7420, Perplexity: 5.7089, Time: 0.21s
2025-04-25 23:49:14,850 - INFO - Batch: 130/141, Loss: 1.7460, Perplexity: 5.7317, Time: 0.21s
2025-04-25 23:49:15,129 - INFO - Batch: 140/141, Loss: 1.7525, Perplexity: 5.7688, Time: 0.28s
2025-04-25 23:49:15,696 - INFO - Validation  Loss: 1.9046, Perplexity: 6.7164, Time: 0.54s
2025-04-25 23:49:15,696 - INFO - Epoch 39/300 - Train Loss: 1.7531, Val Loss: 1.9046
2025-04-25 23:49:15,912 - INFO - Batch: 10/141, Loss: 1.7590, Perplexity: 5.8066, Time: 0.22s
2025-04-25 23:49:16,194 - INFO - Batch: 20/141, Loss: 1.7597, Perplexity: 5.8105, Time: 0.28s
2025-04-25 23:49:16,403 - INFO - Batch: 30/141, Loss: 1.7580, Perplexity: 5.8010, Time: 0.21s
2025-04-25 23:49:16,611 - INFO - Batch: 40/141, Loss: 1.7540, Perplexity: 5.7779, Time: 0.21s
2025-04-25 23:49:16,823 - INFO - Batch: 50/141, Loss: 1.7573, Perplexity: 5.7966, Time: 0.21s
2025-04-25 23:49:17,040 - INFO - Batch: 60/141, Loss: 1.7553, Perplexity: 5.7850, Time: 0.22s
2025-04-25 23:49:17,253 - INFO - Batch: 70/141, Loss: 1.7570, Perplexity: 5.7950, Time: 0.21s
2025-04-25 23:49:17,473 - INFO - Batch: 80/141, Loss: 1.7515, Perplexity: 5.7632, Time: 0.22s
2025-04-25 23:49:17,690 - INFO - Batch: 90/141, Loss: 1.7570, Perplexity: 5.7952, Time: 0.22s
2025-04-25 23:49:17,964 - INFO - Batch: 100/141, Loss: 1.7697, Perplexity: 5.8692, Time: 0.27s
2025-04-25 23:49:18,181 - INFO - Batch: 110/141, Loss: 1.7431, Perplexity: 5.7149, Time: 0.22s
2025-04-25 23:49:18,393 - INFO - Batch: 120/141, Loss: 1.7442, Perplexity: 5.7215, Time: 0.21s
2025-04-25 23:49:18,604 - INFO - Batch: 130/141, Loss: 1.7404, Perplexity: 5.6995, Time: 0.21s
2025-04-25 23:49:18,818 - INFO - Batch: 140/141, Loss: 1.7294, Perplexity: 5.6372, Time: 0.21s
2025-04-25 23:49:19,432 - INFO - Validation  Loss: 1.9048, Perplexity: 6.7178, Time: 0.59s
2025-04-25 23:49:19,432 - INFO - Epoch 40/300 - Train Loss: 1.7513, Val Loss: 1.9048
2025-04-25 23:49:19,643 - INFO - Batch: 10/141, Loss: 1.7478, Perplexity: 5.7421, Time: 0.21s
2025-04-25 23:49:19,858 - INFO - Batch: 20/141, Loss: 1.7564, Perplexity: 5.7916, Time: 0.21s
2025-04-25 23:49:20,072 - INFO - Batch: 30/141, Loss: 1.7503, Perplexity: 5.7561, Time: 0.21s
2025-04-25 23:49:20,292 - INFO - Batch: 40/141, Loss: 1.7553, Perplexity: 5.7851, Time: 0.22s
2025-04-25 23:49:20,512 - INFO - Batch: 50/141, Loss: 1.7575, Perplexity: 5.7977, Time: 0.22s
2025-04-25 23:49:20,726 - INFO - Batch: 60/141, Loss: 1.7442, Perplexity: 5.7215, Time: 0.21s
2025-04-25 23:49:20,996 - INFO - Batch: 70/141, Loss: 1.7588, Perplexity: 5.8057, Time: 0.27s
2025-04-25 23:49:21,215 - INFO - Batch: 80/141, Loss: 1.7512, Perplexity: 5.7616, Time: 0.22s
2025-04-25 23:49:21,482 - INFO - Batch: 90/141, Loss: 1.7408, Perplexity: 5.7021, Time: 0.27s
2025-04-25 23:49:21,699 - INFO - Batch: 100/141, Loss: 1.7486, Perplexity: 5.7466, Time: 0.22s
2025-04-25 23:49:21,911 - INFO - Batch: 110/141, Loss: 1.7441, Perplexity: 5.7210, Time: 0.21s
2025-04-25 23:49:22,130 - INFO - Batch: 120/141, Loss: 1.7532, Perplexity: 5.7732, Time: 0.22s
2025-04-25 23:49:22,344 - INFO - Batch: 130/141, Loss: 1.7451, Perplexity: 5.7265, Time: 0.21s
2025-04-25 23:49:22,617 - INFO - Batch: 140/141, Loss: 1.7377, Perplexity: 5.6845, Time: 0.27s
2025-04-25 23:49:23,164 - INFO - Validation  Loss: 1.9032, Perplexity: 6.7071, Time: 0.52s
2025-04-25 23:49:23,165 - INFO - Epoch 41/300 - Train Loss: 1.7496, Val Loss: 1.9032
2025-04-25 23:49:23,383 - INFO - Batch: 10/141, Loss: 1.7554, Perplexity: 5.7855, Time: 0.22s
2025-04-25 23:49:23,595 - INFO - Batch: 20/141, Loss: 1.7653, Perplexity: 5.8430, Time: 0.21s
2025-04-25 23:49:23,867 - INFO - Batch: 30/141, Loss: 1.7439, Perplexity: 5.7195, Time: 0.27s
2025-04-25 23:49:24,079 - INFO - Batch: 40/141, Loss: 1.7327, Perplexity: 5.6561, Time: 0.21s
2025-04-25 23:49:24,290 - INFO - Batch: 50/141, Loss: 1.7542, Perplexity: 5.7789, Time: 0.21s
2025-04-25 23:49:24,499 - INFO - Batch: 60/141, Loss: 1.7518, Perplexity: 5.7649, Time: 0.21s
2025-04-25 23:49:24,710 - INFO - Batch: 70/141, Loss: 1.7541, Perplexity: 5.7781, Time: 0.21s
2025-04-25 23:49:24,921 - INFO - Batch: 80/141, Loss: 1.7468, Perplexity: 5.7365, Time: 0.21s
2025-04-25 23:49:25,149 - INFO - Batch: 90/141, Loss: 1.7493, Perplexity: 5.7506, Time: 0.23s
2025-04-25 23:49:25,359 - INFO - Batch: 100/141, Loss: 1.7568, Perplexity: 5.7936, Time: 0.21s
2025-04-25 23:49:25,649 - INFO - Batch: 110/141, Loss: 1.7566, Perplexity: 5.7925, Time: 0.29s
2025-04-25 23:49:25,859 - INFO - Batch: 120/141, Loss: 1.7546, Perplexity: 5.7810, Time: 0.21s
2025-04-25 23:49:26,071 - INFO - Batch: 130/141, Loss: 1.7613, Perplexity: 5.8199, Time: 0.21s
2025-04-25 23:49:26,282 - INFO - Batch: 140/141, Loss: 1.7406, Perplexity: 5.7007, Time: 0.21s
2025-04-25 23:49:26,883 - INFO - Validation  Loss: 1.9024, Perplexity: 6.7022, Time: 0.58s
2025-04-25 23:49:26,883 - INFO - Epoch 42/300 - Train Loss: 1.7479, Val Loss: 1.9024
2025-04-25 23:49:27,094 - INFO - Batch: 10/141, Loss: 1.7562, Perplexity: 5.7903, Time: 0.21s
2025-04-25 23:49:27,302 - INFO - Batch: 20/141, Loss: 1.7502, Perplexity: 5.7559, Time: 0.21s
2025-04-25 23:49:27,524 - INFO - Batch: 30/141, Loss: 1.7557, Perplexity: 5.7873, Time: 0.22s
2025-04-25 23:49:27,741 - INFO - Batch: 40/141, Loss: 1.7612, Perplexity: 5.8192, Time: 0.22s
2025-04-25 23:49:27,952 - INFO - Batch: 50/141, Loss: 1.7418, Perplexity: 5.7076, Time: 0.21s
2025-04-25 23:49:28,166 - INFO - Batch: 60/141, Loss: 1.7454, Perplexity: 5.7280, Time: 0.21s
2025-04-25 23:49:28,375 - INFO - Batch: 70/141, Loss: 1.7492, Perplexity: 5.7499, Time: 0.21s
2025-04-25 23:49:28,641 - INFO - Batch: 80/141, Loss: 1.7321, Perplexity: 5.6524, Time: 0.27s
2025-04-25 23:49:28,851 - INFO - Batch: 90/141, Loss: 1.7500, Perplexity: 5.7544, Time: 0.21s
2025-04-25 23:49:29,067 - INFO - Batch: 100/141, Loss: 1.7437, Perplexity: 5.7185, Time: 0.22s
2025-04-25 23:49:29,276 - INFO - Batch: 110/141, Loss: 1.7391, Perplexity: 5.6920, Time: 0.21s
2025-04-25 23:49:29,492 - INFO - Batch: 120/141, Loss: 1.7497, Perplexity: 5.7528, Time: 0.22s
2025-04-25 23:49:29,713 - INFO - Batch: 130/141, Loss: 1.7400, Perplexity: 5.6971, Time: 0.22s
2025-04-25 23:49:29,929 - INFO - Batch: 140/141, Loss: 1.7590, Perplexity: 5.8064, Time: 0.22s
2025-04-25 23:49:30,547 - INFO - Validation  Loss: 1.9018, Perplexity: 6.6978, Time: 0.59s
2025-04-25 23:49:30,547 - INFO - Epoch 43/300 - Train Loss: 1.7463, Val Loss: 1.9018
2025-04-25 23:49:30,758 - INFO - Batch: 10/141, Loss: 1.7476, Perplexity: 5.7408, Time: 0.21s
2025-04-25 23:49:30,975 - INFO - Batch: 20/141, Loss: 1.7502, Perplexity: 5.7560, Time: 0.22s
2025-04-25 23:49:31,192 - INFO - Batch: 30/141, Loss: 1.7464, Perplexity: 5.7340, Time: 0.22s
2025-04-25 23:49:31,465 - INFO - Batch: 40/141, Loss: 1.7605, Perplexity: 5.8153, Time: 0.27s
2025-04-25 23:49:31,679 - INFO - Batch: 50/141, Loss: 1.7531, Perplexity: 5.7725, Time: 0.21s
2025-04-25 23:49:31,892 - INFO - Batch: 60/141, Loss: 1.7552, Perplexity: 5.7844, Time: 0.21s
2025-04-25 23:49:32,107 - INFO - Batch: 70/141, Loss: 1.7493, Perplexity: 5.7505, Time: 0.21s
2025-04-25 23:49:32,319 - INFO - Batch: 80/141, Loss: 1.7604, Perplexity: 5.8149, Time: 0.21s
2025-04-25 23:49:32,535 - INFO - Batch: 90/141, Loss: 1.7489, Perplexity: 5.7483, Time: 0.22s
2025-04-25 23:49:32,748 - INFO - Batch: 100/141, Loss: 1.7483, Perplexity: 5.7449, Time: 0.21s
2025-04-25 23:49:32,960 - INFO - Batch: 110/141, Loss: 1.7547, Perplexity: 5.7818, Time: 0.21s
2025-04-25 23:49:33,232 - INFO - Batch: 120/141, Loss: 1.7308, Perplexity: 5.6451, Time: 0.27s
2025-04-25 23:49:33,444 - INFO - Batch: 130/141, Loss: 1.7452, Perplexity: 5.7270, Time: 0.21s
2025-04-25 23:49:33,669 - INFO - Batch: 140/141, Loss: 1.7571, Perplexity: 5.7957, Time: 0.23s
2025-04-25 23:49:34,225 - INFO - Validation  Loss: 1.9002, Perplexity: 6.6873, Time: 0.53s
2025-04-25 23:49:34,225 - INFO - Epoch 44/300 - Train Loss: 1.7449, Val Loss: 1.9002
2025-04-25 23:49:34,490 - INFO - Batch: 10/141, Loss: 1.7332, Perplexity: 5.6588, Time: 0.26s
2025-04-25 23:49:34,694 - INFO - Batch: 20/141, Loss: 1.7700, Perplexity: 5.8710, Time: 0.20s
2025-04-25 23:49:34,903 - INFO - Batch: 30/141, Loss: 1.7475, Perplexity: 5.7402, Time: 0.21s
2025-04-25 23:49:35,123 - INFO - Batch: 40/141, Loss: 1.7407, Perplexity: 5.7012, Time: 0.22s
2025-04-25 23:49:35,338 - INFO - Batch: 50/141, Loss: 1.7404, Perplexity: 5.6994, Time: 0.21s
2025-04-25 23:49:35,558 - INFO - Batch: 60/141, Loss: 1.7506, Perplexity: 5.7583, Time: 0.22s
2025-04-25 23:49:35,779 - INFO - Batch: 70/141, Loss: 1.7542, Perplexity: 5.7789, Time: 0.22s
2025-04-25 23:49:35,998 - INFO - Batch: 80/141, Loss: 1.7364, Perplexity: 5.6766, Time: 0.22s
2025-04-25 23:49:36,269 - INFO - Batch: 90/141, Loss: 1.7368, Perplexity: 5.6789, Time: 0.27s
2025-04-25 23:49:36,522 - INFO - Batch: 100/141, Loss: 1.7434, Perplexity: 5.7165, Time: 0.25s
2025-04-25 23:49:36,737 - INFO - Batch: 110/141, Loss: 1.7252, Perplexity: 5.6135, Time: 0.21s
2025-04-25 23:49:36,952 - INFO - Batch: 120/141, Loss: 1.7381, Perplexity: 5.6865, Time: 0.21s
2025-04-25 23:49:37,168 - INFO - Batch: 130/141, Loss: 1.7343, Perplexity: 5.6649, Time: 0.22s
2025-04-25 23:49:37,386 - INFO - Batch: 140/141, Loss: 1.7374, Perplexity: 5.6823, Time: 0.22s
2025-04-25 23:49:38,006 - INFO - Validation  Loss: 1.8998, Perplexity: 6.6843, Time: 0.59s
2025-04-25 23:49:38,006 - INFO - Epoch 45/300 - Train Loss: 1.7434, Val Loss: 1.8998
2025-04-25 23:49:38,219 - INFO - Batch: 10/141, Loss: 1.7399, Perplexity: 5.6968, Time: 0.21s
2025-04-25 23:49:38,427 - INFO - Batch: 20/141, Loss: 1.7510, Perplexity: 5.7601, Time: 0.21s
2025-04-25 23:49:38,636 - INFO - Batch: 30/141, Loss: 1.7471, Perplexity: 5.7378, Time: 0.21s
2025-04-25 23:49:38,843 - INFO - Batch: 40/141, Loss: 1.7507, Perplexity: 5.7584, Time: 0.21s
2025-04-25 23:49:39,115 - INFO - Batch: 50/141, Loss: 1.7451, Perplexity: 5.7262, Time: 0.27s
2025-04-25 23:49:39,325 - INFO - Batch: 60/141, Loss: 1.7352, Perplexity: 5.6702, Time: 0.21s
2025-04-25 23:49:39,532 - INFO - Batch: 70/141, Loss: 1.7504, Perplexity: 5.7570, Time: 0.21s
2025-04-25 23:49:39,757 - INFO - Batch: 80/141, Loss: 1.7441, Perplexity: 5.7209, Time: 0.22s
2025-04-25 23:49:39,968 - INFO - Batch: 90/141, Loss: 1.7434, Perplexity: 5.7165, Time: 0.21s
2025-04-25 23:49:40,195 - INFO - Batch: 100/141, Loss: 1.7447, Perplexity: 5.7241, Time: 0.23s
2025-04-25 23:49:40,405 - INFO - Batch: 110/141, Loss: 1.7326, Perplexity: 5.6552, Time: 0.21s
2025-04-25 23:49:40,619 - INFO - Batch: 120/141, Loss: 1.7477, Perplexity: 5.7411, Time: 0.21s
2025-04-25 23:49:40,891 - INFO - Batch: 130/141, Loss: 1.7307, Perplexity: 5.6444, Time: 0.27s
2025-04-25 23:49:41,106 - INFO - Batch: 140/141, Loss: 1.7377, Perplexity: 5.6842, Time: 0.21s
2025-04-25 23:49:41,681 - INFO - Validation  Loss: 1.8993, Perplexity: 6.6811, Time: 0.55s
2025-04-25 23:49:41,681 - INFO - Epoch 46/300 - Train Loss: 1.7422, Val Loss: 1.8993
2025-04-25 23:49:41,893 - INFO - Batch: 10/141, Loss: 1.7400, Perplexity: 5.6972, Time: 0.21s
2025-04-25 23:49:42,167 - INFO - Batch: 20/141, Loss: 1.7424, Perplexity: 5.7110, Time: 0.27s
2025-04-25 23:49:42,380 - INFO - Batch: 30/141, Loss: 1.7324, Perplexity: 5.6544, Time: 0.21s
2025-04-25 23:49:42,592 - INFO - Batch: 40/141, Loss: 1.7356, Perplexity: 5.6724, Time: 0.21s
2025-04-25 23:49:42,804 - INFO - Batch: 50/141, Loss: 1.7325, Perplexity: 5.6548, Time: 0.21s
2025-04-25 23:49:43,019 - INFO - Batch: 60/141, Loss: 1.7634, Perplexity: 5.8321, Time: 0.21s
2025-04-25 23:49:43,233 - INFO - Batch: 70/141, Loss: 1.7505, Perplexity: 5.7576, Time: 0.21s
2025-04-25 23:49:43,449 - INFO - Batch: 80/141, Loss: 1.7412, Perplexity: 5.7045, Time: 0.22s
2025-04-25 23:49:43,732 - INFO - Batch: 90/141, Loss: 1.7519, Perplexity: 5.7655, Time: 0.28s
2025-04-25 23:49:43,937 - INFO - Batch: 100/141, Loss: 1.7476, Perplexity: 5.7410, Time: 0.20s
2025-04-25 23:49:44,149 - INFO - Batch: 110/141, Loss: 1.7322, Perplexity: 5.6531, Time: 0.21s
2025-04-25 23:49:44,360 - INFO - Batch: 120/141, Loss: 1.7352, Perplexity: 5.6700, Time: 0.21s
2025-04-25 23:49:44,575 - INFO - Batch: 130/141, Loss: 1.7465, Perplexity: 5.7346, Time: 0.22s
2025-04-25 23:49:44,792 - INFO - Batch: 140/141, Loss: 1.7373, Perplexity: 5.6817, Time: 0.22s
2025-04-25 23:49:45,414 - INFO - Validation  Loss: 1.8985, Perplexity: 6.6761, Time: 0.60s
2025-04-25 23:49:45,414 - INFO - Epoch 47/300 - Train Loss: 1.7410, Val Loss: 1.8985
2025-04-25 23:49:45,621 - INFO - Batch: 10/141, Loss: 1.7212, Perplexity: 5.5910, Time: 0.21s
2025-04-25 23:49:45,839 - INFO - Batch: 20/141, Loss: 1.7510, Perplexity: 5.7603, Time: 0.22s
2025-04-25 23:49:46,049 - INFO - Batch: 30/141, Loss: 1.7393, Perplexity: 5.6934, Time: 0.21s
2025-04-25 23:49:46,269 - INFO - Batch: 40/141, Loss: 1.7461, Perplexity: 5.7320, Time: 0.22s
2025-04-25 23:49:46,480 - INFO - Batch: 50/141, Loss: 1.7354, Perplexity: 5.6710, Time: 0.21s
2025-04-25 23:49:46,761 - INFO - Batch: 60/141, Loss: 1.7513, Perplexity: 5.7619, Time: 0.28s
2025-04-25 23:49:46,992 - INFO - Batch: 70/141, Loss: 1.7453, Perplexity: 5.7276, Time: 0.23s
2025-04-25 23:49:47,209 - INFO - Batch: 80/141, Loss: 1.7506, Perplexity: 5.7582, Time: 0.22s
2025-04-25 23:49:47,422 - INFO - Batch: 90/141, Loss: 1.7281, Perplexity: 5.6300, Time: 0.21s
2025-04-25 23:49:47,631 - INFO - Batch: 100/141, Loss: 1.7435, Perplexity: 5.7173, Time: 0.21s
2025-04-25 23:49:47,847 - INFO - Batch: 110/141, Loss: 1.7267, Perplexity: 5.6220, Time: 0.22s
2025-04-25 23:49:48,058 - INFO - Batch: 120/141, Loss: 1.7418, Perplexity: 5.7076, Time: 0.21s
2025-04-25 23:49:48,273 - INFO - Batch: 130/141, Loss: 1.7346, Perplexity: 5.6666, Time: 0.21s
2025-04-25 23:49:48,542 - INFO - Batch: 140/141, Loss: 1.7293, Perplexity: 5.6365, Time: 0.27s
2025-04-25 23:49:49,098 - INFO - Validation  Loss: 1.8985, Perplexity: 6.6759, Time: 0.53s
2025-04-25 23:49:49,098 - INFO - Epoch 48/300 - Train Loss: 1.7399, Val Loss: 1.8985
2025-04-25 23:49:49,309 - INFO - Batch: 10/141, Loss: 1.7368, Perplexity: 5.6791, Time: 0.21s
2025-04-25 23:49:49,521 - INFO - Batch: 20/141, Loss: 1.7395, Perplexity: 5.6944, Time: 0.21s
2025-04-25 23:49:49,801 - INFO - Batch: 30/141, Loss: 1.7413, Perplexity: 5.7050, Time: 0.28s
2025-04-25 23:49:50,017 - INFO - Batch: 40/141, Loss: 1.7351, Perplexity: 5.6695, Time: 0.22s
2025-04-25 23:49:50,239 - INFO - Batch: 50/141, Loss: 1.7369, Perplexity: 5.6798, Time: 0.22s
2025-04-25 23:49:50,455 - INFO - Batch: 60/141, Loss: 1.7359, Perplexity: 5.6740, Time: 0.22s
2025-04-25 23:49:50,703 - INFO - Batch: 70/141, Loss: 1.7516, Perplexity: 5.7637, Time: 0.25s
2025-04-25 23:49:50,921 - INFO - Batch: 80/141, Loss: 1.7282, Perplexity: 5.6304, Time: 0.22s
2025-04-25 23:49:51,139 - INFO - Batch: 90/141, Loss: 1.7462, Perplexity: 5.7325, Time: 0.22s
2025-04-25 23:49:51,415 - INFO - Batch: 100/141, Loss: 1.7304, Perplexity: 5.6427, Time: 0.28s
2025-04-25 23:49:51,623 - INFO - Batch: 110/141, Loss: 1.7294, Perplexity: 5.6372, Time: 0.21s
2025-04-25 23:49:51,845 - INFO - Batch: 120/141, Loss: 1.7406, Perplexity: 5.7008, Time: 0.22s
2025-04-25 23:49:52,058 - INFO - Batch: 130/141, Loss: 1.7542, Perplexity: 5.7790, Time: 0.21s
2025-04-25 23:49:52,307 - INFO - Batch: 140/141, Loss: 1.7348, Perplexity: 5.6680, Time: 0.25s
2025-04-25 23:49:52,926 - INFO - Validation  Loss: 1.8964, Perplexity: 6.6616, Time: 0.59s
2025-04-25 23:49:52,927 - INFO - Epoch 49/300 - Train Loss: 1.7387, Val Loss: 1.8964
2025-04-25 23:49:53,132 - INFO - Batch: 10/141, Loss: 1.7452, Perplexity: 5.7270, Time: 0.21s
2025-04-25 23:49:53,341 - INFO - Batch: 20/141, Loss: 1.7345, Perplexity: 5.6662, Time: 0.21s
2025-04-25 23:49:53,549 - INFO - Batch: 30/141, Loss: 1.7349, Perplexity: 5.6685, Time: 0.21s
2025-04-25 23:49:53,767 - INFO - Batch: 40/141, Loss: 1.7343, Perplexity: 5.6648, Time: 0.22s
2025-04-25 23:49:53,985 - INFO - Batch: 50/141, Loss: 1.7434, Perplexity: 5.7166, Time: 0.22s
2025-04-25 23:49:54,200 - INFO - Batch: 60/141, Loss: 1.7506, Perplexity: 5.7579, Time: 0.21s
2025-04-25 23:49:54,468 - INFO - Batch: 70/141, Loss: 1.7414, Perplexity: 5.7053, Time: 0.27s
2025-04-25 23:49:54,673 - INFO - Batch: 80/141, Loss: 1.7380, Perplexity: 5.6860, Time: 0.20s
2025-04-25 23:49:54,884 - INFO - Batch: 90/141, Loss: 1.7342, Perplexity: 5.6641, Time: 0.21s
2025-04-25 23:49:55,100 - INFO - Batch: 100/141, Loss: 1.7284, Perplexity: 5.6315, Time: 0.22s
2025-04-25 23:49:55,320 - INFO - Batch: 110/141, Loss: 1.7432, Perplexity: 5.7158, Time: 0.22s
2025-04-25 23:49:55,532 - INFO - Batch: 120/141, Loss: 1.7412, Perplexity: 5.7041, Time: 0.21s
2025-04-25 23:49:55,747 - INFO - Batch: 130/141, Loss: 1.7416, Perplexity: 5.7067, Time: 0.22s
2025-04-25 23:49:55,967 - INFO - Batch: 140/141, Loss: 1.7332, Perplexity: 5.6588, Time: 0.22s
2025-04-25 23:49:56,574 - INFO - Validation  Loss: 1.8964, Perplexity: 6.6617, Time: 0.58s
2025-04-25 23:49:56,574 - INFO - Epoch 50/300 - Train Loss: 1.7377, Val Loss: 1.8964
2025-04-25 23:49:56,784 - INFO - Batch: 10/141, Loss: 1.7250, Perplexity: 5.6126, Time: 0.21s
2025-04-25 23:49:56,992 - INFO - Batch: 20/141, Loss: 1.7334, Perplexity: 5.6601, Time: 0.21s
2025-04-25 23:49:57,207 - INFO - Batch: 30/141, Loss: 1.7542, Perplexity: 5.7789, Time: 0.22s
2025-04-25 23:49:57,474 - INFO - Batch: 40/141, Loss: 1.7444, Perplexity: 5.7226, Time: 0.27s
2025-04-25 23:49:57,684 - INFO - Batch: 50/141, Loss: 1.7518, Perplexity: 5.7650, Time: 0.21s
2025-04-25 23:49:57,895 - INFO - Batch: 60/141, Loss: 1.7329, Perplexity: 5.6571, Time: 0.21s
2025-04-25 23:49:58,109 - INFO - Batch: 70/141, Loss: 1.7270, Perplexity: 5.6235, Time: 0.21s
2025-04-25 23:49:58,324 - INFO - Batch: 80/141, Loss: 1.7417, Perplexity: 5.7068, Time: 0.21s
2025-04-25 23:49:58,537 - INFO - Batch: 90/141, Loss: 1.7375, Perplexity: 5.6830, Time: 0.21s
2025-04-25 23:49:58,751 - INFO - Batch: 100/141, Loss: 1.7423, Perplexity: 5.7103, Time: 0.21s
2025-04-25 23:49:59,022 - INFO - Batch: 110/141, Loss: 1.7354, Perplexity: 5.6710, Time: 0.27s
2025-04-25 23:49:59,232 - INFO - Batch: 120/141, Loss: 1.7344, Perplexity: 5.6656, Time: 0.21s
2025-04-25 23:49:59,438 - INFO - Batch: 130/141, Loss: 1.7387, Perplexity: 5.6902, Time: 0.21s
2025-04-25 23:49:59,643 - INFO - Batch: 140/141, Loss: 1.7282, Perplexity: 5.6305, Time: 0.20s
2025-04-25 23:50:00,281 - INFO - Validation  Loss: 1.8959, Perplexity: 6.6588, Time: 0.61s
2025-04-25 23:50:00,281 - INFO - Epoch 51/300 - Train Loss: 1.7368, Val Loss: 1.8959
2025-04-25 23:50:00,486 - INFO - Batch: 10/141, Loss: 1.7313, Perplexity: 5.6481, Time: 0.20s
2025-04-25 23:50:00,699 - INFO - Batch: 20/141, Loss: 1.7280, Perplexity: 5.6295, Time: 0.21s
2025-04-25 23:50:00,905 - INFO - Batch: 30/141, Loss: 1.7248, Perplexity: 5.6112, Time: 0.21s
2025-04-25 23:50:01,115 - INFO - Batch: 40/141, Loss: 1.7241, Perplexity: 5.6072, Time: 0.21s
2025-04-25 23:50:01,329 - INFO - Batch: 50/141, Loss: 1.7468, Perplexity: 5.7362, Time: 0.21s
2025-04-25 23:50:01,540 - INFO - Batch: 60/141, Loss: 1.7387, Perplexity: 5.6900, Time: 0.21s
2025-04-25 23:50:01,747 - INFO - Batch: 70/141, Loss: 1.7448, Perplexity: 5.7250, Time: 0.21s
2025-04-25 23:50:02,021 - INFO - Batch: 80/141, Loss: 1.7567, Perplexity: 5.7932, Time: 0.27s
2025-04-25 23:50:02,232 - INFO - Batch: 90/141, Loss: 1.7434, Perplexity: 5.7166, Time: 0.21s
2025-04-25 23:50:02,441 - INFO - Batch: 100/141, Loss: 1.7384, Perplexity: 5.6882, Time: 0.21s
2025-04-25 23:50:02,652 - INFO - Batch: 110/141, Loss: 1.7369, Perplexity: 5.6795, Time: 0.21s
2025-04-25 23:50:02,865 - INFO - Batch: 120/141, Loss: 1.7422, Perplexity: 5.7097, Time: 0.21s
2025-04-25 23:50:03,078 - INFO - Batch: 130/141, Loss: 1.7440, Perplexity: 5.7203, Time: 0.21s
2025-04-25 23:50:03,294 - INFO - Batch: 140/141, Loss: 1.7391, Perplexity: 5.6923, Time: 0.22s
2025-04-25 23:50:03,910 - INFO - Validation  Loss: 1.8972, Perplexity: 6.6670, Time: 0.59s
2025-04-25 23:50:03,910 - INFO - Epoch 52/300 - Train Loss: 1.7359, Val Loss: 1.8972
2025-04-25 23:50:04,122 - INFO - Batch: 10/141, Loss: 1.7333, Perplexity: 5.6592, Time: 0.21s
2025-04-25 23:50:04,373 - INFO - Batch: 20/141, Loss: 1.7440, Perplexity: 5.7202, Time: 0.25s
2025-04-25 23:50:04,591 - INFO - Batch: 30/141, Loss: 1.7420, Perplexity: 5.7088, Time: 0.22s
2025-04-25 23:50:04,861 - INFO - Batch: 40/141, Loss: 1.7377, Perplexity: 5.6845, Time: 0.27s
2025-04-25 23:50:05,067 - INFO - Batch: 50/141, Loss: 1.7385, Perplexity: 5.6889, Time: 0.21s
2025-04-25 23:50:05,281 - INFO - Batch: 60/141, Loss: 1.7276, Perplexity: 5.6273, Time: 0.21s
2025-04-25 23:50:05,495 - INFO - Batch: 70/141, Loss: 1.7313, Perplexity: 5.6483, Time: 0.21s
2025-04-25 23:50:05,707 - INFO - Batch: 80/141, Loss: 1.7290, Perplexity: 5.6352, Time: 0.21s
2025-04-25 23:50:05,923 - INFO - Batch: 90/141, Loss: 1.7302, Perplexity: 5.6419, Time: 0.22s
2025-04-25 23:50:06,139 - INFO - Batch: 100/141, Loss: 1.7341, Perplexity: 5.6641, Time: 0.22s
2025-04-25 23:50:06,357 - INFO - Batch: 110/141, Loss: 1.7290, Perplexity: 5.6350, Time: 0.22s
2025-04-25 23:50:06,629 - INFO - Batch: 120/141, Loss: 1.7234, Perplexity: 5.6037, Time: 0.27s
2025-04-25 23:50:06,843 - INFO - Batch: 130/141, Loss: 1.7399, Perplexity: 5.6967, Time: 0.21s
2025-04-25 23:50:07,052 - INFO - Batch: 140/141, Loss: 1.7188, Perplexity: 5.5780, Time: 0.21s
2025-04-25 23:50:07,611 - INFO - Validation  Loss: 1.8951, Perplexity: 6.6532, Time: 0.53s
2025-04-25 23:50:07,611 - INFO - Epoch 53/300 - Train Loss: 1.7351, Val Loss: 1.8951
2025-04-25 23:50:07,885 - INFO - Batch: 10/141, Loss: 1.7375, Perplexity: 5.6829, Time: 0.27s
2025-04-25 23:50:08,099 - INFO - Batch: 20/141, Loss: 1.7440, Perplexity: 5.7200, Time: 0.21s
2025-04-25 23:50:08,314 - INFO - Batch: 30/141, Loss: 1.7391, Perplexity: 5.6923, Time: 0.21s
2025-04-25 23:50:08,524 - INFO - Batch: 40/141, Loss: 1.7259, Perplexity: 5.6178, Time: 0.21s
2025-04-25 23:50:08,736 - INFO - Batch: 50/141, Loss: 1.7255, Perplexity: 5.6152, Time: 0.21s
2025-04-25 23:50:08,952 - INFO - Batch: 60/141, Loss: 1.7398, Perplexity: 5.6964, Time: 0.22s
2025-04-25 23:50:09,167 - INFO - Batch: 70/141, Loss: 1.7261, Perplexity: 5.6187, Time: 0.22s
2025-04-25 23:50:09,386 - INFO - Batch: 80/141, Loss: 1.7371, Perplexity: 5.6811, Time: 0.22s
2025-04-25 23:50:09,653 - INFO - Batch: 90/141, Loss: 1.7411, Perplexity: 5.7034, Time: 0.27s
2025-04-25 23:50:09,867 - INFO - Batch: 100/141, Loss: 1.7098, Perplexity: 5.5281, Time: 0.21s
2025-04-25 23:50:10,092 - INFO - Batch: 110/141, Loss: 1.7401, Perplexity: 5.6980, Time: 0.23s
2025-04-25 23:50:10,314 - INFO - Batch: 120/141, Loss: 1.7502, Perplexity: 5.7559, Time: 0.22s
2025-04-25 23:50:10,528 - INFO - Batch: 130/141, Loss: 1.7279, Perplexity: 5.6287, Time: 0.21s
2025-04-25 23:50:10,748 - INFO - Batch: 140/141, Loss: 1.7295, Perplexity: 5.6376, Time: 0.22s
2025-04-25 23:50:11,362 - INFO - Validation  Loss: 1.8954, Perplexity: 6.6552, Time: 0.59s
2025-04-25 23:50:11,362 - INFO - Epoch 54/300 - Train Loss: 1.7343, Val Loss: 1.8954
2025-04-25 23:50:11,570 - INFO - Batch: 10/141, Loss: 1.7243, Perplexity: 5.6085, Time: 0.21s
2025-04-25 23:50:11,781 - INFO - Batch: 20/141, Loss: 1.7319, Perplexity: 5.6515, Time: 0.21s
2025-04-25 23:50:11,998 - INFO - Batch: 30/141, Loss: 1.7398, Perplexity: 5.6962, Time: 0.22s
2025-04-25 23:50:12,218 - INFO - Batch: 40/141, Loss: 1.7489, Perplexity: 5.7484, Time: 0.22s
2025-04-25 23:50:12,494 - INFO - Batch: 50/141, Loss: 1.7261, Perplexity: 5.6188, Time: 0.28s
2025-04-25 23:50:12,701 - INFO - Batch: 60/141, Loss: 1.7330, Perplexity: 5.6574, Time: 0.21s
2025-04-25 23:50:12,911 - INFO - Batch: 70/141, Loss: 1.7301, Perplexity: 5.6410, Time: 0.21s
2025-04-25 23:50:13,124 - INFO - Batch: 80/141, Loss: 1.7413, Perplexity: 5.7048, Time: 0.21s
2025-04-25 23:50:13,340 - INFO - Batch: 90/141, Loss: 1.7219, Perplexity: 5.5949, Time: 0.22s
2025-04-25 23:50:13,552 - INFO - Batch: 100/141, Loss: 1.7295, Perplexity: 5.6380, Time: 0.21s
2025-04-25 23:50:13,765 - INFO - Batch: 110/141, Loss: 1.7259, Perplexity: 5.6178, Time: 0.21s
2025-04-25 23:50:13,978 - INFO - Batch: 120/141, Loss: 1.7346, Perplexity: 5.6669, Time: 0.21s
2025-04-25 23:50:14,260 - INFO - Batch: 130/141, Loss: 1.7244, Perplexity: 5.6092, Time: 0.28s
2025-04-25 23:50:14,465 - INFO - Batch: 140/141, Loss: 1.7335, Perplexity: 5.6605, Time: 0.21s
2025-04-25 23:50:15,016 - INFO - Validation  Loss: 1.8942, Perplexity: 6.6472, Time: 0.53s
2025-04-25 23:50:15,016 - INFO - Epoch 55/300 - Train Loss: 1.7335, Val Loss: 1.8942
2025-04-25 23:50:15,234 - INFO - Batch: 10/141, Loss: 1.7290, Perplexity: 5.6353, Time: 0.22s
2025-04-25 23:50:15,503 - INFO - Batch: 20/141, Loss: 1.7343, Perplexity: 5.6647, Time: 0.27s
2025-04-25 23:50:15,710 - INFO - Batch: 30/141, Loss: 1.7175, Perplexity: 5.5705, Time: 0.21s
2025-04-25 23:50:15,924 - INFO - Batch: 40/141, Loss: 1.7389, Perplexity: 5.6912, Time: 0.21s
2025-04-25 23:50:16,141 - INFO - Batch: 50/141, Loss: 1.7351, Perplexity: 5.6697, Time: 0.22s
2025-04-25 23:50:16,358 - INFO - Batch: 60/141, Loss: 1.7232, Perplexity: 5.6025, Time: 0.22s
2025-04-25 23:50:16,569 - INFO - Batch: 70/141, Loss: 1.7332, Perplexity: 5.6586, Time: 0.21s
2025-04-25 23:50:16,783 - INFO - Batch: 80/141, Loss: 1.7552, Perplexity: 5.7845, Time: 0.21s
2025-04-25 23:50:16,996 - INFO - Batch: 90/141, Loss: 1.7267, Perplexity: 5.6223, Time: 0.21s
2025-04-25 23:50:17,267 - INFO - Batch: 100/141, Loss: 1.7303, Perplexity: 5.6424, Time: 0.27s
2025-04-25 23:50:17,474 - INFO - Batch: 110/141, Loss: 1.7278, Perplexity: 5.6280, Time: 0.21s
2025-04-25 23:50:17,684 - INFO - Batch: 120/141, Loss: 1.7329, Perplexity: 5.6572, Time: 0.21s
2025-04-25 23:50:17,900 - INFO - Batch: 130/141, Loss: 1.7206, Perplexity: 5.5881, Time: 0.22s
2025-04-25 23:50:18,117 - INFO - Batch: 140/141, Loss: 1.7397, Perplexity: 5.6957, Time: 0.22s
2025-04-25 23:50:18,740 - INFO - Validation  Loss: 1.8943, Perplexity: 6.6481, Time: 0.60s
2025-04-25 23:50:18,740 - INFO - Epoch 56/300 - Train Loss: 1.7328, Val Loss: 1.8943
2025-04-25 23:50:18,952 - INFO - Batch: 10/141, Loss: 1.7363, Perplexity: 5.6765, Time: 0.21s
2025-04-25 23:50:19,161 - INFO - Batch: 20/141, Loss: 1.7358, Perplexity: 5.6737, Time: 0.21s
2025-04-25 23:50:19,376 - INFO - Batch: 30/141, Loss: 1.7386, Perplexity: 5.6894, Time: 0.21s
2025-04-25 23:50:19,585 - INFO - Batch: 40/141, Loss: 1.7232, Perplexity: 5.6026, Time: 0.21s
2025-04-25 23:50:19,795 - INFO - Batch: 50/141, Loss: 1.7400, Perplexity: 5.6971, Time: 0.21s
2025-04-25 23:50:20,071 - INFO - Batch: 60/141, Loss: 1.7290, Perplexity: 5.6353, Time: 0.28s
2025-04-25 23:50:20,290 - INFO - Batch: 70/141, Loss: 1.7244, Perplexity: 5.6091, Time: 0.22s
2025-04-25 23:50:20,495 - INFO - Batch: 80/141, Loss: 1.7287, Perplexity: 5.6332, Time: 0.20s
2025-04-25 23:50:20,703 - INFO - Batch: 90/141, Loss: 1.7219, Perplexity: 5.5953, Time: 0.21s
2025-04-25 23:50:20,918 - INFO - Batch: 100/141, Loss: 1.7365, Perplexity: 5.6777, Time: 0.21s
2025-04-25 23:50:21,132 - INFO - Batch: 110/141, Loss: 1.7400, Perplexity: 5.6974, Time: 0.21s
2025-04-25 23:50:21,348 - INFO - Batch: 120/141, Loss: 1.7352, Perplexity: 5.6702, Time: 0.22s
2025-04-25 23:50:21,558 - INFO - Batch: 130/141, Loss: 1.7225, Perplexity: 5.5983, Time: 0.21s
2025-04-25 23:50:21,823 - INFO - Batch: 140/141, Loss: 1.7418, Perplexity: 5.7075, Time: 0.27s
2025-04-25 23:50:22,415 - INFO - Validation  Loss: 1.8939, Perplexity: 6.6454, Time: 0.57s
2025-04-25 23:50:22,416 - INFO - Epoch 57/300 - Train Loss: 1.7322, Val Loss: 1.8939
2025-04-25 23:50:22,625 - INFO - Batch: 10/141, Loss: 1.7154, Perplexity: 5.5591, Time: 0.21s
2025-04-25 23:50:22,834 - INFO - Batch: 20/141, Loss: 1.7214, Perplexity: 5.5924, Time: 0.21s
2025-04-25 23:50:23,103 - INFO - Batch: 30/141, Loss: 1.7447, Perplexity: 5.7241, Time: 0.27s
2025-04-25 23:50:23,316 - INFO - Batch: 40/141, Loss: 1.7269, Perplexity: 5.6232, Time: 0.21s
2025-04-25 23:50:23,521 - INFO - Batch: 50/141, Loss: 1.7338, Perplexity: 5.6621, Time: 0.20s
2025-04-25 23:50:23,732 - INFO - Batch: 60/141, Loss: 1.7165, Perplexity: 5.5651, Time: 0.21s
2025-04-25 23:50:23,943 - INFO - Batch: 70/141, Loss: 1.7458, Perplexity: 5.7307, Time: 0.21s
2025-04-25 23:50:24,158 - INFO - Batch: 80/141, Loss: 1.7315, Perplexity: 5.6490, Time: 0.21s
2025-04-25 23:50:24,377 - INFO - Batch: 90/141, Loss: 1.7307, Perplexity: 5.6445, Time: 0.22s
2025-04-25 23:50:24,587 - INFO - Batch: 100/141, Loss: 1.7296, Perplexity: 5.6386, Time: 0.21s
2025-04-25 23:50:24,853 - INFO - Batch: 110/141, Loss: 1.7239, Perplexity: 5.6065, Time: 0.27s
2025-04-25 23:50:25,062 - INFO - Batch: 120/141, Loss: 1.7280, Perplexity: 5.6296, Time: 0.21s
2025-04-25 23:50:25,300 - INFO - Batch: 130/141, Loss: 1.7274, Perplexity: 5.6263, Time: 0.24s
2025-04-25 23:50:25,529 - INFO - Batch: 140/141, Loss: 1.7361, Perplexity: 5.6751, Time: 0.23s
2025-04-25 23:50:26,161 - INFO - Validation  Loss: 1.8946, Perplexity: 6.6502, Time: 0.61s
2025-04-25 23:50:26,161 - INFO - Epoch 58/300 - Train Loss: 1.7315, Val Loss: 1.8946
2025-04-25 23:50:26,376 - INFO - Batch: 10/141, Loss: 1.7296, Perplexity: 5.6386, Time: 0.21s
2025-04-25 23:50:26,582 - INFO - Batch: 20/141, Loss: 1.7371, Perplexity: 5.6810, Time: 0.21s
2025-04-25 23:50:26,796 - INFO - Batch: 30/141, Loss: 1.7419, Perplexity: 5.7083, Time: 0.21s
2025-04-25 23:50:27,009 - INFO - Batch: 40/141, Loss: 1.7311, Perplexity: 5.6466, Time: 0.21s
2025-04-25 23:50:27,223 - INFO - Batch: 50/141, Loss: 1.7417, Perplexity: 5.7071, Time: 0.21s
2025-04-25 23:50:27,437 - INFO - Batch: 60/141, Loss: 1.7374, Perplexity: 5.6824, Time: 0.21s
2025-04-25 23:50:27,705 - INFO - Batch: 70/141, Loss: 1.7263, Perplexity: 5.6196, Time: 0.27s
2025-04-25 23:50:27,909 - INFO - Batch: 80/141, Loss: 1.7167, Perplexity: 5.5663, Time: 0.20s
2025-04-25 23:50:28,116 - INFO - Batch: 90/141, Loss: 1.7303, Perplexity: 5.6426, Time: 0.21s
2025-04-25 23:50:28,341 - INFO - Batch: 100/141, Loss: 1.7103, Perplexity: 5.5304, Time: 0.23s
2025-04-25 23:50:28,554 - INFO - Batch: 110/141, Loss: 1.7473, Perplexity: 5.7393, Time: 0.21s
2025-04-25 23:50:28,763 - INFO - Batch: 120/141, Loss: 1.7169, Perplexity: 5.5675, Time: 0.21s
2025-04-25 23:50:28,972 - INFO - Batch: 130/141, Loss: 1.7262, Perplexity: 5.6191, Time: 0.21s
2025-04-25 23:50:29,187 - INFO - Batch: 140/141, Loss: 1.7377, Perplexity: 5.6841, Time: 0.21s
2025-04-25 23:50:29,793 - INFO - Validation  Loss: 1.8922, Perplexity: 6.6342, Time: 0.58s
2025-04-25 23:50:29,794 - INFO - Epoch 59/300 - Train Loss: 1.7310, Val Loss: 1.8922
2025-04-25 23:50:30,000 - INFO - Batch: 10/141, Loss: 1.7268, Perplexity: 5.6228, Time: 0.21s
2025-04-25 23:50:30,221 - INFO - Batch: 20/141, Loss: 1.7315, Perplexity: 5.6494, Time: 0.22s
2025-04-25 23:50:30,445 - INFO - Batch: 30/141, Loss: 1.7330, Perplexity: 5.6578, Time: 0.22s
2025-04-25 23:50:30,712 - INFO - Batch: 40/141, Loss: 1.7303, Perplexity: 5.6423, Time: 0.27s
2025-04-25 23:50:30,923 - INFO - Batch: 50/141, Loss: 1.7349, Perplexity: 5.6683, Time: 0.21s
2025-04-25 23:50:31,136 - INFO - Batch: 60/141, Loss: 1.7299, Perplexity: 5.6400, Time: 0.21s
2025-04-25 23:50:31,352 - INFO - Batch: 70/141, Loss: 1.7285, Perplexity: 5.6323, Time: 0.22s
2025-04-25 23:50:31,566 - INFO - Batch: 80/141, Loss: 1.7244, Perplexity: 5.6089, Time: 0.21s
2025-04-25 23:50:31,779 - INFO - Batch: 90/141, Loss: 1.7241, Perplexity: 5.6077, Time: 0.21s
2025-04-25 23:50:31,986 - INFO - Batch: 100/141, Loss: 1.7258, Perplexity: 5.6168, Time: 0.21s
2025-04-25 23:50:32,263 - INFO - Batch: 110/141, Loss: 1.7425, Perplexity: 5.7115, Time: 0.28s
2025-04-25 23:50:32,473 - INFO - Batch: 120/141, Loss: 1.7371, Perplexity: 5.6808, Time: 0.21s
2025-04-25 23:50:32,687 - INFO - Batch: 130/141, Loss: 1.7256, Perplexity: 5.6161, Time: 0.21s
2025-04-25 23:50:32,901 - INFO - Batch: 140/141, Loss: 1.7328, Perplexity: 5.6564, Time: 0.21s
2025-04-25 23:50:33,524 - INFO - Validation  Loss: 1.8925, Perplexity: 6.6361, Time: 0.60s
2025-04-25 23:50:33,524 - INFO - Epoch 60/300 - Train Loss: 1.7303, Val Loss: 1.8925
2025-04-25 23:50:33,730 - INFO - Batch: 10/141, Loss: 1.7271, Perplexity: 5.6242, Time: 0.21s
2025-04-25 23:50:33,938 - INFO - Batch: 20/141, Loss: 1.7241, Perplexity: 5.6077, Time: 0.21s
2025-04-25 23:50:34,152 - INFO - Batch: 30/141, Loss: 1.7489, Perplexity: 5.7482, Time: 0.21s
2025-04-25 23:50:34,369 - INFO - Batch: 40/141, Loss: 1.7419, Perplexity: 5.7079, Time: 0.22s
2025-04-25 23:50:34,589 - INFO - Batch: 50/141, Loss: 1.7303, Perplexity: 5.6425, Time: 0.22s
2025-04-25 23:50:34,803 - INFO - Batch: 60/141, Loss: 1.7349, Perplexity: 5.6685, Time: 0.21s
2025-04-25 23:50:35,020 - INFO - Batch: 70/141, Loss: 1.7298, Perplexity: 5.6393, Time: 0.22s
2025-04-25 23:50:35,300 - INFO - Batch: 80/141, Loss: 1.7279, Perplexity: 5.6290, Time: 0.28s
2025-04-25 23:50:35,514 - INFO - Batch: 90/141, Loss: 1.7270, Perplexity: 5.6235, Time: 0.21s
2025-04-25 23:50:35,725 - INFO - Batch: 100/141, Loss: 1.7375, Perplexity: 5.6833, Time: 0.21s
2025-04-25 23:50:35,942 - INFO - Batch: 110/141, Loss: 1.7347, Perplexity: 5.6675, Time: 0.22s
2025-04-25 23:50:36,159 - INFO - Batch: 120/141, Loss: 1.7424, Perplexity: 5.7111, Time: 0.22s
2025-04-25 23:50:36,385 - INFO - Batch: 130/141, Loss: 1.7194, Perplexity: 5.5813, Time: 0.23s
2025-04-25 23:50:36,599 - INFO - Batch: 140/141, Loss: 1.7319, Perplexity: 5.6517, Time: 0.21s
2025-04-25 23:50:37,194 - INFO - Validation  Loss: 1.8924, Perplexity: 6.6355, Time: 0.57s
2025-04-25 23:50:37,194 - INFO - Epoch 61/300 - Train Loss: 1.7298, Val Loss: 1.8924
2025-04-25 23:50:37,403 - INFO - Batch: 10/141, Loss: 1.7218, Perplexity: 5.5945, Time: 0.21s
2025-04-25 23:50:37,614 - INFO - Batch: 20/141, Loss: 1.7335, Perplexity: 5.6603, Time: 0.21s
2025-04-25 23:50:37,833 - INFO - Batch: 30/141, Loss: 1.7514, Perplexity: 5.7629, Time: 0.22s
2025-04-25 23:50:38,047 - INFO - Batch: 40/141, Loss: 1.7334, Perplexity: 5.6599, Time: 0.21s
2025-04-25 23:50:38,320 - INFO - Batch: 50/141, Loss: 1.7349, Perplexity: 5.6686, Time: 0.27s
2025-04-25 23:50:38,534 - INFO - Batch: 60/141, Loss: 1.7322, Perplexity: 5.6530, Time: 0.21s
2025-04-25 23:50:38,778 - INFO - Batch: 70/141, Loss: 1.7368, Perplexity: 5.6793, Time: 0.24s
2025-04-25 23:50:38,992 - INFO - Batch: 80/141, Loss: 1.7282, Perplexity: 5.6308, Time: 0.21s
2025-04-25 23:50:39,201 - INFO - Batch: 90/141, Loss: 1.7328, Perplexity: 5.6563, Time: 0.21s
2025-04-25 23:50:39,413 - INFO - Batch: 100/141, Loss: 1.7278, Perplexity: 5.6281, Time: 0.21s
2025-04-25 23:50:39,625 - INFO - Batch: 110/141, Loss: 1.7431, Perplexity: 5.7148, Time: 0.21s
2025-04-25 23:50:39,898 - INFO - Batch: 120/141, Loss: 1.7262, Perplexity: 5.6195, Time: 0.27s
2025-04-25 23:50:40,124 - INFO - Batch: 130/141, Loss: 1.7171, Perplexity: 5.5683, Time: 0.23s
2025-04-25 23:50:40,355 - INFO - Batch: 140/141, Loss: 1.7316, Perplexity: 5.6498, Time: 0.23s
2025-04-25 23:50:41,065 - INFO - Validation  Loss: 1.8916, Perplexity: 6.6297, Time: 0.68s
2025-04-25 23:50:41,065 - INFO - Epoch 62/300 - Train Loss: 1.7293, Val Loss: 1.8916
2025-04-25 23:50:41,377 - INFO - Batch: 10/141, Loss: 1.7185, Perplexity: 5.5762, Time: 0.31s
2025-04-25 23:50:41,601 - INFO - Batch: 20/141, Loss: 1.7258, Perplexity: 5.6168, Time: 0.22s
2025-04-25 23:50:41,828 - INFO - Batch: 30/141, Loss: 1.7329, Perplexity: 5.6568, Time: 0.23s
2025-04-25 23:50:42,059 - INFO - Batch: 40/141, Loss: 1.7288, Perplexity: 5.6339, Time: 0.23s
2025-04-25 23:50:42,397 - INFO - Batch: 50/141, Loss: 1.7233, Perplexity: 5.6030, Time: 0.34s
2025-04-25 23:50:42,651 - INFO - Batch: 60/141, Loss: 1.7339, Perplexity: 5.6625, Time: 0.25s
2025-04-25 23:50:43,009 - INFO - Batch: 70/141, Loss: 1.7274, Perplexity: 5.6260, Time: 0.36s
2025-04-25 23:50:43,326 - INFO - Batch: 80/141, Loss: 1.7329, Perplexity: 5.6572, Time: 0.32s
2025-04-25 23:50:43,706 - INFO - Batch: 90/141, Loss: 1.7212, Perplexity: 5.5912, Time: 0.38s
2025-04-25 23:50:44,002 - INFO - Batch: 100/141, Loss: 1.7411, Perplexity: 5.7038, Time: 0.30s
2025-04-25 23:50:44,267 - INFO - Batch: 110/141, Loss: 1.7445, Perplexity: 5.7229, Time: 0.26s
2025-04-25 23:50:44,531 - INFO - Batch: 120/141, Loss: 1.7185, Perplexity: 5.5762, Time: 0.26s
2025-04-25 23:50:44,774 - INFO - Batch: 130/141, Loss: 1.7319, Perplexity: 5.6513, Time: 0.24s
2025-04-25 23:50:45,010 - INFO - Batch: 140/141, Loss: 1.7194, Perplexity: 5.5812, Time: 0.24s
2025-04-25 23:50:45,662 - INFO - Validation  Loss: 1.8914, Perplexity: 6.6284, Time: 0.62s
2025-04-25 23:50:45,662 - INFO - Epoch 63/300 - Train Loss: 1.7287, Val Loss: 1.8914
2025-04-25 23:50:45,893 - INFO - Batch: 10/141, Loss: 1.7180, Perplexity: 5.5736, Time: 0.23s
2025-04-25 23:50:46,121 - INFO - Batch: 20/141, Loss: 1.7209, Perplexity: 5.5897, Time: 0.23s
2025-04-25 23:50:46,361 - INFO - Batch: 30/141, Loss: 1.7213, Perplexity: 5.5916, Time: 0.24s
2025-04-25 23:50:46,628 - INFO - Batch: 40/141, Loss: 1.7287, Perplexity: 5.6332, Time: 0.27s
2025-04-25 23:50:46,871 - INFO - Batch: 50/141, Loss: 1.7278, Perplexity: 5.6282, Time: 0.24s
2025-04-25 23:50:47,151 - INFO - Batch: 60/141, Loss: 1.7304, Perplexity: 5.6428, Time: 0.28s
2025-04-25 23:50:47,371 - INFO - Batch: 70/141, Loss: 1.7409, Perplexity: 5.7023, Time: 0.22s
2025-04-25 23:50:47,675 - INFO - Batch: 80/141, Loss: 1.7289, Perplexity: 5.6344, Time: 0.30s
2025-04-25 23:50:47,913 - INFO - Batch: 90/141, Loss: 1.7248, Perplexity: 5.6111, Time: 0.24s
2025-04-25 23:50:48,133 - INFO - Batch: 100/141, Loss: 1.7188, Perplexity: 5.5777, Time: 0.22s
2025-04-25 23:50:48,365 - INFO - Batch: 110/141, Loss: 1.7241, Perplexity: 5.6073, Time: 0.23s
2025-04-25 23:50:48,671 - INFO - Batch: 120/141, Loss: 1.7240, Perplexity: 5.6068, Time: 0.31s
2025-04-25 23:50:49,087 - INFO - Batch: 130/141, Loss: 1.7329, Perplexity: 5.6573, Time: 0.42s
2025-04-25 23:50:49,368 - INFO - Batch: 140/141, Loss: 1.7358, Perplexity: 5.6734, Time: 0.28s
2025-04-25 23:50:50,013 - INFO - Validation  Loss: 1.8913, Perplexity: 6.6282, Time: 0.60s
2025-04-25 23:50:50,013 - INFO - Epoch 64/300 - Train Loss: 1.7282, Val Loss: 1.8913
2025-04-25 23:50:50,235 - INFO - Batch: 10/141, Loss: 1.7172, Perplexity: 5.5691, Time: 0.22s
2025-04-25 23:50:50,539 - INFO - Batch: 20/141, Loss: 1.7348, Perplexity: 5.6675, Time: 0.30s
2025-04-25 23:50:50,803 - INFO - Batch: 30/141, Loss: 1.7274, Perplexity: 5.6259, Time: 0.26s
2025-04-25 23:50:51,093 - INFO - Batch: 40/141, Loss: 1.7152, Perplexity: 5.5579, Time: 0.29s
2025-04-25 23:50:51,327 - INFO - Batch: 50/141, Loss: 1.7351, Perplexity: 5.6693, Time: 0.23s
2025-04-25 23:50:51,579 - INFO - Batch: 60/141, Loss: 1.7305, Perplexity: 5.6434, Time: 0.25s
2025-04-25 23:50:51,874 - INFO - Batch: 70/141, Loss: 1.7221, Perplexity: 5.5965, Time: 0.30s
2025-04-25 23:50:52,099 - INFO - Batch: 80/141, Loss: 1.7187, Perplexity: 5.5774, Time: 0.23s
2025-04-25 23:50:52,324 - INFO - Batch: 90/141, Loss: 1.7127, Perplexity: 5.5437, Time: 0.22s
2025-04-25 23:50:52,623 - INFO - Batch: 100/141, Loss: 1.7174, Perplexity: 5.5698, Time: 0.30s
2025-04-25 23:50:52,871 - INFO - Batch: 110/141, Loss: 1.7095, Perplexity: 5.5265, Time: 0.25s
2025-04-25 23:50:53,123 - INFO - Batch: 120/141, Loss: 1.7340, Perplexity: 5.6631, Time: 0.25s
2025-04-25 23:50:53,445 - INFO - Batch: 130/141, Loss: 1.7394, Perplexity: 5.6941, Time: 0.32s
2025-04-25 23:50:53,696 - INFO - Batch: 140/141, Loss: 1.7399, Perplexity: 5.6968, Time: 0.25s
2025-04-25 23:50:54,367 - INFO - Validation  Loss: 1.8901, Perplexity: 6.6198, Time: 0.63s
2025-04-25 23:50:54,367 - INFO - Epoch 65/300 - Train Loss: 1.7278, Val Loss: 1.8901
2025-04-25 23:50:54,653 - INFO - Batch: 10/141, Loss: 1.7394, Perplexity: 5.6938, Time: 0.29s
2025-04-25 23:50:54,919 - INFO - Batch: 20/141, Loss: 1.7192, Perplexity: 5.5799, Time: 0.27s
2025-04-25 23:50:55,176 - INFO - Batch: 30/141, Loss: 1.7298, Perplexity: 5.6394, Time: 0.26s
2025-04-25 23:50:55,411 - INFO - Batch: 40/141, Loss: 1.7144, Perplexity: 5.5532, Time: 0.24s
2025-04-25 23:50:55,643 - INFO - Batch: 50/141, Loss: 1.7283, Perplexity: 5.6310, Time: 0.23s
2025-04-25 23:50:55,959 - INFO - Batch: 60/141, Loss: 1.7435, Perplexity: 5.7172, Time: 0.32s
2025-04-25 23:50:56,229 - INFO - Batch: 70/141, Loss: 1.7355, Perplexity: 5.6716, Time: 0.27s
2025-04-25 23:50:56,466 - INFO - Batch: 80/141, Loss: 1.7166, Perplexity: 5.5658, Time: 0.24s
2025-04-25 23:50:56,696 - INFO - Batch: 90/141, Loss: 1.7272, Perplexity: 5.6246, Time: 0.23s
2025-04-25 23:50:56,960 - INFO - Batch: 100/141, Loss: 1.7321, Perplexity: 5.6527, Time: 0.26s
2025-04-25 23:50:57,236 - INFO - Batch: 110/141, Loss: 1.7302, Perplexity: 5.6419, Time: 0.28s
2025-04-25 23:50:57,468 - INFO - Batch: 120/141, Loss: 1.7301, Perplexity: 5.6412, Time: 0.23s
2025-04-25 23:50:57,702 - INFO - Batch: 130/141, Loss: 1.7263, Perplexity: 5.6200, Time: 0.23s
2025-04-25 23:50:58,010 - INFO - Batch: 140/141, Loss: 1.7251, Perplexity: 5.6128, Time: 0.31s
2025-04-25 23:50:58,751 - INFO - Validation  Loss: 1.8913, Perplexity: 6.6278, Time: 0.70s
2025-04-25 23:50:58,751 - INFO - Epoch 66/300 - Train Loss: 1.7273, Val Loss: 1.8913
2025-04-25 23:50:59,021 - INFO - Batch: 10/141, Loss: 1.7310, Perplexity: 5.6465, Time: 0.27s
2025-04-25 23:50:59,272 - INFO - Batch: 20/141, Loss: 1.7307, Perplexity: 5.6447, Time: 0.25s
2025-04-25 23:50:59,554 - INFO - Batch: 30/141, Loss: 1.7334, Perplexity: 5.6598, Time: 0.28s
2025-04-25 23:50:59,804 - INFO - Batch: 40/141, Loss: 1.7304, Perplexity: 5.6431, Time: 0.25s
2025-04-25 23:51:00,028 - INFO - Batch: 50/141, Loss: 1.7343, Perplexity: 5.6652, Time: 0.22s
2025-04-25 23:51:00,271 - INFO - Batch: 60/141, Loss: 1.7345, Perplexity: 5.6663, Time: 0.24s
2025-04-25 23:51:00,510 - INFO - Batch: 70/141, Loss: 1.7438, Perplexity: 5.7189, Time: 0.24s
2025-04-25 23:51:00,778 - INFO - Batch: 80/141, Loss: 1.7203, Perplexity: 5.5861, Time: 0.27s
2025-04-25 23:51:01,044 - INFO - Batch: 90/141, Loss: 1.7346, Perplexity: 5.6664, Time: 0.27s
2025-04-25 23:51:01,285 - INFO - Batch: 100/141, Loss: 1.7416, Perplexity: 5.7064, Time: 0.24s
2025-04-25 23:51:01,609 - INFO - Batch: 110/141, Loss: 1.7195, Perplexity: 5.5816, Time: 0.32s
2025-04-25 23:51:01,882 - INFO - Batch: 120/141, Loss: 1.7317, Perplexity: 5.6504, Time: 0.27s
2025-04-25 23:51:02,104 - INFO - Batch: 130/141, Loss: 1.7181, Perplexity: 5.5738, Time: 0.22s
2025-04-25 23:51:02,343 - INFO - Batch: 140/141, Loss: 1.7199, Perplexity: 5.5842, Time: 0.24s
2025-04-25 23:51:03,022 - INFO - Validation  Loss: 1.8908, Perplexity: 6.6244, Time: 0.65s
2025-04-25 23:51:03,022 - INFO - Epoch 67/300 - Train Loss: 1.7268, Val Loss: 1.8908
2025-04-25 23:51:03,245 - INFO - Batch: 10/141, Loss: 1.7159, Perplexity: 5.5616, Time: 0.22s
2025-04-25 23:51:03,469 - INFO - Batch: 20/141, Loss: 1.7137, Perplexity: 5.5497, Time: 0.22s
2025-04-25 23:51:03,708 - INFO - Batch: 30/141, Loss: 1.7229, Perplexity: 5.6010, Time: 0.24s
2025-04-25 23:51:03,962 - INFO - Batch: 40/141, Loss: 1.7326, Perplexity: 5.6556, Time: 0.25s
2025-04-25 23:51:04,239 - INFO - Batch: 50/141, Loss: 1.7232, Perplexity: 5.6027, Time: 0.28s
2025-04-25 23:51:04,502 - INFO - Batch: 60/141, Loss: 1.7264, Perplexity: 5.6201, Time: 0.26s
2025-04-25 23:51:04,856 - INFO - Batch: 70/141, Loss: 1.7387, Perplexity: 5.6897, Time: 0.35s
2025-04-25 23:51:05,104 - INFO - Batch: 80/141, Loss: 1.7192, Perplexity: 5.5798, Time: 0.25s
2025-04-25 23:51:05,400 - INFO - Batch: 90/141, Loss: 1.7263, Perplexity: 5.6199, Time: 0.30s
2025-04-25 23:51:05,697 - INFO - Batch: 100/141, Loss: 1.7335, Perplexity: 5.6603, Time: 0.30s
2025-04-25 23:51:05,932 - INFO - Batch: 110/141, Loss: 1.7371, Perplexity: 5.6810, Time: 0.24s
2025-04-25 23:51:06,182 - INFO - Batch: 120/141, Loss: 1.7149, Perplexity: 5.5563, Time: 0.25s
2025-04-25 23:51:06,432 - INFO - Batch: 130/141, Loss: 1.7345, Perplexity: 5.6661, Time: 0.25s
2025-04-25 23:51:06,662 - INFO - Batch: 140/141, Loss: 1.7288, Perplexity: 5.6341, Time: 0.23s
2025-04-25 23:51:07,351 - INFO - Validation  Loss: 1.8904, Perplexity: 6.6221, Time: 0.65s
2025-04-25 23:51:07,351 - INFO - Epoch 68/300 - Train Loss: 1.7264, Val Loss: 1.8904
2025-04-25 23:51:07,575 - INFO - Batch: 10/141, Loss: 1.7366, Perplexity: 5.6782, Time: 0.22s
2025-04-25 23:51:07,804 - INFO - Batch: 20/141, Loss: 1.7211, Perplexity: 5.5905, Time: 0.23s
2025-04-25 23:51:08,037 - INFO - Batch: 30/141, Loss: 1.7181, Perplexity: 5.5740, Time: 0.23s
2025-04-25 23:51:08,321 - INFO - Batch: 40/141, Loss: 1.7230, Perplexity: 5.6012, Time: 0.28s
2025-04-25 23:51:08,543 - INFO - Batch: 50/141, Loss: 1.7343, Perplexity: 5.6648, Time: 0.22s
2025-04-25 23:51:08,817 - INFO - Batch: 60/141, Loss: 1.7256, Perplexity: 5.6159, Time: 0.27s
2025-04-25 23:51:09,133 - INFO - Batch: 70/141, Loss: 1.7250, Perplexity: 5.6125, Time: 0.32s
2025-04-25 23:51:09,436 - INFO - Batch: 80/141, Loss: 1.7320, Perplexity: 5.6517, Time: 0.30s
2025-04-25 23:51:09,707 - INFO - Batch: 90/141, Loss: 1.7309, Perplexity: 5.6458, Time: 0.27s
2025-04-25 23:51:09,948 - INFO - Batch: 100/141, Loss: 1.7272, Perplexity: 5.6248, Time: 0.24s
2025-04-25 23:51:10,191 - INFO - Batch: 110/141, Loss: 1.7153, Perplexity: 5.5584, Time: 0.24s
2025-04-25 23:51:10,503 - INFO - Batch: 120/141, Loss: 1.7310, Perplexity: 5.6464, Time: 0.31s
2025-04-25 23:51:10,756 - INFO - Batch: 130/141, Loss: 1.7548, Perplexity: 5.7825, Time: 0.25s
2025-04-25 23:51:10,989 - INFO - Batch: 140/141, Loss: 1.7305, Perplexity: 5.6435, Time: 0.23s
2025-04-25 23:51:11,593 - INFO - Validation  Loss: 1.8907, Perplexity: 6.6239, Time: 0.57s
2025-04-25 23:51:11,593 - INFO - Epoch 69/300 - Train Loss: 1.7260, Val Loss: 1.8907
2025-04-25 23:51:11,869 - INFO - Batch: 10/141, Loss: 1.7102, Perplexity: 5.5299, Time: 0.28s
2025-04-25 23:51:12,095 - INFO - Batch: 20/141, Loss: 1.7232, Perplexity: 5.6026, Time: 0.23s
2025-04-25 23:51:12,341 - INFO - Batch: 30/141, Loss: 1.7141, Perplexity: 5.5518, Time: 0.25s
2025-04-25 23:51:12,564 - INFO - Batch: 40/141, Loss: 1.7230, Perplexity: 5.6013, Time: 0.22s
2025-04-25 23:51:12,835 - INFO - Batch: 50/141, Loss: 1.7338, Perplexity: 5.6621, Time: 0.27s
2025-04-25 23:51:13,092 - INFO - Batch: 60/141, Loss: 1.7286, Perplexity: 5.6327, Time: 0.26s
2025-04-25 23:51:13,323 - INFO - Batch: 70/141, Loss: 1.7165, Perplexity: 5.5652, Time: 0.23s
2025-04-25 23:51:13,617 - INFO - Batch: 80/141, Loss: 1.7315, Perplexity: 5.6494, Time: 0.29s
2025-04-25 23:51:13,862 - INFO - Batch: 90/141, Loss: 1.7154, Perplexity: 5.5588, Time: 0.24s
2025-04-25 23:51:14,110 - INFO - Batch: 100/141, Loss: 1.7258, Perplexity: 5.6168, Time: 0.25s
2025-04-25 23:51:14,332 - INFO - Batch: 110/141, Loss: 1.7300, Perplexity: 5.6408, Time: 0.22s
2025-04-25 23:51:14,556 - INFO - Batch: 120/141, Loss: 1.7257, Perplexity: 5.6167, Time: 0.22s
2025-04-25 23:51:14,801 - INFO - Batch: 130/141, Loss: 1.7460, Perplexity: 5.7316, Time: 0.25s
2025-04-25 23:51:15,035 - INFO - Batch: 140/141, Loss: 1.7211, Perplexity: 5.5909, Time: 0.23s
2025-04-25 23:51:15,664 - INFO - Validation  Loss: 1.8900, Perplexity: 6.6197, Time: 0.60s
2025-04-25 23:51:15,665 - INFO - Epoch 70/300 - Train Loss: 1.7255, Val Loss: 1.8900
2025-04-25 23:51:15,945 - INFO - Batch: 10/141, Loss: 1.7232, Perplexity: 5.6022, Time: 0.28s
2025-04-25 23:51:16,195 - INFO - Batch: 20/141, Loss: 1.7244, Perplexity: 5.6090, Time: 0.25s
2025-04-25 23:51:16,420 - INFO - Batch: 30/141, Loss: 1.7258, Perplexity: 5.6171, Time: 0.22s
2025-04-25 23:51:16,651 - INFO - Batch: 40/141, Loss: 1.7360, Perplexity: 5.6748, Time: 0.23s
2025-04-25 23:51:16,943 - INFO - Batch: 50/141, Loss: 1.7222, Perplexity: 5.5967, Time: 0.29s
2025-04-25 23:51:17,192 - INFO - Batch: 60/141, Loss: 1.7150, Perplexity: 5.5569, Time: 0.25s
2025-04-25 23:51:17,425 - INFO - Batch: 70/141, Loss: 1.7267, Perplexity: 5.6223, Time: 0.23s
2025-04-25 23:51:17,650 - INFO - Batch: 80/141, Loss: 1.7263, Perplexity: 5.6201, Time: 0.23s
2025-04-25 23:51:17,911 - INFO - Batch: 90/141, Loss: 1.7179, Perplexity: 5.5729, Time: 0.26s
2025-04-25 23:51:18,190 - INFO - Batch: 100/141, Loss: 1.7111, Perplexity: 5.5349, Time: 0.28s
2025-04-25 23:51:18,416 - INFO - Batch: 110/141, Loss: 1.7244, Perplexity: 5.6091, Time: 0.23s
2025-04-25 23:51:18,636 - INFO - Batch: 120/141, Loss: 1.7192, Perplexity: 5.5798, Time: 0.22s
2025-04-25 23:51:19,007 - INFO - Batch: 130/141, Loss: 1.7300, Perplexity: 5.6408, Time: 0.37s
2025-04-25 23:51:19,316 - INFO - Batch: 140/141, Loss: 1.7225, Perplexity: 5.5986, Time: 0.31s
2025-04-25 23:51:20,139 - INFO - Validation  Loss: 1.8893, Perplexity: 6.6145, Time: 0.77s
2025-04-25 23:51:20,139 - INFO - Epoch 71/300 - Train Loss: 1.7252, Val Loss: 1.8893
2025-04-25 23:51:20,479 - INFO - Batch: 10/141, Loss: 1.7210, Perplexity: 5.5903, Time: 0.34s
2025-04-25 23:51:20,710 - INFO - Batch: 20/141, Loss: 1.7162, Perplexity: 5.5633, Time: 0.23s
2025-04-25 23:51:20,948 - INFO - Batch: 30/141, Loss: 1.7251, Perplexity: 5.6132, Time: 0.24s
2025-04-25 23:51:21,306 - INFO - Batch: 40/141, Loss: 1.7338, Perplexity: 5.6621, Time: 0.36s
2025-04-25 23:51:21,663 - INFO - Batch: 50/141, Loss: 1.7155, Perplexity: 5.5594, Time: 0.36s
2025-04-25 23:51:21,949 - INFO - Batch: 60/141, Loss: 1.7386, Perplexity: 5.6895, Time: 0.29s
2025-04-25 23:51:22,250 - INFO - Batch: 70/141, Loss: 1.7186, Perplexity: 5.5769, Time: 0.30s
2025-04-25 23:51:22,539 - INFO - Batch: 80/141, Loss: 1.7372, Perplexity: 5.6816, Time: 0.29s
2025-04-25 23:51:22,878 - INFO - Batch: 90/141, Loss: 1.7159, Perplexity: 5.5616, Time: 0.34s
2025-04-25 23:51:23,141 - INFO - Batch: 100/141, Loss: 1.7219, Perplexity: 5.5952, Time: 0.26s
2025-04-25 23:51:23,422 - INFO - Batch: 110/141, Loss: 1.7226, Perplexity: 5.5991, Time: 0.28s
2025-04-25 23:51:23,684 - INFO - Batch: 120/141, Loss: 1.7290, Perplexity: 5.6348, Time: 0.26s
2025-04-25 23:51:23,976 - INFO - Batch: 130/141, Loss: 1.7244, Perplexity: 5.6090, Time: 0.29s
2025-04-25 23:51:24,226 - INFO - Batch: 140/141, Loss: 1.7294, Perplexity: 5.6374, Time: 0.25s
2025-04-25 23:51:25,096 - INFO - Validation  Loss: 1.8894, Perplexity: 6.6153, Time: 0.83s
2025-04-25 23:51:25,096 - INFO - Epoch 72/300 - Train Loss: 1.7248, Val Loss: 1.8894
2025-04-25 23:51:25,429 - INFO - Batch: 10/141, Loss: 1.7167, Perplexity: 5.5664, Time: 0.33s
2025-04-25 23:51:25,732 - INFO - Batch: 20/141, Loss: 1.7338, Perplexity: 5.6621, Time: 0.30s
2025-04-25 23:51:25,979 - INFO - Batch: 30/141, Loss: 1.7274, Perplexity: 5.6257, Time: 0.25s
2025-04-25 23:51:26,263 - INFO - Batch: 40/141, Loss: 1.7133, Perplexity: 5.5470, Time: 0.28s
2025-04-25 23:51:26,496 - INFO - Batch: 50/141, Loss: 1.7305, Perplexity: 5.6432, Time: 0.23s
2025-04-25 23:51:26,795 - INFO - Batch: 60/141, Loss: 1.7249, Perplexity: 5.6120, Time: 0.30s
2025-04-25 23:51:27,028 - INFO - Batch: 70/141, Loss: 1.7105, Perplexity: 5.5315, Time: 0.23s
2025-04-25 23:51:27,254 - INFO - Batch: 80/141, Loss: 1.7345, Perplexity: 5.6662, Time: 0.23s
2025-04-25 23:51:27,472 - INFO - Batch: 90/141, Loss: 1.7148, Perplexity: 5.5556, Time: 0.22s
2025-04-25 23:51:27,694 - INFO - Batch: 100/141, Loss: 1.7288, Perplexity: 5.6341, Time: 0.22s
2025-04-25 23:51:27,919 - INFO - Batch: 110/141, Loss: 1.7396, Perplexity: 5.6950, Time: 0.23s
2025-04-25 23:51:28,146 - INFO - Batch: 120/141, Loss: 1.7228, Perplexity: 5.6003, Time: 0.23s
2025-04-25 23:51:28,427 - INFO - Batch: 130/141, Loss: 1.7232, Perplexity: 5.6026, Time: 0.28s
2025-04-25 23:51:28,640 - INFO - Batch: 140/141, Loss: 1.7327, Perplexity: 5.6560, Time: 0.21s
2025-04-25 23:51:29,226 - INFO - Validation  Loss: 1.8885, Perplexity: 6.6094, Time: 0.56s
2025-04-25 23:51:29,226 - INFO - Epoch 73/300 - Train Loss: 1.7244, Val Loss: 1.8885
2025-04-25 23:51:29,445 - INFO - Batch: 10/141, Loss: 1.7130, Perplexity: 5.5458, Time: 0.22s
2025-04-25 23:51:29,726 - INFO - Batch: 20/141, Loss: 1.7322, Perplexity: 5.6529, Time: 0.28s
2025-04-25 23:51:29,951 - INFO - Batch: 30/141, Loss: 1.7197, Perplexity: 5.5830, Time: 0.22s
2025-04-25 23:51:30,260 - INFO - Batch: 40/141, Loss: 1.7199, Perplexity: 5.5842, Time: 0.31s
2025-04-25 23:51:30,614 - INFO - Batch: 50/141, Loss: 1.7282, Perplexity: 5.6302, Time: 0.35s
2025-04-25 23:51:30,873 - INFO - Batch: 60/141, Loss: 1.7278, Perplexity: 5.6282, Time: 0.26s
2025-04-25 23:51:31,129 - INFO - Batch: 70/141, Loss: 1.7153, Perplexity: 5.5585, Time: 0.26s
2025-04-25 23:51:31,459 - INFO - Batch: 80/141, Loss: 1.7273, Perplexity: 5.6253, Time: 0.33s
2025-04-25 23:51:31,803 - INFO - Batch: 90/141, Loss: 1.7186, Perplexity: 5.5766, Time: 0.34s
2025-04-25 23:51:32,111 - INFO - Batch: 100/141, Loss: 1.7285, Perplexity: 5.6325, Time: 0.31s
2025-04-25 23:51:32,392 - INFO - Batch: 110/141, Loss: 1.7224, Perplexity: 5.5979, Time: 0.28s
2025-04-25 23:51:32,695 - INFO - Batch: 120/141, Loss: 1.7338, Perplexity: 5.6624, Time: 0.30s
2025-04-25 23:51:33,000 - INFO - Batch: 130/141, Loss: 1.7224, Perplexity: 5.5977, Time: 0.31s
2025-04-25 23:51:33,230 - INFO - Batch: 140/141, Loss: 1.7310, Perplexity: 5.6464, Time: 0.23s
2025-04-25 23:51:34,069 - INFO - Validation  Loss: 1.8890, Perplexity: 6.6127, Time: 0.81s
2025-04-25 23:51:34,070 - INFO - Epoch 74/300 - Train Loss: 1.7240, Val Loss: 1.8890
2025-04-25 23:51:34,374 - INFO - Batch: 10/141, Loss: 1.7226, Perplexity: 5.5990, Time: 0.30s
2025-04-25 23:51:34,642 - INFO - Batch: 20/141, Loss: 1.7233, Perplexity: 5.6030, Time: 0.27s
2025-04-25 23:51:34,908 - INFO - Batch: 30/141, Loss: 1.7185, Perplexity: 5.5759, Time: 0.27s
2025-04-25 23:51:35,186 - INFO - Batch: 40/141, Loss: 1.7167, Perplexity: 5.5662, Time: 0.28s
2025-04-25 23:51:35,418 - INFO - Batch: 50/141, Loss: 1.7273, Perplexity: 5.6257, Time: 0.23s
2025-04-25 23:51:35,648 - INFO - Batch: 60/141, Loss: 1.7321, Perplexity: 5.6527, Time: 0.23s
2025-04-25 23:51:35,930 - INFO - Batch: 70/141, Loss: 1.7173, Perplexity: 5.5697, Time: 0.28s
2025-04-25 23:51:36,160 - INFO - Batch: 80/141, Loss: 1.7271, Perplexity: 5.6244, Time: 0.23s
2025-04-25 23:51:36,377 - INFO - Batch: 90/141, Loss: 1.7273, Perplexity: 5.6253, Time: 0.22s
2025-04-25 23:51:36,594 - INFO - Batch: 100/141, Loss: 1.7378, Perplexity: 5.6846, Time: 0.22s
2025-04-25 23:51:36,811 - INFO - Batch: 110/141, Loss: 1.7333, Perplexity: 5.6590, Time: 0.22s
2025-04-25 23:51:37,030 - INFO - Batch: 120/141, Loss: 1.7337, Perplexity: 5.6615, Time: 0.22s
2025-04-25 23:51:37,251 - INFO - Batch: 130/141, Loss: 1.7243, Perplexity: 5.6084, Time: 0.22s
2025-04-25 23:51:37,539 - INFO - Batch: 140/141, Loss: 1.7217, Perplexity: 5.5942, Time: 0.29s
2025-04-25 23:51:38,129 - INFO - Validation  Loss: 1.8892, Perplexity: 6.6140, Time: 0.56s
2025-04-25 23:51:38,129 - INFO - Epoch 75/300 - Train Loss: 1.7236, Val Loss: 1.8892
2025-04-25 23:51:38,420 - INFO - Batch: 10/141, Loss: 1.7210, Perplexity: 5.5899, Time: 0.29s
2025-04-25 23:51:38,716 - INFO - Batch: 20/141, Loss: 1.7144, Perplexity: 5.5535, Time: 0.30s
2025-04-25 23:51:39,084 - INFO - Batch: 30/141, Loss: 1.7214, Perplexity: 5.5925, Time: 0.37s
2025-04-25 23:51:39,359 - INFO - Batch: 40/141, Loss: 1.7248, Perplexity: 5.6115, Time: 0.28s
2025-04-25 23:51:39,632 - INFO - Batch: 50/141, Loss: 1.7158, Perplexity: 5.5609, Time: 0.27s
2025-04-25 23:51:39,872 - INFO - Batch: 60/141, Loss: 1.7261, Perplexity: 5.6186, Time: 0.24s
2025-04-25 23:51:40,155 - INFO - Batch: 70/141, Loss: 1.7288, Perplexity: 5.6338, Time: 0.28s
2025-04-25 23:51:40,431 - INFO - Batch: 80/141, Loss: 1.7276, Perplexity: 5.6269, Time: 0.28s
2025-04-25 23:51:40,715 - INFO - Batch: 90/141, Loss: 1.7193, Perplexity: 5.5804, Time: 0.28s
2025-04-25 23:51:40,964 - INFO - Batch: 100/141, Loss: 1.7252, Perplexity: 5.6136, Time: 0.25s
2025-04-25 23:51:41,266 - INFO - Batch: 110/141, Loss: 1.7180, Perplexity: 5.5736, Time: 0.30s
2025-04-25 23:51:41,502 - INFO - Batch: 120/141, Loss: 1.7290, Perplexity: 5.6348, Time: 0.24s
2025-04-25 23:51:41,728 - INFO - Batch: 130/141, Loss: 1.7118, Perplexity: 5.5389, Time: 0.23s
2025-04-25 23:51:41,947 - INFO - Batch: 140/141, Loss: 1.7107, Perplexity: 5.5330, Time: 0.22s
2025-04-25 23:51:42,608 - INFO - Validation  Loss: 1.8890, Perplexity: 6.6130, Time: 0.63s
2025-04-25 23:51:42,609 - INFO - Epoch 76/300 - Train Loss: 1.7233, Val Loss: 1.8890
2025-04-25 23:51:42,824 - INFO - Batch: 10/141, Loss: 1.7244, Perplexity: 5.6089, Time: 0.22s
2025-04-25 23:51:43,038 - INFO - Batch: 20/141, Loss: 1.7225, Perplexity: 5.5985, Time: 0.21s
2025-04-25 23:51:43,265 - INFO - Batch: 30/141, Loss: 1.7223, Perplexity: 5.5973, Time: 0.23s
2025-04-25 23:51:43,486 - INFO - Batch: 40/141, Loss: 1.7219, Perplexity: 5.5951, Time: 0.22s
2025-04-25 23:51:43,725 - INFO - Batch: 50/141, Loss: 1.7248, Perplexity: 5.6116, Time: 0.24s
2025-04-25 23:51:43,949 - INFO - Batch: 60/141, Loss: 1.7235, Perplexity: 5.6043, Time: 0.22s
2025-04-25 23:51:44,174 - INFO - Batch: 70/141, Loss: 1.7244, Perplexity: 5.6092, Time: 0.22s
2025-04-25 23:51:44,463 - INFO - Batch: 80/141, Loss: 1.7117, Perplexity: 5.5384, Time: 0.29s
2025-04-25 23:51:44,694 - INFO - Batch: 90/141, Loss: 1.7321, Perplexity: 5.6525, Time: 0.23s
2025-04-25 23:51:44,920 - INFO - Batch: 100/141, Loss: 1.7238, Perplexity: 5.6057, Time: 0.22s
2025-04-25 23:51:45,142 - INFO - Batch: 110/141, Loss: 1.7032, Perplexity: 5.4915, Time: 0.22s
2025-04-25 23:51:45,367 - INFO - Batch: 120/141, Loss: 1.7132, Perplexity: 5.5467, Time: 0.22s
2025-04-25 23:51:45,596 - INFO - Batch: 130/141, Loss: 1.7192, Perplexity: 5.5801, Time: 0.23s
2025-04-25 23:51:45,827 - INFO - Batch: 140/141, Loss: 1.7126, Perplexity: 5.5431, Time: 0.23s
2025-04-25 23:51:46,462 - INFO - Validation  Loss: 1.8879, Perplexity: 6.6057, Time: 0.61s
2025-04-25 23:51:46,462 - INFO - Epoch 77/300 - Train Loss: 1.7229, Val Loss: 1.8879
2025-04-25 23:51:46,695 - INFO - Batch: 10/141, Loss: 1.7338, Perplexity: 5.6621, Time: 0.23s
2025-04-25 23:51:46,951 - INFO - Batch: 20/141, Loss: 1.7192, Perplexity: 5.5801, Time: 0.26s
2025-04-25 23:51:47,184 - INFO - Batch: 30/141, Loss: 1.7229, Perplexity: 5.6007, Time: 0.23s
2025-04-25 23:51:47,465 - INFO - Batch: 40/141, Loss: 1.7120, Perplexity: 5.5401, Time: 0.28s
2025-04-25 23:51:47,697 - INFO - Batch: 50/141, Loss: 1.7235, Perplexity: 5.6043, Time: 0.23s
2025-04-25 23:51:47,924 - INFO - Batch: 60/141, Loss: 1.7281, Perplexity: 5.6297, Time: 0.23s
2025-04-25 23:51:48,149 - INFO - Batch: 70/141, Loss: 1.7261, Perplexity: 5.6186, Time: 0.23s
2025-04-25 23:51:48,380 - INFO - Batch: 80/141, Loss: 1.7268, Perplexity: 5.6228, Time: 0.23s
2025-04-25 23:51:48,616 - INFO - Batch: 90/141, Loss: 1.7175, Perplexity: 5.5706, Time: 0.24s
2025-04-25 23:51:48,852 - INFO - Batch: 100/141, Loss: 1.7094, Perplexity: 5.5255, Time: 0.24s
2025-04-25 23:51:49,089 - INFO - Batch: 110/141, Loss: 1.7146, Perplexity: 5.5547, Time: 0.24s
2025-04-25 23:51:49,382 - INFO - Batch: 120/141, Loss: 1.7257, Perplexity: 5.6165, Time: 0.29s
2025-04-25 23:51:49,602 - INFO - Batch: 130/141, Loss: 1.7253, Perplexity: 5.6142, Time: 0.22s
2025-04-25 23:51:49,822 - INFO - Batch: 140/141, Loss: 1.7287, Perplexity: 5.6332, Time: 0.22s
2025-04-25 23:51:50,417 - INFO - Validation  Loss: 1.8891, Perplexity: 6.6137, Time: 0.56s
2025-04-25 23:51:50,417 - INFO - Epoch 78/300 - Train Loss: 1.7225, Val Loss: 1.8891
2025-04-25 23:51:50,742 - INFO - Batch: 10/141, Loss: 1.7361, Perplexity: 5.6754, Time: 0.33s
2025-04-25 23:51:50,972 - INFO - Batch: 20/141, Loss: 1.7317, Perplexity: 5.6503, Time: 0.23s
2025-04-25 23:51:51,208 - INFO - Batch: 30/141, Loss: 1.7233, Perplexity: 5.6029, Time: 0.24s
2025-04-25 23:51:51,496 - INFO - Batch: 40/141, Loss: 1.7118, Perplexity: 5.5391, Time: 0.29s
2025-04-25 23:51:51,723 - INFO - Batch: 50/141, Loss: 1.7307, Perplexity: 5.6446, Time: 0.23s
2025-04-25 23:51:51,954 - INFO - Batch: 60/141, Loss: 1.7309, Perplexity: 5.6457, Time: 0.23s
2025-04-25 23:51:52,187 - INFO - Batch: 70/141, Loss: 1.7290, Perplexity: 5.6352, Time: 0.23s
2025-04-25 23:51:52,472 - INFO - Batch: 80/141, Loss: 1.7204, Perplexity: 5.5865, Time: 0.29s
2025-04-25 23:51:52,691 - INFO - Batch: 90/141, Loss: 1.7294, Perplexity: 5.6375, Time: 0.22s
2025-04-25 23:51:52,926 - INFO - Batch: 100/141, Loss: 1.7157, Perplexity: 5.5608, Time: 0.24s
2025-04-25 23:51:53,165 - INFO - Batch: 110/141, Loss: 1.7245, Perplexity: 5.6096, Time: 0.24s
2025-04-25 23:51:53,394 - INFO - Batch: 120/141, Loss: 1.7208, Perplexity: 5.5888, Time: 0.23s
2025-04-25 23:51:53,623 - INFO - Batch: 130/141, Loss: 1.7291, Perplexity: 5.6356, Time: 0.23s
2025-04-25 23:51:53,846 - INFO - Batch: 140/141, Loss: 1.7284, Perplexity: 5.6317, Time: 0.22s
2025-04-25 23:51:54,492 - INFO - Validation  Loss: 1.8889, Perplexity: 6.6121, Time: 0.62s
2025-04-25 23:51:54,492 - INFO - Epoch 79/300 - Train Loss: 1.7222, Val Loss: 1.8889
2025-04-25 23:51:54,721 - INFO - Batch: 10/141, Loss: 1.7248, Perplexity: 5.6115, Time: 0.23s
2025-04-25 23:51:54,950 - INFO - Batch: 20/141, Loss: 1.7313, Perplexity: 5.6478, Time: 0.23s
2025-04-25 23:51:55,177 - INFO - Batch: 30/141, Loss: 1.7165, Perplexity: 5.5648, Time: 0.23s
2025-04-25 23:51:55,432 - INFO - Batch: 40/141, Loss: 1.7130, Perplexity: 5.5454, Time: 0.26s
2025-04-25 23:51:55,721 - INFO - Batch: 50/141, Loss: 1.7275, Perplexity: 5.6268, Time: 0.29s
2025-04-25 23:51:55,934 - INFO - Batch: 60/141, Loss: 1.7284, Perplexity: 5.6317, Time: 0.21s
2025-04-25 23:51:56,152 - INFO - Batch: 70/141, Loss: 1.7193, Perplexity: 5.5804, Time: 0.22s
2025-04-25 23:51:56,377 - INFO - Batch: 80/141, Loss: 1.7487, Perplexity: 5.7473, Time: 0.22s
2025-04-25 23:51:56,595 - INFO - Batch: 90/141, Loss: 1.7188, Perplexity: 5.5780, Time: 0.22s
2025-04-25 23:51:56,821 - INFO - Batch: 100/141, Loss: 1.7270, Perplexity: 5.6237, Time: 0.23s
2025-04-25 23:51:57,044 - INFO - Batch: 110/141, Loss: 1.7315, Perplexity: 5.6489, Time: 0.22s
2025-04-25 23:51:57,265 - INFO - Batch: 120/141, Loss: 1.7172, Perplexity: 5.5691, Time: 0.22s
2025-04-25 23:51:57,551 - INFO - Batch: 130/141, Loss: 1.7306, Perplexity: 5.6443, Time: 0.29s
2025-04-25 23:51:57,774 - INFO - Batch: 140/141, Loss: 1.7362, Perplexity: 5.6756, Time: 0.22s
2025-04-25 23:51:58,472 - INFO - Validation  Loss: 1.8878, Perplexity: 6.6049, Time: 0.67s
2025-04-25 23:51:58,472 - INFO - Epoch 80/300 - Train Loss: 1.7219, Val Loss: 1.8878
2025-04-25 23:51:58,697 - INFO - Batch: 10/141, Loss: 1.7167, Perplexity: 5.5664, Time: 0.23s
2025-04-25 23:51:58,982 - INFO - Batch: 20/141, Loss: 1.7425, Perplexity: 5.7115, Time: 0.28s
2025-04-25 23:51:59,202 - INFO - Batch: 30/141, Loss: 1.7238, Perplexity: 5.6059, Time: 0.22s
2025-04-25 23:51:59,436 - INFO - Batch: 40/141, Loss: 1.7194, Perplexity: 5.5814, Time: 0.23s
2025-04-25 23:51:59,746 - INFO - Batch: 50/141, Loss: 1.7195, Perplexity: 5.5816, Time: 0.31s
2025-04-25 23:52:00,036 - INFO - Batch: 60/141, Loss: 1.7106, Perplexity: 5.5322, Time: 0.29s
2025-04-25 23:52:00,310 - INFO - Batch: 70/141, Loss: 1.7109, Perplexity: 5.5342, Time: 0.27s
2025-04-25 23:52:00,534 - INFO - Batch: 80/141, Loss: 1.7180, Perplexity: 5.5736, Time: 0.22s
2025-04-25 23:52:00,835 - INFO - Batch: 90/141, Loss: 1.7230, Perplexity: 5.6013, Time: 0.30s
2025-04-25 23:52:01,060 - INFO - Batch: 100/141, Loss: 1.7219, Perplexity: 5.5951, Time: 0.23s
2025-04-25 23:52:01,289 - INFO - Batch: 110/141, Loss: 1.7043, Perplexity: 5.4975, Time: 0.23s
2025-04-25 23:52:01,510 - INFO - Batch: 120/141, Loss: 1.7209, Perplexity: 5.5895, Time: 0.22s
2025-04-25 23:52:01,747 - INFO - Batch: 130/141, Loss: 1.7216, Perplexity: 5.5934, Time: 0.24s
2025-04-25 23:52:01,973 - INFO - Batch: 140/141, Loss: 1.7142, Perplexity: 5.5523, Time: 0.23s
2025-04-25 23:52:02,679 - INFO - Validation  Loss: 1.8877, Perplexity: 6.6044, Time: 0.68s
2025-04-25 23:52:02,679 - INFO - Epoch 81/300 - Train Loss: 1.7215, Val Loss: 1.8877
2025-04-25 23:52:02,917 - INFO - Batch: 10/141, Loss: 1.7338, Perplexity: 5.6622, Time: 0.24s
2025-04-25 23:52:03,146 - INFO - Batch: 20/141, Loss: 1.7289, Perplexity: 5.6342, Time: 0.23s
2025-04-25 23:52:03,375 - INFO - Batch: 30/141, Loss: 1.7098, Perplexity: 5.5278, Time: 0.23s
2025-04-25 23:52:03,602 - INFO - Batch: 40/141, Loss: 1.7233, Perplexity: 5.6027, Time: 0.23s
2025-04-25 23:52:03,823 - INFO - Batch: 50/141, Loss: 1.7272, Perplexity: 5.6248, Time: 0.22s
2025-04-25 23:52:04,118 - INFO - Batch: 60/141, Loss: 1.7325, Perplexity: 5.6548, Time: 0.30s
2025-04-25 23:52:04,375 - INFO - Batch: 70/141, Loss: 1.7259, Perplexity: 5.6177, Time: 0.26s
2025-04-25 23:52:04,663 - INFO - Batch: 80/141, Loss: 1.7144, Perplexity: 5.5532, Time: 0.29s
2025-04-25 23:52:04,942 - INFO - Batch: 90/141, Loss: 1.7225, Perplexity: 5.5987, Time: 0.28s
2025-04-25 23:52:05,189 - INFO - Batch: 100/141, Loss: 1.7258, Perplexity: 5.6168, Time: 0.25s
2025-04-25 23:52:05,411 - INFO - Batch: 110/141, Loss: 1.7285, Perplexity: 5.6323, Time: 0.22s
2025-04-25 23:52:05,645 - INFO - Batch: 120/141, Loss: 1.7190, Perplexity: 5.5787, Time: 0.23s
2025-04-25 23:52:05,875 - INFO - Batch: 130/141, Loss: 1.7184, Perplexity: 5.5758, Time: 0.23s
2025-04-25 23:52:06,163 - INFO - Batch: 140/141, Loss: 1.7284, Perplexity: 5.6314, Time: 0.29s
2025-04-25 23:52:06,768 - INFO - Validation  Loss: 1.8881, Perplexity: 6.6070, Time: 0.56s
2025-04-25 23:52:06,768 - INFO - Epoch 82/300 - Train Loss: 1.7212, Val Loss: 1.8881
2025-04-25 23:52:06,987 - INFO - Batch: 10/141, Loss: 1.7155, Perplexity: 5.5596, Time: 0.22s
2025-04-25 23:52:07,206 - INFO - Batch: 20/141, Loss: 1.7260, Perplexity: 5.6181, Time: 0.22s
2025-04-25 23:52:07,482 - INFO - Batch: 30/141, Loss: 1.7302, Perplexity: 5.6418, Time: 0.28s
2025-04-25 23:52:07,768 - INFO - Batch: 40/141, Loss: 1.7218, Perplexity: 5.5947, Time: 0.29s
2025-04-25 23:52:08,020 - INFO - Batch: 50/141, Loss: 1.7165, Perplexity: 5.5650, Time: 0.25s
2025-04-25 23:52:08,313 - INFO - Batch: 60/141, Loss: 1.7333, Perplexity: 5.6590, Time: 0.29s
2025-04-25 23:52:08,608 - INFO - Batch: 70/141, Loss: 1.7120, Perplexity: 5.5399, Time: 0.29s
2025-04-25 23:52:08,915 - INFO - Batch: 80/141, Loss: 1.7146, Perplexity: 5.5547, Time: 0.31s
2025-04-25 23:52:09,210 - INFO - Batch: 90/141, Loss: 1.7163, Perplexity: 5.5638, Time: 0.30s
2025-04-25 23:52:09,537 - INFO - Batch: 100/141, Loss: 1.7190, Perplexity: 5.5787, Time: 0.33s
2025-04-25 23:52:09,762 - INFO - Batch: 110/141, Loss: 1.7182, Perplexity: 5.5745, Time: 0.22s
2025-04-25 23:52:09,997 - INFO - Batch: 120/141, Loss: 1.7270, Perplexity: 5.6240, Time: 0.24s
2025-04-25 23:52:10,281 - INFO - Batch: 130/141, Loss: 1.7099, Perplexity: 5.5286, Time: 0.28s
2025-04-25 23:52:10,575 - INFO - Batch: 140/141, Loss: 1.7290, Perplexity: 5.6351, Time: 0.29s
2025-04-25 23:52:11,323 - INFO - Validation  Loss: 1.8878, Perplexity: 6.6050, Time: 0.71s
2025-04-25 23:52:11,323 - INFO - Epoch 83/300 - Train Loss: 1.7209, Val Loss: 1.8878
2025-04-25 23:52:11,541 - INFO - Batch: 10/141, Loss: 1.7055, Perplexity: 5.5039, Time: 0.22s
2025-04-25 23:52:11,776 - INFO - Batch: 20/141, Loss: 1.7276, Perplexity: 5.6271, Time: 0.24s
2025-04-25 23:52:11,996 - INFO - Batch: 30/141, Loss: 1.7359, Perplexity: 5.6741, Time: 0.22s
2025-04-25 23:52:12,221 - INFO - Batch: 40/141, Loss: 1.7331, Perplexity: 5.6579, Time: 0.22s
2025-04-25 23:52:12,440 - INFO - Batch: 50/141, Loss: 1.7245, Perplexity: 5.6097, Time: 0.22s
2025-04-25 23:52:12,660 - INFO - Batch: 60/141, Loss: 1.7205, Perplexity: 5.5872, Time: 0.22s
2025-04-25 23:52:12,937 - INFO - Batch: 70/141, Loss: 1.7236, Perplexity: 5.6045, Time: 0.28s
2025-04-25 23:52:13,155 - INFO - Batch: 80/141, Loss: 1.7099, Perplexity: 5.5285, Time: 0.22s
2025-04-25 23:52:13,377 - INFO - Batch: 90/141, Loss: 1.7330, Perplexity: 5.6578, Time: 0.22s
2025-04-25 23:52:13,601 - INFO - Batch: 100/141, Loss: 1.7026, Perplexity: 5.4885, Time: 0.22s
2025-04-25 23:52:13,828 - INFO - Batch: 110/141, Loss: 1.7291, Perplexity: 5.6355, Time: 0.23s
2025-04-25 23:52:14,049 - INFO - Batch: 120/141, Loss: 1.7059, Perplexity: 5.5066, Time: 0.22s
2025-04-25 23:52:14,272 - INFO - Batch: 130/141, Loss: 1.7263, Perplexity: 5.6198, Time: 0.22s
2025-04-25 23:52:14,489 - INFO - Batch: 140/141, Loss: 1.7187, Perplexity: 5.5773, Time: 0.22s
2025-04-25 23:52:15,122 - INFO - Validation  Loss: 1.8873, Perplexity: 6.6017, Time: 0.54s
2025-04-25 23:52:15,122 - INFO - Epoch 84/300 - Train Loss: 1.7206, Val Loss: 1.8873
2025-04-25 23:52:15,342 - INFO - Batch: 10/141, Loss: 1.7252, Perplexity: 5.6134, Time: 0.22s
2025-04-25 23:52:15,564 - INFO - Batch: 20/141, Loss: 1.7113, Perplexity: 5.5359, Time: 0.22s
2025-04-25 23:52:15,863 - INFO - Batch: 30/141, Loss: 1.7155, Perplexity: 5.5593, Time: 0.30s
2025-04-25 23:52:16,088 - INFO - Batch: 40/141, Loss: 1.7309, Perplexity: 5.6456, Time: 0.22s
2025-04-25 23:52:16,318 - INFO - Batch: 50/141, Loss: 1.7134, Perplexity: 5.5479, Time: 0.23s
2025-04-25 23:52:16,575 - INFO - Batch: 60/141, Loss: 1.7267, Perplexity: 5.6219, Time: 0.26s
2025-04-25 23:52:16,840 - INFO - Batch: 70/141, Loss: 1.7050, Perplexity: 5.5016, Time: 0.26s
2025-04-25 23:52:17,120 - INFO - Batch: 80/141, Loss: 1.7324, Perplexity: 5.6542, Time: 0.28s
2025-04-25 23:52:17,415 - INFO - Batch: 90/141, Loss: 1.7020, Perplexity: 5.4849, Time: 0.30s
2025-04-25 23:52:17,665 - INFO - Batch: 100/141, Loss: 1.7106, Perplexity: 5.5324, Time: 0.25s
2025-04-25 23:52:17,947 - INFO - Batch: 110/141, Loss: 1.7310, Perplexity: 5.6463, Time: 0.28s
2025-04-25 23:52:18,177 - INFO - Batch: 120/141, Loss: 1.7276, Perplexity: 5.6269, Time: 0.23s
2025-04-25 23:52:18,432 - INFO - Batch: 130/141, Loss: 1.7212, Perplexity: 5.5913, Time: 0.26s
2025-04-25 23:52:18,676 - INFO - Batch: 140/141, Loss: 1.7346, Perplexity: 5.6664, Time: 0.24s
2025-04-25 23:52:19,334 - INFO - Validation  Loss: 1.8871, Perplexity: 6.6003, Time: 0.63s
2025-04-25 23:52:19,334 - INFO - Epoch 85/300 - Train Loss: 1.7203, Val Loss: 1.8871
2025-04-25 23:52:19,560 - INFO - Batch: 10/141, Loss: 1.7162, Perplexity: 5.5633, Time: 0.23s
2025-04-25 23:52:19,787 - INFO - Batch: 20/141, Loss: 1.7198, Perplexity: 5.5834, Time: 0.23s
2025-04-25 23:52:20,006 - INFO - Batch: 30/141, Loss: 1.7097, Perplexity: 5.5272, Time: 0.22s
2025-04-25 23:52:20,315 - INFO - Batch: 40/141, Loss: 1.7157, Perplexity: 5.5603, Time: 0.31s
2025-04-25 23:52:20,718 - INFO - Batch: 50/141, Loss: 1.7298, Perplexity: 5.6394, Time: 0.40s
2025-04-25 23:52:20,962 - INFO - Batch: 60/141, Loss: 1.7283, Perplexity: 5.6311, Time: 0.24s
2025-04-25 23:52:21,184 - INFO - Batch: 70/141, Loss: 1.7193, Perplexity: 5.5809, Time: 0.22s
2025-04-25 23:52:21,549 - INFO - Batch: 80/141, Loss: 1.7185, Perplexity: 5.5764, Time: 0.37s
2025-04-25 23:52:21,842 - INFO - Batch: 90/141, Loss: 1.7423, Perplexity: 5.7105, Time: 0.29s
2025-04-25 23:52:22,128 - INFO - Batch: 100/141, Loss: 1.7180, Perplexity: 5.5733, Time: 0.29s
2025-04-25 23:52:22,363 - INFO - Batch: 110/141, Loss: 1.7341, Perplexity: 5.6637, Time: 0.23s
2025-04-25 23:52:22,599 - INFO - Batch: 120/141, Loss: 1.7211, Perplexity: 5.5908, Time: 0.24s
2025-04-25 23:52:22,822 - INFO - Batch: 130/141, Loss: 1.7181, Perplexity: 5.5738, Time: 0.22s
2025-04-25 23:52:23,043 - INFO - Batch: 140/141, Loss: 1.7107, Perplexity: 5.5326, Time: 0.22s
2025-04-25 23:52:23,676 - INFO - Validation  Loss: 1.8877, Perplexity: 6.6040, Time: 0.60s
2025-04-25 23:52:23,676 - INFO - Epoch 86/300 - Train Loss: 1.7199, Val Loss: 1.8877
2025-04-25 23:52:23,894 - INFO - Batch: 10/141, Loss: 1.7244, Perplexity: 5.6092, Time: 0.22s
2025-04-25 23:52:24,111 - INFO - Batch: 20/141, Loss: 1.7295, Perplexity: 5.6377, Time: 0.22s
2025-04-25 23:52:24,342 - INFO - Batch: 30/141, Loss: 1.7071, Perplexity: 5.5129, Time: 0.23s
2025-04-25 23:52:24,669 - INFO - Batch: 40/141, Loss: 1.7245, Perplexity: 5.6097, Time: 0.33s
2025-04-25 23:52:24,911 - INFO - Batch: 50/141, Loss: 1.7137, Perplexity: 5.5495, Time: 0.24s
2025-04-25 23:52:25,140 - INFO - Batch: 60/141, Loss: 1.7255, Perplexity: 5.6156, Time: 0.23s
2025-04-25 23:52:25,360 - INFO - Batch: 70/141, Loss: 1.7076, Perplexity: 5.5157, Time: 0.22s
2025-04-25 23:52:25,578 - INFO - Batch: 80/141, Loss: 1.7209, Perplexity: 5.5895, Time: 0.22s
2025-04-25 23:52:25,801 - INFO - Batch: 90/141, Loss: 1.7145, Perplexity: 5.5540, Time: 0.22s
2025-04-25 23:52:26,025 - INFO - Batch: 100/141, Loss: 1.7125, Perplexity: 5.5425, Time: 0.22s
2025-04-25 23:52:26,263 - INFO - Batch: 110/141, Loss: 1.7103, Perplexity: 5.5309, Time: 0.24s
2025-04-25 23:52:26,562 - INFO - Batch: 120/141, Loss: 1.7066, Perplexity: 5.5100, Time: 0.30s
2025-04-25 23:52:26,806 - INFO - Batch: 130/141, Loss: 1.7285, Perplexity: 5.6324, Time: 0.24s
2025-04-25 23:52:27,032 - INFO - Batch: 140/141, Loss: 1.7162, Perplexity: 5.5632, Time: 0.23s
2025-04-25 23:52:27,621 - INFO - Validation  Loss: 1.8866, Perplexity: 6.5970, Time: 0.56s
2025-04-25 23:52:27,621 - INFO - Epoch 87/300 - Train Loss: 1.7196, Val Loss: 1.8866
2025-04-25 23:52:27,926 - INFO - Batch: 10/141, Loss: 1.7286, Perplexity: 5.6330, Time: 0.30s
2025-04-25 23:52:28,164 - INFO - Batch: 20/141, Loss: 1.7296, Perplexity: 5.6382, Time: 0.24s
2025-04-25 23:52:28,384 - INFO - Batch: 30/141, Loss: 1.7230, Perplexity: 5.6015, Time: 0.22s
2025-04-25 23:52:28,616 - INFO - Batch: 40/141, Loss: 1.7096, Perplexity: 5.5265, Time: 0.23s
2025-04-25 23:52:28,845 - INFO - Batch: 50/141, Loss: 1.7082, Perplexity: 5.5190, Time: 0.23s
2025-04-25 23:52:29,068 - INFO - Batch: 60/141, Loss: 1.7211, Perplexity: 5.5904, Time: 0.22s
2025-04-25 23:52:29,288 - INFO - Batch: 70/141, Loss: 1.7144, Perplexity: 5.5536, Time: 0.22s
2025-04-25 23:52:29,525 - INFO - Batch: 80/141, Loss: 1.7297, Perplexity: 5.6388, Time: 0.24s
2025-04-25 23:52:29,811 - INFO - Batch: 90/141, Loss: 1.7341, Perplexity: 5.6640, Time: 0.29s
2025-04-25 23:52:30,035 - INFO - Batch: 100/141, Loss: 1.7249, Perplexity: 5.6119, Time: 0.22s
2025-04-25 23:52:30,262 - INFO - Batch: 110/141, Loss: 1.7383, Perplexity: 5.6874, Time: 0.23s
2025-04-25 23:52:30,506 - INFO - Batch: 120/141, Loss: 1.7249, Perplexity: 5.6119, Time: 0.24s
2025-04-25 23:52:30,731 - INFO - Batch: 130/141, Loss: 1.7099, Perplexity: 5.5285, Time: 0.23s
2025-04-25 23:52:30,954 - INFO - Batch: 140/141, Loss: 1.7128, Perplexity: 5.5445, Time: 0.22s
2025-04-25 23:52:31,587 - INFO - Validation  Loss: 1.8871, Perplexity: 6.6000, Time: 0.60s
2025-04-25 23:52:31,587 - INFO - Epoch 88/300 - Train Loss: 1.7194, Val Loss: 1.8871
2025-04-25 23:52:31,800 - INFO - Batch: 10/141, Loss: 1.7305, Perplexity: 5.6432, Time: 0.21s
2025-04-25 23:52:32,033 - INFO - Batch: 20/141, Loss: 1.7075, Perplexity: 5.5151, Time: 0.23s
2025-04-25 23:52:32,253 - INFO - Batch: 30/141, Loss: 1.7252, Perplexity: 5.6135, Time: 0.22s
2025-04-25 23:52:32,474 - INFO - Batch: 40/141, Loss: 1.7120, Perplexity: 5.5401, Time: 0.22s
2025-04-25 23:52:32,775 - INFO - Batch: 50/141, Loss: 1.7262, Perplexity: 5.6193, Time: 0.30s
2025-04-25 23:52:32,998 - INFO - Batch: 60/141, Loss: 1.7279, Perplexity: 5.6288, Time: 0.22s
2025-04-25 23:52:33,238 - INFO - Batch: 70/141, Loss: 1.7158, Perplexity: 5.5612, Time: 0.24s
2025-04-25 23:52:33,457 - INFO - Batch: 80/141, Loss: 1.7210, Perplexity: 5.5901, Time: 0.22s
2025-04-25 23:52:33,682 - INFO - Batch: 90/141, Loss: 1.7229, Perplexity: 5.6008, Time: 0.23s
2025-04-25 23:52:33,899 - INFO - Batch: 100/141, Loss: 1.7286, Perplexity: 5.6326, Time: 0.22s
2025-04-25 23:52:34,122 - INFO - Batch: 110/141, Loss: 1.7158, Perplexity: 5.5614, Time: 0.22s
2025-04-25 23:52:34,349 - INFO - Batch: 120/141, Loss: 1.7075, Perplexity: 5.5153, Time: 0.23s
2025-04-25 23:52:34,628 - INFO - Batch: 130/141, Loss: 1.7290, Perplexity: 5.6351, Time: 0.28s
2025-04-25 23:52:34,857 - INFO - Batch: 140/141, Loss: 1.7174, Perplexity: 5.5699, Time: 0.23s
2025-04-25 23:52:35,451 - INFO - Validation  Loss: 1.8866, Perplexity: 6.5970, Time: 0.56s
2025-04-25 23:52:35,452 - INFO - Epoch 89/300 - Train Loss: 1.7190, Val Loss: 1.8866
2025-04-25 23:52:35,672 - INFO - Batch: 10/141, Loss: 1.7246, Perplexity: 5.6102, Time: 0.22s
2025-04-25 23:52:35,956 - INFO - Batch: 20/141, Loss: 1.7134, Perplexity: 5.5478, Time: 0.28s
2025-04-25 23:52:36,186 - INFO - Batch: 30/141, Loss: 1.7158, Perplexity: 5.5613, Time: 0.23s
2025-04-25 23:52:36,434 - INFO - Batch: 40/141, Loss: 1.7213, Perplexity: 5.5917, Time: 0.25s
2025-04-25 23:52:36,677 - INFO - Batch: 50/141, Loss: 1.7089, Perplexity: 5.5231, Time: 0.24s
2025-04-25 23:52:36,917 - INFO - Batch: 60/141, Loss: 1.7242, Perplexity: 5.6080, Time: 0.24s
2025-04-25 23:52:37,157 - INFO - Batch: 70/141, Loss: 1.7114, Perplexity: 5.5365, Time: 0.24s
2025-04-25 23:52:37,402 - INFO - Batch: 80/141, Loss: 1.7179, Perplexity: 5.5726, Time: 0.25s
2025-04-25 23:52:37,640 - INFO - Batch: 90/141, Loss: 1.7132, Perplexity: 5.5465, Time: 0.24s
2025-04-25 23:52:37,927 - INFO - Batch: 100/141, Loss: 1.7150, Perplexity: 5.5569, Time: 0.29s
2025-04-25 23:52:38,161 - INFO - Batch: 110/141, Loss: 1.7197, Perplexity: 5.5831, Time: 0.23s
2025-04-25 23:52:38,380 - INFO - Batch: 120/141, Loss: 1.7202, Perplexity: 5.5855, Time: 0.22s
2025-04-25 23:52:38,606 - INFO - Batch: 130/141, Loss: 1.7189, Perplexity: 5.5785, Time: 0.23s
2025-04-25 23:52:38,843 - INFO - Batch: 140/141, Loss: 1.7183, Perplexity: 5.5751, Time: 0.24s
2025-04-25 23:52:39,565 - INFO - Validation  Loss: 1.8860, Perplexity: 6.5931, Time: 0.68s
2025-04-25 23:52:39,565 - INFO - Epoch 90/300 - Train Loss: 1.7188, Val Loss: 1.8860
2025-04-25 23:52:39,778 - INFO - Batch: 10/141, Loss: 1.7164, Perplexity: 5.5645, Time: 0.21s
2025-04-25 23:52:39,994 - INFO - Batch: 20/141, Loss: 1.7199, Perplexity: 5.5838, Time: 0.22s
2025-04-25 23:52:40,222 - INFO - Batch: 30/141, Loss: 1.7211, Perplexity: 5.5907, Time: 0.23s
2025-04-25 23:52:40,440 - INFO - Batch: 40/141, Loss: 1.7073, Perplexity: 5.5139, Time: 0.22s
2025-04-25 23:52:40,665 - INFO - Batch: 50/141, Loss: 1.7153, Perplexity: 5.5584, Time: 0.22s
2025-04-25 23:52:40,942 - INFO - Batch: 60/141, Loss: 1.7024, Perplexity: 5.4872, Time: 0.28s
2025-04-25 23:52:41,159 - INFO - Batch: 70/141, Loss: 1.7140, Perplexity: 5.5509, Time: 0.22s
2025-04-25 23:52:41,377 - INFO - Batch: 80/141, Loss: 1.7147, Perplexity: 5.5549, Time: 0.22s
2025-04-25 23:52:41,598 - INFO - Batch: 90/141, Loss: 1.7178, Perplexity: 5.5721, Time: 0.22s
2025-04-25 23:52:41,816 - INFO - Batch: 100/141, Loss: 1.7032, Perplexity: 5.4914, Time: 0.22s
2025-04-25 23:52:42,035 - INFO - Batch: 110/141, Loss: 1.7200, Perplexity: 5.5848, Time: 0.22s
2025-04-25 23:52:42,273 - INFO - Batch: 120/141, Loss: 1.7199, Perplexity: 5.5842, Time: 0.24s
2025-04-25 23:52:42,502 - INFO - Batch: 130/141, Loss: 1.7197, Perplexity: 5.5829, Time: 0.23s
2025-04-25 23:52:42,781 - INFO - Batch: 140/141, Loss: 1.7041, Perplexity: 5.4965, Time: 0.28s
2025-04-25 23:52:43,379 - INFO - Validation  Loss: 1.8858, Perplexity: 6.5914, Time: 0.56s
2025-04-25 23:52:43,379 - INFO - Epoch 91/300 - Train Loss: 1.7185, Val Loss: 1.8858
2025-04-25 23:52:43,644 - INFO - Batch: 10/141, Loss: 1.7259, Perplexity: 5.6177, Time: 0.27s
2025-04-25 23:52:43,893 - INFO - Batch: 20/141, Loss: 1.7281, Perplexity: 5.6297, Time: 0.25s
2025-04-25 23:52:44,177 - INFO - Batch: 30/141, Loss: 1.7295, Perplexity: 5.6380, Time: 0.28s
2025-04-25 23:52:44,393 - INFO - Batch: 40/141, Loss: 1.7201, Perplexity: 5.5851, Time: 0.22s
2025-04-25 23:52:44,618 - INFO - Batch: 50/141, Loss: 1.7213, Perplexity: 5.5916, Time: 0.22s
2025-04-25 23:52:44,836 - INFO - Batch: 60/141, Loss: 1.7187, Perplexity: 5.5775, Time: 0.22s
2025-04-25 23:52:45,058 - INFO - Batch: 70/141, Loss: 1.7266, Perplexity: 5.6214, Time: 0.22s
2025-04-25 23:52:45,297 - INFO - Batch: 80/141, Loss: 1.7147, Perplexity: 5.5551, Time: 0.24s
2025-04-25 23:52:45,544 - INFO - Batch: 90/141, Loss: 1.7179, Perplexity: 5.5730, Time: 0.25s
2025-04-25 23:52:45,839 - INFO - Batch: 100/141, Loss: 1.7058, Perplexity: 5.5058, Time: 0.29s
2025-04-25 23:52:46,058 - INFO - Batch: 110/141, Loss: 1.7209, Perplexity: 5.5895, Time: 0.22s
2025-04-25 23:52:46,287 - INFO - Batch: 120/141, Loss: 1.7362, Perplexity: 5.6755, Time: 0.23s
2025-04-25 23:52:46,508 - INFO - Batch: 130/141, Loss: 1.7135, Perplexity: 5.5481, Time: 0.22s
2025-04-25 23:52:46,748 - INFO - Batch: 140/141, Loss: 1.7291, Perplexity: 5.6354, Time: 0.24s
2025-04-25 23:52:47,429 - INFO - Validation  Loss: 1.8862, Perplexity: 6.5940, Time: 0.64s
2025-04-25 23:52:47,429 - INFO - Epoch 92/300 - Train Loss: 1.7183, Val Loss: 1.8862
2025-04-25 23:52:47,649 - INFO - Batch: 10/141, Loss: 1.7179, Perplexity: 5.5727, Time: 0.22s
2025-04-25 23:52:47,882 - INFO - Batch: 20/141, Loss: 1.7098, Perplexity: 5.5277, Time: 0.23s
2025-04-25 23:52:48,129 - INFO - Batch: 30/141, Loss: 1.7272, Perplexity: 5.6249, Time: 0.25s
2025-04-25 23:52:48,367 - INFO - Batch: 40/141, Loss: 1.7102, Perplexity: 5.5299, Time: 0.24s
2025-04-25 23:52:48,604 - INFO - Batch: 50/141, Loss: 1.7204, Perplexity: 5.5865, Time: 0.24s
2025-04-25 23:52:48,827 - INFO - Batch: 60/141, Loss: 1.7211, Perplexity: 5.5905, Time: 0.22s
2025-04-25 23:52:49,110 - INFO - Batch: 70/141, Loss: 1.7287, Perplexity: 5.6336, Time: 0.28s
2025-04-25 23:52:49,335 - INFO - Batch: 80/141, Loss: 1.7218, Perplexity: 5.5948, Time: 0.22s
2025-04-25 23:52:49,558 - INFO - Batch: 90/141, Loss: 1.7172, Perplexity: 5.5691, Time: 0.22s
2025-04-25 23:52:49,779 - INFO - Batch: 100/141, Loss: 1.7136, Perplexity: 5.5487, Time: 0.22s
2025-04-25 23:52:50,003 - INFO - Batch: 110/141, Loss: 1.7262, Perplexity: 5.6193, Time: 0.22s
2025-04-25 23:52:50,234 - INFO - Batch: 120/141, Loss: 1.7348, Perplexity: 5.6678, Time: 0.23s
2025-04-25 23:52:50,470 - INFO - Batch: 130/141, Loss: 1.7240, Perplexity: 5.6067, Time: 0.24s
2025-04-25 23:52:50,713 - INFO - Batch: 140/141, Loss: 1.7348, Perplexity: 5.6679, Time: 0.24s
2025-04-25 23:52:51,331 - INFO - Validation  Loss: 1.8858, Perplexity: 6.5916, Time: 0.59s
2025-04-25 23:52:51,331 - INFO - Epoch 93/300 - Train Loss: 1.7180, Val Loss: 1.8858
2025-04-25 23:52:51,582 - INFO - Batch: 10/141, Loss: 1.7164, Perplexity: 5.5643, Time: 0.25s
2025-04-25 23:52:51,801 - INFO - Batch: 20/141, Loss: 1.7327, Perplexity: 5.6557, Time: 0.22s
2025-04-25 23:52:52,033 - INFO - Batch: 30/141, Loss: 1.7275, Perplexity: 5.6263, Time: 0.23s
2025-04-25 23:52:52,317 - INFO - Batch: 40/141, Loss: 1.7276, Perplexity: 5.6269, Time: 0.28s
2025-04-25 23:52:52,545 - INFO - Batch: 50/141, Loss: 1.7190, Perplexity: 5.5788, Time: 0.23s
2025-04-25 23:52:52,793 - INFO - Batch: 60/141, Loss: 1.7126, Perplexity: 5.5434, Time: 0.25s
2025-04-25 23:52:53,019 - INFO - Batch: 70/141, Loss: 1.7139, Perplexity: 5.5505, Time: 0.23s
2025-04-25 23:52:53,240 - INFO - Batch: 80/141, Loss: 1.7184, Perplexity: 5.5759, Time: 0.22s
2025-04-25 23:52:53,463 - INFO - Batch: 90/141, Loss: 1.7185, Perplexity: 5.5762, Time: 0.22s
2025-04-25 23:52:53,681 - INFO - Batch: 100/141, Loss: 1.7060, Perplexity: 5.5072, Time: 0.22s
2025-04-25 23:52:53,972 - INFO - Batch: 110/141, Loss: 1.7282, Perplexity: 5.6305, Time: 0.29s
2025-04-25 23:52:54,201 - INFO - Batch: 120/141, Loss: 1.7208, Perplexity: 5.5891, Time: 0.23s
2025-04-25 23:52:54,448 - INFO - Batch: 130/141, Loss: 1.7070, Perplexity: 5.5123, Time: 0.25s
2025-04-25 23:52:54,689 - INFO - Batch: 140/141, Loss: 1.7266, Perplexity: 5.6215, Time: 0.24s
2025-04-25 23:52:55,395 - INFO - Validation  Loss: 1.8854, Perplexity: 6.5893, Time: 0.67s
2025-04-25 23:52:55,396 - INFO - Epoch 94/300 - Train Loss: 1.7177, Val Loss: 1.8854
2025-04-25 23:52:55,625 - INFO - Batch: 10/141, Loss: 1.7146, Perplexity: 5.5544, Time: 0.23s
2025-04-25 23:52:55,842 - INFO - Batch: 20/141, Loss: 1.7147, Perplexity: 5.5550, Time: 0.22s
2025-04-25 23:52:56,084 - INFO - Batch: 30/141, Loss: 1.7143, Perplexity: 5.5525, Time: 0.24s
2025-04-25 23:52:56,333 - INFO - Batch: 40/141, Loss: 1.7105, Perplexity: 5.5315, Time: 0.25s
2025-04-25 23:52:56,564 - INFO - Batch: 50/141, Loss: 1.7120, Perplexity: 5.5400, Time: 0.23s
2025-04-25 23:52:56,801 - INFO - Batch: 60/141, Loss: 1.7101, Perplexity: 5.5294, Time: 0.24s
2025-04-25 23:52:57,045 - INFO - Batch: 70/141, Loss: 1.6959, Perplexity: 5.4516, Time: 0.24s
2025-04-25 23:52:57,324 - INFO - Batch: 80/141, Loss: 1.7100, Perplexity: 5.5291, Time: 0.28s
2025-04-25 23:52:57,560 - INFO - Batch: 90/141, Loss: 1.7082, Perplexity: 5.5188, Time: 0.24s
2025-04-25 23:52:57,787 - INFO - Batch: 100/141, Loss: 1.7251, Perplexity: 5.6131, Time: 0.23s
2025-04-25 23:52:58,013 - INFO - Batch: 110/141, Loss: 1.7128, Perplexity: 5.5445, Time: 0.23s
2025-04-25 23:52:58,237 - INFO - Batch: 120/141, Loss: 1.7176, Perplexity: 5.5714, Time: 0.22s
2025-04-25 23:52:58,478 - INFO - Batch: 130/141, Loss: 1.7027, Perplexity: 5.4889, Time: 0.24s
2025-04-25 23:52:58,708 - INFO - Batch: 140/141, Loss: 1.7193, Perplexity: 5.5804, Time: 0.23s
2025-04-25 23:52:59,362 - INFO - Validation  Loss: 1.8856, Perplexity: 6.5905, Time: 0.63s
2025-04-25 23:52:59,363 - INFO - Epoch 95/300 - Train Loss: 1.7175, Val Loss: 1.8856
2025-04-25 23:52:59,580 - INFO - Batch: 10/141, Loss: 1.7086, Perplexity: 5.5211, Time: 0.22s
2025-04-25 23:52:59,810 - INFO - Batch: 20/141, Loss: 1.7231, Perplexity: 5.6020, Time: 0.23s
2025-04-25 23:53:00,025 - INFO - Batch: 30/141, Loss: 1.7072, Perplexity: 5.5137, Time: 0.22s
2025-04-25 23:53:00,248 - INFO - Batch: 40/141, Loss: 1.7112, Perplexity: 5.5356, Time: 0.22s
2025-04-25 23:53:00,554 - INFO - Batch: 50/141, Loss: 1.7197, Perplexity: 5.5830, Time: 0.31s
2025-04-25 23:53:00,777 - INFO - Batch: 60/141, Loss: 1.7267, Perplexity: 5.6222, Time: 0.22s
2025-04-25 23:53:00,995 - INFO - Batch: 70/141, Loss: 1.7245, Perplexity: 5.6095, Time: 0.22s
2025-04-25 23:53:01,210 - INFO - Batch: 80/141, Loss: 1.7191, Perplexity: 5.5797, Time: 0.21s
2025-04-25 23:53:01,423 - INFO - Batch: 90/141, Loss: 1.7221, Perplexity: 5.5963, Time: 0.21s
2025-04-25 23:53:01,643 - INFO - Batch: 100/141, Loss: 1.7188, Perplexity: 5.5780, Time: 0.22s
2025-04-25 23:53:01,864 - INFO - Batch: 110/141, Loss: 1.7140, Perplexity: 5.5508, Time: 0.22s
2025-04-25 23:53:02,142 - INFO - Batch: 120/141, Loss: 1.7309, Perplexity: 5.6458, Time: 0.28s
2025-04-25 23:53:02,360 - INFO - Batch: 130/141, Loss: 1.7306, Perplexity: 5.6440, Time: 0.22s
2025-04-25 23:53:02,594 - INFO - Batch: 140/141, Loss: 1.7096, Perplexity: 5.5266, Time: 0.23s
2025-04-25 23:53:03,162 - INFO - Validation  Loss: 1.8851, Perplexity: 6.5870, Time: 0.54s
2025-04-25 23:53:03,162 - INFO - Epoch 96/300 - Train Loss: 1.7172, Val Loss: 1.8851
2025-04-25 23:53:03,437 - INFO - Batch: 10/141, Loss: 1.7172, Perplexity: 5.5689, Time: 0.28s
2025-04-25 23:53:03,670 - INFO - Batch: 20/141, Loss: 1.7085, Perplexity: 5.5206, Time: 0.23s
2025-04-25 23:53:03,892 - INFO - Batch: 30/141, Loss: 1.7205, Perplexity: 5.5873, Time: 0.22s
2025-04-25 23:53:04,134 - INFO - Batch: 40/141, Loss: 1.7027, Perplexity: 5.4888, Time: 0.24s
2025-04-25 23:53:04,363 - INFO - Batch: 50/141, Loss: 1.7187, Perplexity: 5.5773, Time: 0.23s
2025-04-25 23:53:04,597 - INFO - Batch: 60/141, Loss: 1.7217, Perplexity: 5.5941, Time: 0.23s
2025-04-25 23:53:04,835 - INFO - Batch: 70/141, Loss: 1.7050, Perplexity: 5.5011, Time: 0.24s
2025-04-25 23:53:05,058 - INFO - Batch: 80/141, Loss: 1.7293, Perplexity: 5.6365, Time: 0.22s
2025-04-25 23:53:05,339 - INFO - Batch: 90/141, Loss: 1.7234, Perplexity: 5.6037, Time: 0.28s
2025-04-25 23:53:05,551 - INFO - Batch: 100/141, Loss: 1.7133, Perplexity: 5.5472, Time: 0.21s
2025-04-25 23:53:05,778 - INFO - Batch: 110/141, Loss: 1.7287, Perplexity: 5.6334, Time: 0.23s
2025-04-25 23:53:05,996 - INFO - Batch: 120/141, Loss: 1.7209, Perplexity: 5.5894, Time: 0.22s
2025-04-25 23:53:06,218 - INFO - Batch: 130/141, Loss: 1.7216, Perplexity: 5.5935, Time: 0.22s
2025-04-25 23:53:06,446 - INFO - Batch: 140/141, Loss: 1.7200, Perplexity: 5.5846, Time: 0.23s
2025-04-25 23:53:07,072 - INFO - Validation  Loss: 1.8852, Perplexity: 6.5876, Time: 0.60s
2025-04-25 23:53:07,072 - INFO - Epoch 97/300 - Train Loss: 1.7170, Val Loss: 1.8852
2025-04-25 23:53:07,289 - INFO - Batch: 10/141, Loss: 1.7059, Perplexity: 5.5064, Time: 0.22s
2025-04-25 23:53:07,504 - INFO - Batch: 20/141, Loss: 1.7172, Perplexity: 5.5690, Time: 0.22s
2025-04-25 23:53:07,735 - INFO - Batch: 30/141, Loss: 1.7086, Perplexity: 5.5210, Time: 0.23s
2025-04-25 23:53:07,968 - INFO - Batch: 40/141, Loss: 1.7225, Perplexity: 5.5987, Time: 0.23s
2025-04-25 23:53:08,262 - INFO - Batch: 50/141, Loss: 1.7181, Perplexity: 5.5737, Time: 0.29s
2025-04-25 23:53:08,498 - INFO - Batch: 60/141, Loss: 1.7194, Perplexity: 5.5814, Time: 0.24s
2025-04-25 23:53:08,722 - INFO - Batch: 70/141, Loss: 1.7133, Perplexity: 5.5475, Time: 0.22s
2025-04-25 23:53:08,949 - INFO - Batch: 80/141, Loss: 1.7169, Perplexity: 5.5671, Time: 0.23s
2025-04-25 23:53:09,176 - INFO - Batch: 90/141, Loss: 1.7244, Perplexity: 5.6091, Time: 0.23s
2025-04-25 23:53:09,397 - INFO - Batch: 100/141, Loss: 1.7155, Perplexity: 5.5594, Time: 0.22s
2025-04-25 23:53:09,622 - INFO - Batch: 110/141, Loss: 1.7139, Perplexity: 5.5508, Time: 0.22s
2025-04-25 23:53:09,853 - INFO - Batch: 120/141, Loss: 1.7317, Perplexity: 5.6502, Time: 0.23s
2025-04-25 23:53:10,136 - INFO - Batch: 130/141, Loss: 1.7176, Perplexity: 5.5711, Time: 0.28s
2025-04-25 23:53:10,404 - INFO - Batch: 140/141, Loss: 1.7153, Perplexity: 5.5585, Time: 0.27s
2025-04-25 23:53:11,003 - INFO - Validation  Loss: 1.8849, Perplexity: 6.5854, Time: 0.57s
2025-04-25 23:53:11,003 - INFO - Epoch 98/300 - Train Loss: 1.7167, Val Loss: 1.8849
2025-04-25 23:53:11,230 - INFO - Batch: 10/141, Loss: 1.7142, Perplexity: 5.5523, Time: 0.23s
2025-04-25 23:53:11,512 - INFO - Batch: 20/141, Loss: 1.7083, Perplexity: 5.5197, Time: 0.28s
2025-04-25 23:53:11,731 - INFO - Batch: 30/141, Loss: 1.7161, Perplexity: 5.5627, Time: 0.22s
2025-04-25 23:53:11,947 - INFO - Batch: 40/141, Loss: 1.7186, Perplexity: 5.5769, Time: 0.22s
2025-04-25 23:53:12,163 - INFO - Batch: 50/141, Loss: 1.7154, Perplexity: 5.5589, Time: 0.22s
2025-04-25 23:53:12,382 - INFO - Batch: 60/141, Loss: 1.7280, Perplexity: 5.6293, Time: 0.22s
2025-04-25 23:53:12,623 - INFO - Batch: 70/141, Loss: 1.7313, Perplexity: 5.6477, Time: 0.24s
2025-04-25 23:53:12,893 - INFO - Batch: 80/141, Loss: 1.6990, Perplexity: 5.4687, Time: 0.27s
2025-04-25 23:53:13,117 - INFO - Batch: 90/141, Loss: 1.7209, Perplexity: 5.5893, Time: 0.22s
2025-04-25 23:53:13,413 - INFO - Batch: 100/141, Loss: 1.7099, Perplexity: 5.5284, Time: 0.30s
2025-04-25 23:53:13,641 - INFO - Batch: 110/141, Loss: 1.7401, Perplexity: 5.6977, Time: 0.23s
2025-04-25 23:53:13,871 - INFO - Batch: 120/141, Loss: 1.7303, Perplexity: 5.6423, Time: 0.23s
2025-04-25 23:53:14,098 - INFO - Batch: 130/141, Loss: 1.7139, Perplexity: 5.5504, Time: 0.23s
2025-04-25 23:53:14,331 - INFO - Batch: 140/141, Loss: 1.7192, Perplexity: 5.5802, Time: 0.23s
2025-04-25 23:53:15,011 - INFO - Validation  Loss: 1.8844, Perplexity: 6.5821, Time: 0.64s
2025-04-25 23:53:15,011 - INFO - Epoch 99/300 - Train Loss: 1.7165, Val Loss: 1.8844
2025-04-25 23:53:15,255 - INFO - Batch: 10/141, Loss: 1.7127, Perplexity: 5.5439, Time: 0.24s
2025-04-25 23:53:15,476 - INFO - Batch: 20/141, Loss: 1.7254, Perplexity: 5.6149, Time: 0.22s
2025-04-25 23:53:15,691 - INFO - Batch: 30/141, Loss: 1.7019, Perplexity: 5.4843, Time: 0.21s
2025-04-25 23:53:15,913 - INFO - Batch: 40/141, Loss: 1.6982, Perplexity: 5.4642, Time: 0.22s
2025-04-25 23:53:16,142 - INFO - Batch: 50/141, Loss: 1.7116, Perplexity: 5.5378, Time: 0.23s
2025-04-25 23:53:16,421 - INFO - Batch: 60/141, Loss: 1.7334, Perplexity: 5.6600, Time: 0.28s
2025-04-25 23:53:16,645 - INFO - Batch: 70/141, Loss: 1.7050, Perplexity: 5.5015, Time: 0.22s
2025-04-25 23:53:16,865 - INFO - Batch: 80/141, Loss: 1.7183, Perplexity: 5.5752, Time: 0.22s
2025-04-25 23:53:17,091 - INFO - Batch: 90/141, Loss: 1.7199, Perplexity: 5.5842, Time: 0.23s
2025-04-25 23:53:17,311 - INFO - Batch: 100/141, Loss: 1.7194, Perplexity: 5.5813, Time: 0.22s
2025-04-25 23:53:17,532 - INFO - Batch: 110/141, Loss: 1.6989, Perplexity: 5.4680, Time: 0.22s
2025-04-25 23:53:17,746 - INFO - Batch: 120/141, Loss: 1.7222, Perplexity: 5.5966, Time: 0.21s
2025-04-25 23:53:17,966 - INFO - Batch: 130/141, Loss: 1.7224, Perplexity: 5.5979, Time: 0.22s
2025-04-25 23:53:18,244 - INFO - Batch: 140/141, Loss: 1.7073, Perplexity: 5.5142, Time: 0.28s
2025-04-25 23:53:18,793 - INFO - Validation  Loss: 1.8845, Perplexity: 6.5831, Time: 0.52s
2025-04-25 23:53:18,793 - INFO - Epoch 100/300 - Train Loss: 1.7163, Val Loss: 1.8845
2025-04-25 23:53:19,026 - INFO - Batch: 10/141, Loss: 1.7257, Perplexity: 5.6164, Time: 0.23s
2025-04-25 23:53:19,247 - INFO - Batch: 20/141, Loss: 1.7307, Perplexity: 5.6448, Time: 0.22s
2025-04-25 23:53:19,526 - INFO - Batch: 30/141, Loss: 1.7148, Perplexity: 5.5557, Time: 0.28s
2025-04-25 23:53:19,772 - INFO - Batch: 40/141, Loss: 1.7301, Perplexity: 5.6411, Time: 0.25s
2025-04-25 23:53:20,049 - INFO - Batch: 50/141, Loss: 1.7291, Perplexity: 5.6358, Time: 0.28s
2025-04-25 23:53:20,350 - INFO - Batch: 60/141, Loss: 1.7422, Perplexity: 5.7098, Time: 0.30s
2025-04-25 23:53:20,663 - INFO - Batch: 70/141, Loss: 1.7322, Perplexity: 5.6530, Time: 0.31s
2025-04-25 23:53:20,937 - INFO - Batch: 80/141, Loss: 1.7198, Perplexity: 5.5833, Time: 0.27s
2025-04-25 23:53:21,170 - INFO - Batch: 90/141, Loss: 1.7039, Perplexity: 5.4954, Time: 0.23s
2025-04-25 23:53:21,395 - INFO - Batch: 100/141, Loss: 1.7036, Perplexity: 5.4939, Time: 0.23s
2025-04-25 23:53:21,685 - INFO - Batch: 110/141, Loss: 1.7061, Perplexity: 5.5075, Time: 0.29s
2025-04-25 23:53:21,907 - INFO - Batch: 120/141, Loss: 1.7123, Perplexity: 5.5418, Time: 0.22s
2025-04-25 23:53:22,146 - INFO - Batch: 130/141, Loss: 1.7114, Perplexity: 5.5369, Time: 0.24s
2025-04-25 23:53:22,371 - INFO - Batch: 140/141, Loss: 1.7113, Perplexity: 5.5362, Time: 0.22s
2025-04-25 23:53:23,068 - INFO - Validation  Loss: 1.8842, Perplexity: 6.5808, Time: 0.66s
2025-04-25 23:53:23,068 - INFO - Epoch 101/300 - Train Loss: 1.7160, Val Loss: 1.8842
2025-04-25 23:53:23,327 - INFO - Batch: 10/141, Loss: 1.7179, Perplexity: 5.5728, Time: 0.26s
2025-04-25 23:53:23,567 - INFO - Batch: 20/141, Loss: 1.7128, Perplexity: 5.5444, Time: 0.24s
2025-04-25 23:53:23,815 - INFO - Batch: 30/141, Loss: 1.7088, Perplexity: 5.5225, Time: 0.25s
2025-04-25 23:53:24,060 - INFO - Batch: 40/141, Loss: 1.7031, Perplexity: 5.4912, Time: 0.24s
2025-04-25 23:53:24,289 - INFO - Batch: 50/141, Loss: 1.7203, Perplexity: 5.5862, Time: 0.23s
2025-04-25 23:53:24,516 - INFO - Batch: 60/141, Loss: 1.7302, Perplexity: 5.6418, Time: 0.23s
2025-04-25 23:53:24,841 - INFO - Batch: 70/141, Loss: 1.7213, Perplexity: 5.5917, Time: 0.33s
2025-04-25 23:53:25,067 - INFO - Batch: 80/141, Loss: 1.7295, Perplexity: 5.6377, Time: 0.23s
2025-04-25 23:53:25,298 - INFO - Batch: 90/141, Loss: 1.7238, Perplexity: 5.6055, Time: 0.23s
2025-04-25 23:53:25,517 - INFO - Batch: 100/141, Loss: 1.7089, Perplexity: 5.5231, Time: 0.22s
2025-04-25 23:53:25,742 - INFO - Batch: 110/141, Loss: 1.7121, Perplexity: 5.5404, Time: 0.23s
2025-04-25 23:53:25,961 - INFO - Batch: 120/141, Loss: 1.7195, Perplexity: 5.5816, Time: 0.22s
2025-04-25 23:53:26,192 - INFO - Batch: 130/141, Loss: 1.7170, Perplexity: 5.5678, Time: 0.23s
2025-04-25 23:53:26,459 - INFO - Batch: 140/141, Loss: 1.7050, Perplexity: 5.5013, Time: 0.27s
2025-04-25 23:53:27,139 - INFO - Validation  Loss: 1.8840, Perplexity: 6.5797, Time: 0.65s
2025-04-25 23:53:27,139 - INFO - Epoch 102/300 - Train Loss: 1.7158, Val Loss: 1.8840
2025-04-25 23:53:27,358 - INFO - Batch: 10/141, Loss: 1.7167, Perplexity: 5.5663, Time: 0.22s
2025-04-25 23:53:27,582 - INFO - Batch: 20/141, Loss: 1.7205, Perplexity: 5.5871, Time: 0.22s
2025-04-25 23:53:27,803 - INFO - Batch: 30/141, Loss: 1.7237, Perplexity: 5.6055, Time: 0.22s
2025-04-25 23:53:28,084 - INFO - Batch: 40/141, Loss: 1.7150, Perplexity: 5.5565, Time: 0.28s
2025-04-25 23:53:28,300 - INFO - Batch: 50/141, Loss: 1.7131, Perplexity: 5.5463, Time: 0.22s
2025-04-25 23:53:28,530 - INFO - Batch: 60/141, Loss: 1.7099, Perplexity: 5.5285, Time: 0.23s
2025-04-25 23:53:28,764 - INFO - Batch: 70/141, Loss: 1.7172, Perplexity: 5.5687, Time: 0.23s
2025-04-25 23:53:28,988 - INFO - Batch: 80/141, Loss: 1.7110, Perplexity: 5.5347, Time: 0.22s
2025-04-25 23:53:29,216 - INFO - Batch: 90/141, Loss: 1.7032, Perplexity: 5.4913, Time: 0.23s
2025-04-25 23:53:29,444 - INFO - Batch: 100/141, Loss: 1.7260, Perplexity: 5.6183, Time: 0.23s
2025-04-25 23:53:29,672 - INFO - Batch: 110/141, Loss: 1.7080, Perplexity: 5.5179, Time: 0.23s
2025-04-25 23:53:29,966 - INFO - Batch: 120/141, Loss: 1.7267, Perplexity: 5.6219, Time: 0.29s
2025-04-25 23:53:30,200 - INFO - Batch: 130/141, Loss: 1.7043, Perplexity: 5.4977, Time: 0.23s
2025-04-25 23:53:30,422 - INFO - Batch: 140/141, Loss: 1.7259, Perplexity: 5.6175, Time: 0.22s
2025-04-25 23:53:31,096 - INFO - Validation  Loss: 1.8837, Perplexity: 6.5777, Time: 0.63s
2025-04-25 23:53:31,096 - INFO - Epoch 103/300 - Train Loss: 1.7156, Val Loss: 1.8837
2025-04-25 23:53:31,324 - INFO - Batch: 10/141, Loss: 1.7002, Perplexity: 5.4752, Time: 0.23s
2025-04-25 23:53:31,546 - INFO - Batch: 20/141, Loss: 1.7011, Perplexity: 5.4801, Time: 0.22s
2025-04-25 23:53:31,778 - INFO - Batch: 30/141, Loss: 1.7261, Perplexity: 5.6187, Time: 0.23s
2025-04-25 23:53:32,021 - INFO - Batch: 40/141, Loss: 1.7109, Perplexity: 5.5340, Time: 0.24s
2025-04-25 23:53:32,327 - INFO - Batch: 50/141, Loss: 1.7031, Perplexity: 5.4907, Time: 0.31s
2025-04-25 23:53:32,579 - INFO - Batch: 60/141, Loss: 1.7143, Perplexity: 5.5527, Time: 0.25s
2025-04-25 23:53:32,821 - INFO - Batch: 70/141, Loss: 1.7180, Perplexity: 5.5731, Time: 0.24s
2025-04-25 23:53:33,117 - INFO - Batch: 80/141, Loss: 1.7206, Perplexity: 5.5879, Time: 0.30s
2025-04-25 23:53:33,342 - INFO - Batch: 90/141, Loss: 1.7143, Perplexity: 5.5530, Time: 0.23s
2025-04-25 23:53:33,563 - INFO - Batch: 100/141, Loss: 1.7218, Perplexity: 5.5945, Time: 0.22s
2025-04-25 23:53:33,782 - INFO - Batch: 110/141, Loss: 1.7113, Perplexity: 5.5363, Time: 0.22s
2025-04-25 23:53:34,000 - INFO - Batch: 120/141, Loss: 1.7222, Perplexity: 5.5966, Time: 0.22s
2025-04-25 23:53:34,260 - INFO - Batch: 130/141, Loss: 1.7187, Perplexity: 5.5774, Time: 0.26s
2025-04-25 23:53:34,541 - INFO - Batch: 140/141, Loss: 1.6986, Perplexity: 5.4662, Time: 0.28s
2025-04-25 23:53:35,273 - INFO - Validation  Loss: 1.8837, Perplexity: 6.5775, Time: 0.69s
2025-04-25 23:53:35,273 - INFO - Epoch 104/300 - Train Loss: 1.7154, Val Loss: 1.8837
2025-04-25 23:53:35,489 - INFO - Batch: 10/141, Loss: 1.7126, Perplexity: 5.5435, Time: 0.22s
2025-04-25 23:53:35,706 - INFO - Batch: 20/141, Loss: 1.7273, Perplexity: 5.6255, Time: 0.22s
2025-04-25 23:53:35,939 - INFO - Batch: 30/141, Loss: 1.6976, Perplexity: 5.4606, Time: 0.23s
2025-04-25 23:53:36,160 - INFO - Batch: 40/141, Loss: 1.7190, Perplexity: 5.5791, Time: 0.22s
2025-04-25 23:53:36,447 - INFO - Batch: 50/141, Loss: 1.7137, Perplexity: 5.5492, Time: 0.29s
2025-04-25 23:53:36,667 - INFO - Batch: 60/141, Loss: 1.7145, Perplexity: 5.5536, Time: 0.22s
2025-04-25 23:53:36,912 - INFO - Batch: 70/141, Loss: 1.7256, Perplexity: 5.6160, Time: 0.24s
2025-04-25 23:53:37,192 - INFO - Batch: 80/141, Loss: 1.7197, Perplexity: 5.5831, Time: 0.28s
2025-04-25 23:53:37,427 - INFO - Batch: 90/141, Loss: 1.7285, Perplexity: 5.6323, Time: 0.23s
2025-04-25 23:53:37,644 - INFO - Batch: 100/141, Loss: 1.7154, Perplexity: 5.5592, Time: 0.22s
2025-04-25 23:53:37,861 - INFO - Batch: 110/141, Loss: 1.7158, Perplexity: 5.5612, Time: 0.22s
2025-04-25 23:53:38,147 - INFO - Batch: 120/141, Loss: 1.7303, Perplexity: 5.6426, Time: 0.29s
2025-04-25 23:53:38,364 - INFO - Batch: 130/141, Loss: 1.6945, Perplexity: 5.4438, Time: 0.22s
2025-04-25 23:53:38,592 - INFO - Batch: 140/141, Loss: 1.7203, Perplexity: 5.5864, Time: 0.23s
2025-04-25 23:53:39,180 - INFO - Validation  Loss: 1.8836, Perplexity: 6.5768, Time: 0.56s
2025-04-25 23:53:39,180 - INFO - Epoch 105/300 - Train Loss: 1.7152, Val Loss: 1.8836
2025-04-25 23:53:39,466 - INFO - Batch: 10/141, Loss: 1.7057, Perplexity: 5.5055, Time: 0.29s
2025-04-25 23:53:39,680 - INFO - Batch: 20/141, Loss: 1.7092, Perplexity: 5.5245, Time: 0.21s
2025-04-25 23:53:39,902 - INFO - Batch: 30/141, Loss: 1.7146, Perplexity: 5.5546, Time: 0.22s
2025-04-25 23:53:40,132 - INFO - Batch: 40/141, Loss: 1.7016, Perplexity: 5.4826, Time: 0.23s
2025-04-25 23:53:40,349 - INFO - Batch: 50/141, Loss: 1.7170, Perplexity: 5.5677, Time: 0.22s
2025-04-25 23:53:40,639 - INFO - Batch: 60/141, Loss: 1.7228, Perplexity: 5.6005, Time: 0.29s
2025-04-25 23:53:40,971 - INFO - Batch: 70/141, Loss: 1.7138, Perplexity: 5.5499, Time: 0.33s
2025-04-25 23:53:41,280 - INFO - Batch: 80/141, Loss: 1.7130, Perplexity: 5.5453, Time: 0.31s
2025-04-25 23:53:41,678 - INFO - Batch: 90/141, Loss: 1.7166, Perplexity: 5.5657, Time: 0.40s
2025-04-25 23:53:41,972 - INFO - Batch: 100/141, Loss: 1.7122, Perplexity: 5.5412, Time: 0.29s
2025-04-25 23:53:42,261 - INFO - Batch: 110/141, Loss: 1.7190, Perplexity: 5.5792, Time: 0.29s
2025-04-25 23:53:42,546 - INFO - Batch: 120/141, Loss: 1.7044, Perplexity: 5.4982, Time: 0.28s
2025-04-25 23:53:42,832 - INFO - Batch: 130/141, Loss: 1.7128, Perplexity: 5.5446, Time: 0.29s
2025-04-25 23:53:43,149 - INFO - Batch: 140/141, Loss: 1.7059, Perplexity: 5.5062, Time: 0.32s
2025-04-25 23:53:43,940 - INFO - Validation  Loss: 1.8833, Perplexity: 6.5754, Time: 0.75s
2025-04-25 23:53:43,940 - INFO - Epoch 106/300 - Train Loss: 1.7150, Val Loss: 1.8833
2025-04-25 23:53:44,165 - INFO - Batch: 10/141, Loss: 1.7094, Perplexity: 5.5259, Time: 0.23s
2025-04-25 23:53:44,383 - INFO - Batch: 20/141, Loss: 1.7248, Perplexity: 5.6111, Time: 0.22s
2025-04-25 23:53:44,639 - INFO - Batch: 30/141, Loss: 1.7058, Perplexity: 5.5058, Time: 0.25s
2025-04-25 23:53:44,863 - INFO - Batch: 40/141, Loss: 1.7139, Perplexity: 5.5507, Time: 0.22s
2025-04-25 23:53:45,116 - INFO - Batch: 50/141, Loss: 1.7114, Perplexity: 5.5367, Time: 0.25s
2025-04-25 23:53:45,431 - INFO - Batch: 60/141, Loss: 1.7363, Perplexity: 5.6765, Time: 0.32s
2025-04-25 23:53:45,657 - INFO - Batch: 70/141, Loss: 1.7169, Perplexity: 5.5670, Time: 0.23s
2025-04-25 23:53:45,899 - INFO - Batch: 80/141, Loss: 1.7195, Perplexity: 5.5817, Time: 0.24s
2025-04-25 23:53:46,189 - INFO - Batch: 90/141, Loss: 1.7119, Perplexity: 5.5397, Time: 0.29s
2025-04-25 23:53:46,446 - INFO - Batch: 100/141, Loss: 1.7210, Perplexity: 5.5901, Time: 0.26s
2025-04-25 23:53:46,669 - INFO - Batch: 110/141, Loss: 1.7177, Perplexity: 5.5720, Time: 0.22s
2025-04-25 23:53:46,973 - INFO - Batch: 120/141, Loss: 1.7278, Perplexity: 5.6285, Time: 0.30s
2025-04-25 23:53:47,281 - INFO - Batch: 130/141, Loss: 1.7109, Perplexity: 5.5341, Time: 0.31s
2025-04-25 23:53:47,650 - INFO - Batch: 140/141, Loss: 1.7168, Perplexity: 5.5665, Time: 0.37s
2025-04-25 23:53:48,383 - INFO - Validation  Loss: 1.8829, Perplexity: 6.5727, Time: 0.69s
2025-04-25 23:53:48,384 - INFO - Epoch 107/300 - Train Loss: 1.7147, Val Loss: 1.8829
2025-04-25 23:53:48,603 - INFO - Batch: 10/141, Loss: 1.7155, Perplexity: 5.5593, Time: 0.22s
2025-04-25 23:53:48,896 - INFO - Batch: 20/141, Loss: 1.7083, Perplexity: 5.5194, Time: 0.29s
2025-04-25 23:53:49,142 - INFO - Batch: 30/141, Loss: 1.7090, Perplexity: 5.5232, Time: 0.25s
2025-04-25 23:53:49,360 - INFO - Batch: 40/141, Loss: 1.7148, Perplexity: 5.5557, Time: 0.22s
2025-04-25 23:53:49,598 - INFO - Batch: 50/141, Loss: 1.6985, Perplexity: 5.4658, Time: 0.24s
2025-04-25 23:53:49,818 - INFO - Batch: 60/141, Loss: 1.7102, Perplexity: 5.5299, Time: 0.22s
2025-04-25 23:53:50,043 - INFO - Batch: 70/141, Loss: 1.7196, Perplexity: 5.5822, Time: 0.23s
2025-04-25 23:53:50,270 - INFO - Batch: 80/141, Loss: 1.7241, Perplexity: 5.6074, Time: 0.23s
2025-04-25 23:53:50,513 - INFO - Batch: 90/141, Loss: 1.7219, Perplexity: 5.5951, Time: 0.24s
2025-04-25 23:53:50,806 - INFO - Batch: 100/141, Loss: 1.7174, Perplexity: 5.5700, Time: 0.29s
2025-04-25 23:53:51,030 - INFO - Batch: 110/141, Loss: 1.6998, Perplexity: 5.4726, Time: 0.22s
2025-04-25 23:53:51,258 - INFO - Batch: 120/141, Loss: 1.7205, Perplexity: 5.5871, Time: 0.23s
2025-04-25 23:53:51,483 - INFO - Batch: 130/141, Loss: 1.7226, Perplexity: 5.5988, Time: 0.22s
2025-04-25 23:53:51,704 - INFO - Batch: 140/141, Loss: 1.7215, Perplexity: 5.5929, Time: 0.22s
2025-04-25 23:53:52,435 - INFO - Validation  Loss: 1.8828, Perplexity: 6.5721, Time: 0.69s
2025-04-25 23:53:52,435 - INFO - Epoch 108/300 - Train Loss: 1.7145, Val Loss: 1.8828
2025-04-25 23:53:52,669 - INFO - Batch: 10/141, Loss: 1.7164, Perplexity: 5.5647, Time: 0.23s
2025-04-25 23:53:52,909 - INFO - Batch: 20/141, Loss: 1.7295, Perplexity: 5.6377, Time: 0.24s
2025-04-25 23:53:53,144 - INFO - Batch: 30/141, Loss: 1.7150, Perplexity: 5.5568, Time: 0.23s
2025-04-25 23:53:53,368 - INFO - Batch: 40/141, Loss: 1.7092, Perplexity: 5.5244, Time: 0.22s
2025-04-25 23:53:53,607 - INFO - Batch: 50/141, Loss: 1.7101, Perplexity: 5.5297, Time: 0.24s
2025-04-25 23:53:53,841 - INFO - Batch: 60/141, Loss: 1.7224, Perplexity: 5.5980, Time: 0.23s
2025-04-25 23:53:54,118 - INFO - Batch: 70/141, Loss: 1.7123, Perplexity: 5.5416, Time: 0.28s
2025-04-25 23:53:54,338 - INFO - Batch: 80/141, Loss: 1.6989, Perplexity: 5.4679, Time: 0.22s
2025-04-25 23:53:54,570 - INFO - Batch: 90/141, Loss: 1.7118, Perplexity: 5.5390, Time: 0.23s
2025-04-25 23:53:54,803 - INFO - Batch: 100/141, Loss: 1.7288, Perplexity: 5.6341, Time: 0.23s
2025-04-25 23:53:55,098 - INFO - Batch: 110/141, Loss: 1.7229, Perplexity: 5.6007, Time: 0.29s
2025-04-25 23:53:55,388 - INFO - Batch: 120/141, Loss: 1.7209, Perplexity: 5.5895, Time: 0.29s
2025-04-25 23:53:55,694 - INFO - Batch: 130/141, Loss: 1.7036, Perplexity: 5.4936, Time: 0.31s
2025-04-25 23:53:56,073 - INFO - Batch: 140/141, Loss: 1.7189, Perplexity: 5.5781, Time: 0.38s
2025-04-25 23:53:56,827 - INFO - Validation  Loss: 1.8830, Perplexity: 6.5729, Time: 0.72s
2025-04-25 23:53:56,828 - INFO - Epoch 109/300 - Train Loss: 1.7143, Val Loss: 1.8830
2025-04-25 23:53:57,123 - INFO - Batch: 10/141, Loss: 1.7317, Perplexity: 5.6504, Time: 0.30s
2025-04-25 23:53:57,419 - INFO - Batch: 20/141, Loss: 1.7147, Perplexity: 5.5550, Time: 0.30s
2025-04-25 23:53:57,808 - INFO - Batch: 30/141, Loss: 1.7095, Perplexity: 5.5260, Time: 0.39s
2025-04-25 23:53:58,107 - INFO - Batch: 40/141, Loss: 1.7044, Perplexity: 5.4982, Time: 0.30s
2025-04-25 23:53:58,398 - INFO - Batch: 50/141, Loss: 1.7045, Perplexity: 5.4986, Time: 0.29s
2025-04-25 23:53:58,693 - INFO - Batch: 60/141, Loss: 1.7177, Perplexity: 5.5715, Time: 0.29s
2025-04-25 23:53:58,992 - INFO - Batch: 70/141, Loss: 1.7072, Perplexity: 5.5135, Time: 0.30s
2025-04-25 23:53:59,285 - INFO - Batch: 80/141, Loss: 1.7039, Perplexity: 5.4954, Time: 0.29s
2025-04-25 23:53:59,594 - INFO - Batch: 90/141, Loss: 1.7190, Perplexity: 5.5787, Time: 0.31s
2025-04-25 23:53:59,902 - INFO - Batch: 100/141, Loss: 1.7151, Perplexity: 5.5574, Time: 0.31s
2025-04-25 23:54:00,278 - INFO - Batch: 110/141, Loss: 1.7004, Perplexity: 5.4759, Time: 0.38s
2025-04-25 23:54:00,563 - INFO - Batch: 120/141, Loss: 1.6986, Perplexity: 5.4661, Time: 0.28s
2025-04-25 23:54:00,858 - INFO - Batch: 130/141, Loss: 1.7104, Perplexity: 5.5310, Time: 0.30s
2025-04-25 23:54:01,228 - INFO - Batch: 140/141, Loss: 1.7142, Perplexity: 5.5524, Time: 0.37s
2025-04-25 23:54:02,116 - INFO - Validation  Loss: 1.8824, Perplexity: 6.5695, Time: 0.84s
2025-04-25 23:54:02,116 - INFO - Epoch 110/300 - Train Loss: 1.7141, Val Loss: 1.8824
2025-04-25 23:54:02,425 - INFO - Batch: 10/141, Loss: 1.7099, Perplexity: 5.5287, Time: 0.31s
2025-04-25 23:54:02,716 - INFO - Batch: 20/141, Loss: 1.7062, Perplexity: 5.5079, Time: 0.29s
2025-04-25 23:54:03,002 - INFO - Batch: 30/141, Loss: 1.7307, Perplexity: 5.6447, Time: 0.29s
2025-04-25 23:54:03,295 - INFO - Batch: 40/141, Loss: 1.7050, Perplexity: 5.5014, Time: 0.29s
2025-04-25 23:54:03,600 - INFO - Batch: 50/141, Loss: 1.7227, Perplexity: 5.5995, Time: 0.31s
2025-04-25 23:54:03,916 - INFO - Batch: 60/141, Loss: 1.7137, Perplexity: 5.5497, Time: 0.32s
2025-04-25 23:54:04,292 - INFO - Batch: 70/141, Loss: 1.7115, Perplexity: 5.5374, Time: 0.38s
2025-04-25 23:54:04,581 - INFO - Batch: 80/141, Loss: 1.7071, Perplexity: 5.5128, Time: 0.29s
2025-04-25 23:54:04,867 - INFO - Batch: 90/141, Loss: 1.7211, Perplexity: 5.5907, Time: 0.29s
2025-04-25 23:54:05,158 - INFO - Batch: 100/141, Loss: 1.7074, Perplexity: 5.5148, Time: 0.29s
2025-04-25 23:54:05,452 - INFO - Batch: 110/141, Loss: 1.6942, Perplexity: 5.4422, Time: 0.29s
2025-04-25 23:54:05,764 - INFO - Batch: 120/141, Loss: 1.7067, Perplexity: 5.5110, Time: 0.31s
2025-04-25 23:54:06,060 - INFO - Batch: 130/141, Loss: 1.7062, Perplexity: 5.5081, Time: 0.30s
2025-04-25 23:54:06,349 - INFO - Batch: 140/141, Loss: 1.7049, Perplexity: 5.5008, Time: 0.29s
2025-04-25 23:54:07,192 - INFO - Validation  Loss: 1.8815, Perplexity: 6.5635, Time: 0.81s
2025-04-25 23:54:07,192 - INFO - Epoch 111/300 - Train Loss: 1.7139, Val Loss: 1.8815
2025-04-25 23:54:07,480 - INFO - Batch: 10/141, Loss: 1.7103, Perplexity: 5.5305, Time: 0.29s
2025-04-25 23:54:07,775 - INFO - Batch: 20/141, Loss: 1.7094, Perplexity: 5.5254, Time: 0.29s
2025-04-25 23:54:08,074 - INFO - Batch: 30/141, Loss: 1.7099, Perplexity: 5.5282, Time: 0.30s
2025-04-25 23:54:08,452 - INFO - Batch: 40/141, Loss: 1.7265, Perplexity: 5.6208, Time: 0.38s
2025-04-25 23:54:08,742 - INFO - Batch: 50/141, Loss: 1.7180, Perplexity: 5.5735, Time: 0.29s
2025-04-25 23:54:09,032 - INFO - Batch: 60/141, Loss: 1.7046, Perplexity: 5.4990, Time: 0.29s
2025-04-25 23:54:09,327 - INFO - Batch: 70/141, Loss: 1.7285, Perplexity: 5.6321, Time: 0.30s
2025-04-25 23:54:09,626 - INFO - Batch: 80/141, Loss: 1.7256, Perplexity: 5.6159, Time: 0.30s
2025-04-25 23:54:09,932 - INFO - Batch: 90/141, Loss: 1.7157, Perplexity: 5.5604, Time: 0.31s
2025-04-25 23:54:10,243 - INFO - Batch: 100/141, Loss: 1.7131, Perplexity: 5.5460, Time: 0.31s
2025-04-25 23:54:10,540 - INFO - Batch: 110/141, Loss: 1.7237, Perplexity: 5.6050, Time: 0.30s
2025-04-25 23:54:10,921 - INFO - Batch: 120/141, Loss: 1.7241, Perplexity: 5.6075, Time: 0.38s
2025-04-25 23:54:11,222 - INFO - Batch: 130/141, Loss: 1.7031, Perplexity: 5.4910, Time: 0.30s
2025-04-25 23:54:11,523 - INFO - Batch: 140/141, Loss: 1.7152, Perplexity: 5.5580, Time: 0.30s
2025-04-25 23:54:12,307 - INFO - Validation  Loss: 1.8816, Perplexity: 6.5641, Time: 0.75s
2025-04-25 23:54:12,307 - INFO - Epoch 112/300 - Train Loss: 1.7137, Val Loss: 1.8816
2025-04-25 23:54:12,679 - INFO - Batch: 10/141, Loss: 1.7071, Perplexity: 5.5130, Time: 0.37s
2025-04-25 23:54:13,035 - INFO - Batch: 20/141, Loss: 1.7027, Perplexity: 5.4888, Time: 0.36s
2025-04-25 23:54:13,344 - INFO - Batch: 30/141, Loss: 1.7174, Perplexity: 5.5702, Time: 0.31s
2025-04-25 23:54:13,644 - INFO - Batch: 40/141, Loss: 1.7016, Perplexity: 5.4826, Time: 0.30s
2025-04-25 23:54:13,941 - INFO - Batch: 50/141, Loss: 1.7189, Perplexity: 5.5782, Time: 0.30s
2025-04-25 23:54:14,224 - INFO - Batch: 60/141, Loss: 1.7278, Perplexity: 5.6282, Time: 0.28s
2025-04-25 23:54:14,508 - INFO - Batch: 70/141, Loss: 1.7094, Perplexity: 5.5258, Time: 0.28s
2025-04-25 23:54:14,885 - INFO - Batch: 80/141, Loss: 1.7003, Perplexity: 5.4755, Time: 0.38s
2025-04-25 23:54:15,174 - INFO - Batch: 90/141, Loss: 1.7184, Perplexity: 5.5757, Time: 0.29s
2025-04-25 23:54:15,484 - INFO - Batch: 100/141, Loss: 1.7243, Perplexity: 5.6087, Time: 0.31s
2025-04-25 23:54:15,774 - INFO - Batch: 110/141, Loss: 1.7192, Perplexity: 5.5800, Time: 0.29s
2025-04-25 23:54:16,073 - INFO - Batch: 120/141, Loss: 1.7156, Perplexity: 5.5600, Time: 0.30s
2025-04-25 23:54:16,365 - INFO - Batch: 130/141, Loss: 1.7130, Perplexity: 5.5454, Time: 0.29s
2025-04-25 23:54:16,655 - INFO - Batch: 140/141, Loss: 1.7187, Perplexity: 5.5771, Time: 0.29s
2025-04-25 23:54:17,507 - INFO - Validation  Loss: 1.8809, Perplexity: 6.5592, Time: 0.81s
2025-04-25 23:54:17,507 - INFO - Epoch 113/300 - Train Loss: 1.7135, Val Loss: 1.8809
2025-04-25 23:54:17,796 - INFO - Batch: 10/141, Loss: 1.7145, Perplexity: 5.5539, Time: 0.29s
2025-04-25 23:54:18,093 - INFO - Batch: 20/141, Loss: 1.7136, Perplexity: 5.5486, Time: 0.30s
2025-04-25 23:54:18,389 - INFO - Batch: 30/141, Loss: 1.7152, Perplexity: 5.5576, Time: 0.30s
2025-04-25 23:54:18,708 - INFO - Batch: 40/141, Loss: 1.7270, Perplexity: 5.6236, Time: 0.32s
2025-04-25 23:54:19,089 - INFO - Batch: 50/141, Loss: 1.6984, Perplexity: 5.4654, Time: 0.38s
2025-04-25 23:54:19,382 - INFO - Batch: 60/141, Loss: 1.7190, Perplexity: 5.5789, Time: 0.29s
2025-04-25 23:54:19,691 - INFO - Batch: 70/141, Loss: 1.7138, Perplexity: 5.5500, Time: 0.31s
2025-04-25 23:54:20,004 - INFO - Batch: 80/141, Loss: 1.7001, Perplexity: 5.4743, Time: 0.31s
2025-04-25 23:54:20,304 - INFO - Batch: 90/141, Loss: 1.7264, Perplexity: 5.6202, Time: 0.30s
2025-04-25 23:54:20,657 - INFO - Batch: 100/141, Loss: 1.7142, Perplexity: 5.5520, Time: 0.35s
2025-04-25 23:54:20,963 - INFO - Batch: 110/141, Loss: 1.7195, Perplexity: 5.5820, Time: 0.31s
2025-04-25 23:54:21,267 - INFO - Batch: 120/141, Loss: 1.7127, Perplexity: 5.5441, Time: 0.30s
2025-04-25 23:54:21,651 - INFO - Batch: 130/141, Loss: 1.7344, Perplexity: 5.6654, Time: 0.38s
2025-04-25 23:54:21,955 - INFO - Batch: 140/141, Loss: 1.7138, Perplexity: 5.5502, Time: 0.30s
2025-04-25 23:54:22,776 - INFO - Validation  Loss: 1.8809, Perplexity: 6.5596, Time: 0.78s
2025-04-25 23:54:22,776 - INFO - Epoch 114/300 - Train Loss: 1.7134, Val Loss: 1.8809
2025-04-25 23:54:23,070 - INFO - Batch: 10/141, Loss: 1.6987, Perplexity: 5.4669, Time: 0.29s
2025-04-25 23:54:23,452 - INFO - Batch: 20/141, Loss: 1.7056, Perplexity: 5.5045, Time: 0.38s
2025-04-25 23:54:23,811 - INFO - Batch: 30/141, Loss: 1.6952, Perplexity: 5.4479, Time: 0.36s
2025-04-25 23:54:24,143 - INFO - Batch: 40/141, Loss: 1.7146, Perplexity: 5.5544, Time: 0.33s
2025-04-25 23:54:24,507 - INFO - Batch: 50/141, Loss: 1.7252, Perplexity: 5.6137, Time: 0.36s
2025-04-25 23:54:24,842 - INFO - Batch: 60/141, Loss: 1.7064, Perplexity: 5.5092, Time: 0.34s
2025-04-25 23:54:25,264 - INFO - Batch: 70/141, Loss: 1.7208, Perplexity: 5.5888, Time: 0.42s
2025-04-25 23:54:25,609 - INFO - Batch: 80/141, Loss: 1.7029, Perplexity: 5.4898, Time: 0.35s
2025-04-25 23:54:26,048 - INFO - Batch: 90/141, Loss: 1.7228, Perplexity: 5.6001, Time: 0.44s
2025-04-25 23:54:26,408 - INFO - Batch: 100/141, Loss: 1.7012, Perplexity: 5.4807, Time: 0.36s
2025-04-25 23:54:26,749 - INFO - Batch: 110/141, Loss: 1.7123, Perplexity: 5.5419, Time: 0.34s
2025-04-25 23:54:27,052 - INFO - Batch: 120/141, Loss: 1.7117, Perplexity: 5.5384, Time: 0.30s
2025-04-25 23:54:27,342 - INFO - Batch: 130/141, Loss: 1.7217, Perplexity: 5.5940, Time: 0.29s
2025-04-25 23:54:27,684 - INFO - Batch: 140/141, Loss: 1.7247, Perplexity: 5.6109, Time: 0.34s
2025-04-25 23:54:28,657 - INFO - Validation  Loss: 1.8810, Perplexity: 6.5603, Time: 0.92s
2025-04-25 23:54:28,657 - INFO - Epoch 115/300 - Train Loss: 1.7132, Val Loss: 1.8810
2025-04-25 23:54:28,993 - INFO - Batch: 10/141, Loss: 1.7097, Perplexity: 5.5273, Time: 0.34s
2025-04-25 23:54:29,354 - INFO - Batch: 20/141, Loss: 1.7068, Perplexity: 5.5110, Time: 0.36s
2025-04-25 23:54:29,739 - INFO - Batch: 30/141, Loss: 1.6959, Perplexity: 5.4516, Time: 0.39s
2025-04-25 23:54:30,101 - INFO - Batch: 40/141, Loss: 1.6958, Perplexity: 5.4512, Time: 0.36s
2025-04-25 23:54:30,416 - INFO - Batch: 50/141, Loss: 1.7193, Perplexity: 5.5808, Time: 0.32s
2025-04-25 23:54:30,744 - INFO - Batch: 60/141, Loss: 1.7291, Perplexity: 5.6355, Time: 0.33s
2025-04-25 23:54:30,973 - INFO - Batch: 70/141, Loss: 1.6958, Perplexity: 5.4508, Time: 0.23s
2025-04-25 23:54:31,212 - INFO - Batch: 80/141, Loss: 1.7247, Perplexity: 5.6111, Time: 0.24s
2025-04-25 23:54:31,439 - INFO - Batch: 90/141, Loss: 1.7152, Perplexity: 5.5580, Time: 0.23s
2025-04-25 23:54:31,677 - INFO - Batch: 100/141, Loss: 1.7176, Perplexity: 5.5710, Time: 0.24s
2025-04-25 23:54:31,897 - INFO - Batch: 110/141, Loss: 1.7227, Perplexity: 5.5995, Time: 0.22s
2025-04-25 23:54:32,123 - INFO - Batch: 120/141, Loss: 1.6970, Perplexity: 5.4576, Time: 0.23s
2025-04-25 23:54:32,351 - INFO - Batch: 130/141, Loss: 1.7163, Perplexity: 5.5637, Time: 0.23s
2025-04-25 23:54:32,633 - INFO - Batch: 140/141, Loss: 1.7217, Perplexity: 5.5939, Time: 0.28s
2025-04-25 23:54:33,201 - INFO - Validation  Loss: 1.8813, Perplexity: 6.5621, Time: 0.54s
2025-04-25 23:54:33,201 - INFO - Epoch 116/300 - Train Loss: 1.7130, Val Loss: 1.8813
2025-04-25 23:54:33,459 - INFO - Batch: 10/141, Loss: 1.7091, Perplexity: 5.5241, Time: 0.26s
2025-04-25 23:54:33,744 - INFO - Batch: 20/141, Loss: 1.7141, Perplexity: 5.5515, Time: 0.28s
2025-04-25 23:54:33,956 - INFO - Batch: 30/141, Loss: 1.7210, Perplexity: 5.5901, Time: 0.21s
2025-04-25 23:54:34,203 - INFO - Batch: 40/141, Loss: 1.7283, Perplexity: 5.6310, Time: 0.25s
2025-04-25 23:54:34,430 - INFO - Batch: 50/141, Loss: 1.7063, Perplexity: 5.5085, Time: 0.23s
2025-04-25 23:54:34,663 - INFO - Batch: 60/141, Loss: 1.7072, Perplexity: 5.5133, Time: 0.23s
2025-04-25 23:54:34,903 - INFO - Batch: 70/141, Loss: 1.7175, Perplexity: 5.5708, Time: 0.24s
2025-04-25 23:54:35,141 - INFO - Batch: 80/141, Loss: 1.7230, Perplexity: 5.6011, Time: 0.24s
2025-04-25 23:54:35,377 - INFO - Batch: 90/141, Loss: 1.7184, Perplexity: 5.5756, Time: 0.24s
2025-04-25 23:54:35,664 - INFO - Batch: 100/141, Loss: 1.7161, Perplexity: 5.5629, Time: 0.29s
2025-04-25 23:54:35,902 - INFO - Batch: 110/141, Loss: 1.7093, Perplexity: 5.5248, Time: 0.24s
2025-04-25 23:54:36,118 - INFO - Batch: 120/141, Loss: 1.7065, Perplexity: 5.5097, Time: 0.22s
2025-04-25 23:54:36,348 - INFO - Batch: 130/141, Loss: 1.6988, Perplexity: 5.4674, Time: 0.23s
2025-04-25 23:54:36,570 - INFO - Batch: 140/141, Loss: 1.7007, Perplexity: 5.4778, Time: 0.22s
2025-04-25 23:54:37,338 - INFO - Validation  Loss: 1.8807, Perplexity: 6.5579, Time: 0.74s
2025-04-25 23:54:37,338 - INFO - Epoch 117/300 - Train Loss: 1.7129, Val Loss: 1.8807
2025-04-25 23:54:37,627 - INFO - Batch: 10/141, Loss: 1.7163, Perplexity: 5.5638, Time: 0.29s
2025-04-25 23:54:37,925 - INFO - Batch: 20/141, Loss: 1.7041, Perplexity: 5.4964, Time: 0.30s
2025-04-25 23:54:38,187 - INFO - Batch: 30/141, Loss: 1.7080, Perplexity: 5.5180, Time: 0.26s
2025-04-25 23:54:38,422 - INFO - Batch: 40/141, Loss: 1.7134, Perplexity: 5.5477, Time: 0.23s
2025-04-25 23:54:38,647 - INFO - Batch: 50/141, Loss: 1.7007, Perplexity: 5.4777, Time: 0.23s
2025-04-25 23:54:38,922 - INFO - Batch: 60/141, Loss: 1.7135, Perplexity: 5.5481, Time: 0.27s
2025-04-25 23:54:39,227 - INFO - Batch: 70/141, Loss: 1.7000, Perplexity: 5.4740, Time: 0.31s
2025-04-25 23:54:39,460 - INFO - Batch: 80/141, Loss: 1.7091, Perplexity: 5.5238, Time: 0.23s
2025-04-25 23:54:39,690 - INFO - Batch: 90/141, Loss: 1.7160, Perplexity: 5.5621, Time: 0.23s
2025-04-25 23:54:39,922 - INFO - Batch: 100/141, Loss: 1.7105, Perplexity: 5.5320, Time: 0.23s
2025-04-25 23:54:40,152 - INFO - Batch: 110/141, Loss: 1.7147, Perplexity: 5.5552, Time: 0.23s
2025-04-25 23:54:40,410 - INFO - Batch: 120/141, Loss: 1.7073, Perplexity: 5.5140, Time: 0.26s
2025-04-25 23:54:40,634 - INFO - Batch: 130/141, Loss: 1.7083, Perplexity: 5.5197, Time: 0.22s
2025-04-25 23:54:40,919 - INFO - Batch: 140/141, Loss: 1.7246, Perplexity: 5.6105, Time: 0.28s
2025-04-25 23:54:41,472 - INFO - Validation  Loss: 1.8804, Perplexity: 6.5560, Time: 0.52s
2025-04-25 23:54:41,473 - INFO - Epoch 118/300 - Train Loss: 1.7127, Val Loss: 1.8804
2025-04-25 23:54:41,727 - INFO - Batch: 10/141, Loss: 1.7121, Perplexity: 5.5407, Time: 0.25s
2025-04-25 23:54:41,996 - INFO - Batch: 20/141, Loss: 1.7084, Perplexity: 5.5199, Time: 0.27s
2025-04-25 23:54:42,292 - INFO - Batch: 30/141, Loss: 1.7318, Perplexity: 5.6507, Time: 0.30s
2025-04-25 23:54:42,549 - INFO - Batch: 40/141, Loss: 1.7062, Perplexity: 5.5078, Time: 0.26s
2025-04-25 23:54:42,778 - INFO - Batch: 50/141, Loss: 1.7120, Perplexity: 5.5403, Time: 0.23s
2025-04-25 23:54:43,011 - INFO - Batch: 60/141, Loss: 1.7176, Perplexity: 5.5712, Time: 0.23s
2025-04-25 23:54:43,246 - INFO - Batch: 70/141, Loss: 1.7041, Perplexity: 5.4966, Time: 0.23s
2025-04-25 23:54:43,472 - INFO - Batch: 80/141, Loss: 1.6947, Perplexity: 5.4448, Time: 0.23s
2025-04-25 23:54:43,697 - INFO - Batch: 90/141, Loss: 1.7181, Perplexity: 5.5739, Time: 0.22s
2025-04-25 23:54:43,925 - INFO - Batch: 100/141, Loss: 1.7052, Perplexity: 5.5023, Time: 0.23s
2025-04-25 23:54:44,220 - INFO - Batch: 110/141, Loss: 1.7222, Perplexity: 5.5970, Time: 0.30s
2025-04-25 23:54:44,447 - INFO - Batch: 120/141, Loss: 1.7250, Perplexity: 5.6126, Time: 0.23s
2025-04-25 23:54:44,666 - INFO - Batch: 130/141, Loss: 1.7139, Perplexity: 5.5504, Time: 0.22s
2025-04-25 23:54:44,890 - INFO - Batch: 140/141, Loss: 1.7198, Perplexity: 5.5833, Time: 0.22s
2025-04-25 23:54:45,538 - INFO - Validation  Loss: 1.8814, Perplexity: 6.5626, Time: 0.62s
2025-04-25 23:54:45,538 - INFO - Epoch 119/300 - Train Loss: 1.7126, Val Loss: 1.8814
2025-04-25 23:54:45,757 - INFO - Batch: 10/141, Loss: 1.7137, Perplexity: 5.5496, Time: 0.22s
2025-04-25 23:54:45,983 - INFO - Batch: 20/141, Loss: 1.6996, Perplexity: 5.4717, Time: 0.23s
2025-04-25 23:54:46,200 - INFO - Batch: 30/141, Loss: 1.7238, Perplexity: 5.6057, Time: 0.22s
2025-04-25 23:54:46,424 - INFO - Batch: 40/141, Loss: 1.7082, Perplexity: 5.5188, Time: 0.22s
2025-04-25 23:54:46,648 - INFO - Batch: 50/141, Loss: 1.7119, Perplexity: 5.5395, Time: 0.22s
2025-04-25 23:54:46,901 - INFO - Batch: 60/141, Loss: 1.7185, Perplexity: 5.5760, Time: 0.25s
2025-04-25 23:54:47,129 - INFO - Batch: 70/141, Loss: 1.7217, Perplexity: 5.5942, Time: 0.23s
2025-04-25 23:54:47,409 - INFO - Batch: 80/141, Loss: 1.7177, Perplexity: 5.5717, Time: 0.28s
2025-04-25 23:54:47,630 - INFO - Batch: 90/141, Loss: 1.6951, Perplexity: 5.4473, Time: 0.22s
2025-04-25 23:54:47,854 - INFO - Batch: 100/141, Loss: 1.7052, Perplexity: 5.5023, Time: 0.22s
2025-04-25 23:54:48,079 - INFO - Batch: 110/141, Loss: 1.7155, Perplexity: 5.5596, Time: 0.22s
2025-04-25 23:54:48,304 - INFO - Batch: 120/141, Loss: 1.7203, Perplexity: 5.5864, Time: 0.22s
2025-04-25 23:54:48,539 - INFO - Batch: 130/141, Loss: 1.7186, Perplexity: 5.5766, Time: 0.23s
2025-04-25 23:54:48,764 - INFO - Batch: 140/141, Loss: 1.7172, Perplexity: 5.5687, Time: 0.22s
2025-04-25 23:54:49,401 - INFO - Validation  Loss: 1.8803, Perplexity: 6.5557, Time: 0.61s
2025-04-25 23:54:49,401 - INFO - Epoch 120/300 - Train Loss: 1.7124, Val Loss: 1.8803
2025-04-25 23:54:49,623 - INFO - Batch: 10/141, Loss: 1.7078, Perplexity: 5.5169, Time: 0.22s
2025-04-25 23:54:49,849 - INFO - Batch: 20/141, Loss: 1.7075, Perplexity: 5.5154, Time: 0.23s
2025-04-25 23:54:50,079 - INFO - Batch: 30/141, Loss: 1.7026, Perplexity: 5.4885, Time: 0.23s
2025-04-25 23:54:50,365 - INFO - Batch: 40/141, Loss: 1.7042, Perplexity: 5.4972, Time: 0.29s
2025-04-25 23:54:50,630 - INFO - Batch: 50/141, Loss: 1.7128, Perplexity: 5.5447, Time: 0.26s
2025-04-25 23:54:50,864 - INFO - Batch: 60/141, Loss: 1.7198, Perplexity: 5.5836, Time: 0.23s
2025-04-25 23:54:51,113 - INFO - Batch: 70/141, Loss: 1.7052, Perplexity: 5.5026, Time: 0.25s
2025-04-25 23:54:51,352 - INFO - Batch: 80/141, Loss: 1.7121, Perplexity: 5.5406, Time: 0.24s
2025-04-25 23:54:51,588 - INFO - Batch: 90/141, Loss: 1.7004, Perplexity: 5.4759, Time: 0.24s
2025-04-25 23:54:51,845 - INFO - Batch: 100/141, Loss: 1.7116, Perplexity: 5.5379, Time: 0.26s
2025-04-25 23:54:52,125 - INFO - Batch: 110/141, Loss: 1.7302, Perplexity: 5.6416, Time: 0.28s
2025-04-25 23:54:52,430 - INFO - Batch: 120/141, Loss: 1.7108, Perplexity: 5.5335, Time: 0.31s
2025-04-25 23:54:52,677 - INFO - Batch: 130/141, Loss: 1.7252, Perplexity: 5.6139, Time: 0.25s
2025-04-25 23:54:52,910 - INFO - Batch: 140/141, Loss: 1.7027, Perplexity: 5.4888, Time: 0.23s
2025-04-25 23:54:53,525 - INFO - Validation  Loss: 1.8803, Perplexity: 6.5555, Time: 0.59s
2025-04-25 23:54:53,525 - INFO - Epoch 121/300 - Train Loss: 1.7122, Val Loss: 1.8803
2025-04-25 23:54:53,806 - INFO - Batch: 10/141, Loss: 1.7050, Perplexity: 5.5015, Time: 0.28s
2025-04-25 23:54:54,064 - INFO - Batch: 20/141, Loss: 1.7074, Perplexity: 5.5146, Time: 0.26s
2025-04-25 23:54:54,314 - INFO - Batch: 30/141, Loss: 1.7133, Perplexity: 5.5471, Time: 0.25s
2025-04-25 23:54:54,590 - INFO - Batch: 40/141, Loss: 1.7187, Perplexity: 5.5775, Time: 0.28s
2025-04-25 23:54:54,828 - INFO - Batch: 50/141, Loss: 1.7160, Perplexity: 5.5624, Time: 0.24s
2025-04-25 23:54:55,070 - INFO - Batch: 60/141, Loss: 1.7087, Perplexity: 5.5217, Time: 0.24s
2025-04-25 23:54:55,337 - INFO - Batch: 70/141, Loss: 1.7064, Perplexity: 5.5090, Time: 0.27s
2025-04-25 23:54:55,627 - INFO - Batch: 80/141, Loss: 1.7154, Perplexity: 5.5588, Time: 0.29s
2025-04-25 23:54:55,948 - INFO - Batch: 90/141, Loss: 1.7019, Perplexity: 5.4845, Time: 0.32s
2025-04-25 23:54:56,210 - INFO - Batch: 100/141, Loss: 1.7064, Perplexity: 5.5090, Time: 0.26s
2025-04-25 23:54:56,518 - INFO - Batch: 110/141, Loss: 1.7115, Perplexity: 5.5372, Time: 0.31s
2025-04-25 23:54:56,801 - INFO - Batch: 120/141, Loss: 1.6962, Perplexity: 5.4532, Time: 0.28s
2025-04-25 23:54:57,112 - INFO - Batch: 130/141, Loss: 1.7063, Perplexity: 5.5086, Time: 0.31s
2025-04-25 23:54:57,423 - INFO - Batch: 140/141, Loss: 1.7223, Perplexity: 5.5972, Time: 0.31s
2025-04-25 23:54:58,333 - INFO - Validation  Loss: 1.8806, Perplexity: 6.5572, Time: 0.87s
2025-04-25 23:54:58,333 - INFO - Epoch 122/300 - Train Loss: 1.7122, Val Loss: 1.8806
2025-04-25 23:54:58,623 - INFO - Batch: 10/141, Loss: 1.7158, Perplexity: 5.5613, Time: 0.29s
2025-04-25 23:54:58,888 - INFO - Batch: 20/141, Loss: 1.7115, Perplexity: 5.5372, Time: 0.27s
2025-04-25 23:54:59,142 - INFO - Batch: 30/141, Loss: 1.6991, Perplexity: 5.4689, Time: 0.25s
2025-04-25 23:54:59,389 - INFO - Batch: 40/141, Loss: 1.7167, Perplexity: 5.5661, Time: 0.25s
2025-04-25 23:54:59,759 - INFO - Batch: 50/141, Loss: 1.7159, Perplexity: 5.5615, Time: 0.37s
2025-04-25 23:55:00,022 - INFO - Batch: 60/141, Loss: 1.7206, Perplexity: 5.5881, Time: 0.26s
2025-04-25 23:55:00,337 - INFO - Batch: 70/141, Loss: 1.7058, Perplexity: 5.5059, Time: 0.31s
2025-04-25 23:55:00,623 - INFO - Batch: 80/141, Loss: 1.6960, Perplexity: 5.4518, Time: 0.29s
2025-04-25 23:55:00,856 - INFO - Batch: 90/141, Loss: 1.7252, Perplexity: 5.6139, Time: 0.23s
2025-04-25 23:55:01,107 - INFO - Batch: 100/141, Loss: 1.7159, Perplexity: 5.5615, Time: 0.25s
2025-04-25 23:55:01,331 - INFO - Batch: 110/141, Loss: 1.7096, Perplexity: 5.5265, Time: 0.22s
2025-04-25 23:55:01,565 - INFO - Batch: 120/141, Loss: 1.7199, Perplexity: 5.5839, Time: 0.23s
2025-04-25 23:55:01,864 - INFO - Batch: 130/141, Loss: 1.7127, Perplexity: 5.5440, Time: 0.30s
2025-04-25 23:55:02,078 - INFO - Batch: 140/141, Loss: 1.7061, Perplexity: 5.5076, Time: 0.21s
2025-04-25 23:55:02,672 - INFO - Validation  Loss: 1.8793, Perplexity: 6.5487, Time: 0.57s
2025-04-25 23:55:02,672 - INFO - Epoch 123/300 - Train Loss: 1.7119, Val Loss: 1.8793
2025-04-25 23:55:02,896 - INFO - Batch: 10/141, Loss: 1.7119, Perplexity: 5.5396, Time: 0.22s
2025-04-25 23:55:03,178 - INFO - Batch: 20/141, Loss: 1.7004, Perplexity: 5.4764, Time: 0.28s
2025-04-25 23:55:03,396 - INFO - Batch: 30/141, Loss: 1.7154, Perplexity: 5.5587, Time: 0.22s
2025-04-25 23:55:03,617 - INFO - Batch: 40/141, Loss: 1.7059, Perplexity: 5.5061, Time: 0.22s
2025-04-25 23:55:03,835 - INFO - Batch: 50/141, Loss: 1.7088, Perplexity: 5.5221, Time: 0.22s
2025-04-25 23:55:04,054 - INFO - Batch: 60/141, Loss: 1.7120, Perplexity: 5.5402, Time: 0.22s
2025-04-25 23:55:04,281 - INFO - Batch: 70/141, Loss: 1.7308, Perplexity: 5.6453, Time: 0.23s
2025-04-25 23:55:04,500 - INFO - Batch: 80/141, Loss: 1.7211, Perplexity: 5.5909, Time: 0.22s
2025-04-25 23:55:04,793 - INFO - Batch: 90/141, Loss: 1.7175, Perplexity: 5.5708, Time: 0.29s
2025-04-25 23:55:05,005 - INFO - Batch: 100/141, Loss: 1.7159, Perplexity: 5.5614, Time: 0.21s
2025-04-25 23:55:05,221 - INFO - Batch: 110/141, Loss: 1.7018, Perplexity: 5.4837, Time: 0.22s
2025-04-25 23:55:05,451 - INFO - Batch: 120/141, Loss: 1.7055, Perplexity: 5.5041, Time: 0.23s
2025-04-25 23:55:05,679 - INFO - Batch: 130/141, Loss: 1.7007, Perplexity: 5.4779, Time: 0.23s
2025-04-25 23:55:05,908 - INFO - Batch: 140/141, Loss: 1.7100, Perplexity: 5.5288, Time: 0.23s
2025-04-25 23:55:06,583 - INFO - Validation  Loss: 1.8793, Perplexity: 6.5491, Time: 0.65s
2025-04-25 23:55:06,583 - INFO - Epoch 124/300 - Train Loss: 1.7118, Val Loss: 1.8793
2025-04-25 23:55:06,812 - INFO - Batch: 10/141, Loss: 1.7088, Perplexity: 5.5224, Time: 0.23s
2025-04-25 23:55:07,037 - INFO - Batch: 20/141, Loss: 1.7127, Perplexity: 5.5440, Time: 0.22s
2025-04-25 23:55:07,268 - INFO - Batch: 30/141, Loss: 1.7128, Perplexity: 5.5445, Time: 0.23s
2025-04-25 23:55:07,480 - INFO - Batch: 40/141, Loss: 1.7013, Perplexity: 5.4812, Time: 0.21s
2025-04-25 23:55:07,698 - INFO - Batch: 50/141, Loss: 1.7047, Perplexity: 5.4999, Time: 0.22s
2025-04-25 23:55:07,970 - INFO - Batch: 60/141, Loss: 1.7191, Perplexity: 5.5793, Time: 0.27s
2025-04-25 23:55:08,191 - INFO - Batch: 70/141, Loss: 1.7205, Perplexity: 5.5874, Time: 0.22s
2025-04-25 23:55:08,409 - INFO - Batch: 80/141, Loss: 1.7070, Perplexity: 5.5122, Time: 0.22s
2025-04-25 23:55:08,639 - INFO - Batch: 90/141, Loss: 1.7195, Perplexity: 5.5819, Time: 0.23s
2025-04-25 23:55:08,939 - INFO - Batch: 100/141, Loss: 1.7053, Perplexity: 5.5032, Time: 0.30s
2025-04-25 23:55:09,194 - INFO - Batch: 110/141, Loss: 1.7272, Perplexity: 5.6250, Time: 0.25s
2025-04-25 23:55:09,505 - INFO - Batch: 120/141, Loss: 1.7148, Perplexity: 5.5554, Time: 0.31s
2025-04-25 23:55:09,752 - INFO - Batch: 130/141, Loss: 1.7241, Perplexity: 5.6074, Time: 0.25s
2025-04-25 23:55:10,047 - INFO - Batch: 140/141, Loss: 1.7003, Perplexity: 5.4758, Time: 0.30s
2025-04-25 23:55:10,696 - INFO - Validation  Loss: 1.8797, Perplexity: 6.5516, Time: 0.62s
2025-04-25 23:55:10,696 - INFO - Epoch 125/300 - Train Loss: 1.7116, Val Loss: 1.8797
2025-04-25 23:55:10,952 - INFO - Batch: 10/141, Loss: 1.7257, Perplexity: 5.6165, Time: 0.26s
2025-04-25 23:55:11,169 - INFO - Batch: 20/141, Loss: 1.7112, Perplexity: 5.5356, Time: 0.22s
2025-04-25 23:55:11,444 - INFO - Batch: 30/141, Loss: 1.7027, Perplexity: 5.4888, Time: 0.27s
2025-04-25 23:55:11,683 - INFO - Batch: 40/141, Loss: 1.7217, Perplexity: 5.5938, Time: 0.24s
2025-04-25 23:55:11,989 - INFO - Batch: 50/141, Loss: 1.7238, Perplexity: 5.6056, Time: 0.31s
2025-04-25 23:55:12,287 - INFO - Batch: 60/141, Loss: 1.6987, Perplexity: 5.4670, Time: 0.30s
2025-04-25 23:55:12,583 - INFO - Batch: 70/141, Loss: 1.7041, Perplexity: 5.4964, Time: 0.30s
2025-04-25 23:55:12,808 - INFO - Batch: 80/141, Loss: 1.7301, Perplexity: 5.6411, Time: 0.22s
2025-04-25 23:55:13,068 - INFO - Batch: 90/141, Loss: 1.7105, Perplexity: 5.5317, Time: 0.26s
2025-04-25 23:55:13,398 - INFO - Batch: 100/141, Loss: 1.7120, Perplexity: 5.5402, Time: 0.33s
2025-04-25 23:55:13,636 - INFO - Batch: 110/141, Loss: 1.7115, Perplexity: 5.5370, Time: 0.24s
2025-04-25 23:55:13,863 - INFO - Batch: 120/141, Loss: 1.6952, Perplexity: 5.4477, Time: 0.23s
2025-04-25 23:55:14,078 - INFO - Batch: 130/141, Loss: 1.7069, Perplexity: 5.5121, Time: 0.22s
2025-04-25 23:55:14,320 - INFO - Batch: 140/141, Loss: 1.7138, Perplexity: 5.5500, Time: 0.24s
2025-04-25 23:55:15,000 - INFO - Validation  Loss: 1.8788, Perplexity: 6.5454, Time: 0.65s
2025-04-25 23:55:15,001 - INFO - Epoch 126/300 - Train Loss: 1.7115, Val Loss: 1.8788
2025-04-25 23:55:15,238 - INFO - Batch: 10/141, Loss: 1.7184, Perplexity: 5.5754, Time: 0.24s
2025-04-25 23:55:15,467 - INFO - Batch: 20/141, Loss: 1.7136, Perplexity: 5.5486, Time: 0.23s
2025-04-25 23:55:15,697 - INFO - Batch: 30/141, Loss: 1.7018, Perplexity: 5.4839, Time: 0.23s
2025-04-25 23:55:15,933 - INFO - Batch: 40/141, Loss: 1.7077, Perplexity: 5.5161, Time: 0.24s
2025-04-25 23:55:16,183 - INFO - Batch: 50/141, Loss: 1.7087, Perplexity: 5.5218, Time: 0.25s
2025-04-25 23:55:16,431 - INFO - Batch: 60/141, Loss: 1.7040, Perplexity: 5.4957, Time: 0.25s
2025-04-25 23:55:16,847 - INFO - Batch: 70/141, Loss: 1.7108, Perplexity: 5.5336, Time: 0.42s
2025-04-25 23:55:17,066 - INFO - Batch: 80/141, Loss: 1.7362, Perplexity: 5.6759, Time: 0.22s
2025-04-25 23:55:17,337 - INFO - Batch: 90/141, Loss: 1.6993, Perplexity: 5.4704, Time: 0.27s
2025-04-25 23:55:17,560 - INFO - Batch: 100/141, Loss: 1.7116, Perplexity: 5.5379, Time: 0.22s
2025-04-25 23:55:17,790 - INFO - Batch: 110/141, Loss: 1.7249, Perplexity: 5.6118, Time: 0.23s
2025-04-25 23:55:18,019 - INFO - Batch: 120/141, Loss: 1.7170, Perplexity: 5.5676, Time: 0.23s
2025-04-25 23:55:18,245 - INFO - Batch: 130/141, Loss: 1.7120, Perplexity: 5.5399, Time: 0.23s
2025-04-25 23:55:18,474 - INFO - Batch: 140/141, Loss: 1.7204, Perplexity: 5.5870, Time: 0.23s
2025-04-25 23:55:19,119 - INFO - Validation  Loss: 1.8789, Perplexity: 6.5461, Time: 0.62s
2025-04-25 23:55:19,119 - INFO - Epoch 127/300 - Train Loss: 1.7114, Val Loss: 1.8789
2025-04-25 23:55:19,337 - INFO - Batch: 10/141, Loss: 1.7089, Perplexity: 5.5227, Time: 0.22s
2025-04-25 23:55:19,550 - INFO - Batch: 20/141, Loss: 1.7081, Perplexity: 5.5184, Time: 0.21s
2025-04-25 23:55:19,756 - INFO - Batch: 30/141, Loss: 1.7208, Perplexity: 5.5889, Time: 0.21s
2025-04-25 23:55:20,094 - INFO - Batch: 40/141, Loss: 1.7199, Perplexity: 5.5839, Time: 0.34s
2025-04-25 23:55:20,309 - INFO - Batch: 50/141, Loss: 1.7149, Perplexity: 5.5560, Time: 0.22s
2025-04-25 23:55:20,535 - INFO - Batch: 60/141, Loss: 1.7040, Perplexity: 5.4961, Time: 0.23s
2025-04-25 23:55:20,751 - INFO - Batch: 70/141, Loss: 1.7125, Perplexity: 5.5426, Time: 0.22s
2025-04-25 23:55:20,980 - INFO - Batch: 80/141, Loss: 1.7165, Perplexity: 5.5648, Time: 0.23s
2025-04-25 23:55:21,206 - INFO - Batch: 90/141, Loss: 1.7155, Perplexity: 5.5593, Time: 0.23s
2025-04-25 23:55:21,466 - INFO - Batch: 100/141, Loss: 1.7255, Perplexity: 5.6152, Time: 0.26s
2025-04-25 23:55:21,802 - INFO - Batch: 110/141, Loss: 1.7223, Perplexity: 5.5975, Time: 0.34s
2025-04-25 23:55:22,057 - INFO - Batch: 120/141, Loss: 1.6934, Perplexity: 5.4378, Time: 0.26s
2025-04-25 23:55:22,310 - INFO - Batch: 130/141, Loss: 1.7252, Perplexity: 5.6138, Time: 0.25s
2025-04-25 23:55:22,621 - INFO - Batch: 140/141, Loss: 1.7048, Perplexity: 5.5003, Time: 0.31s
2025-04-25 23:55:23,313 - INFO - Validation  Loss: 1.8788, Perplexity: 6.5459, Time: 0.66s
2025-04-25 23:55:23,313 - INFO - Epoch 128/300 - Train Loss: 1.7112, Val Loss: 1.8788
2025-04-25 23:55:23,542 - INFO - Batch: 10/141, Loss: 1.7132, Perplexity: 5.5467, Time: 0.23s
2025-04-25 23:55:23,760 - INFO - Batch: 20/141, Loss: 1.7148, Perplexity: 5.5557, Time: 0.22s
2025-04-25 23:55:24,021 - INFO - Batch: 30/141, Loss: 1.6994, Perplexity: 5.4709, Time: 0.26s
2025-04-25 23:55:24,247 - INFO - Batch: 40/141, Loss: 1.7140, Perplexity: 5.5512, Time: 0.23s
2025-04-25 23:55:24,479 - INFO - Batch: 50/141, Loss: 1.7343, Perplexity: 5.6650, Time: 0.23s
2025-04-25 23:55:24,720 - INFO - Batch: 60/141, Loss: 1.7193, Perplexity: 5.5804, Time: 0.24s
2025-04-25 23:55:24,961 - INFO - Batch: 70/141, Loss: 1.7076, Perplexity: 5.5157, Time: 0.24s
2025-04-25 23:55:25,282 - INFO - Batch: 80/141, Loss: 1.6936, Perplexity: 5.4392, Time: 0.32s
2025-04-25 23:55:25,512 - INFO - Batch: 90/141, Loss: 1.7107, Perplexity: 5.5327, Time: 0.23s
2025-04-25 23:55:25,745 - INFO - Batch: 100/141, Loss: 1.7273, Perplexity: 5.6253, Time: 0.23s
2025-04-25 23:55:25,975 - INFO - Batch: 110/141, Loss: 1.7202, Perplexity: 5.5856, Time: 0.23s
2025-04-25 23:55:26,239 - INFO - Batch: 120/141, Loss: 1.7092, Perplexity: 5.5248, Time: 0.26s
2025-04-25 23:55:26,463 - INFO - Batch: 130/141, Loss: 1.7026, Perplexity: 5.4880, Time: 0.22s
2025-04-25 23:55:26,693 - INFO - Batch: 140/141, Loss: 1.7179, Perplexity: 5.5728, Time: 0.23s
2025-04-25 23:55:27,466 - INFO - Validation  Loss: 1.8792, Perplexity: 6.5484, Time: 0.74s
2025-04-25 23:55:27,466 - INFO - Epoch 129/300 - Train Loss: 1.7111, Val Loss: 1.8792
2025-04-25 23:55:27,764 - INFO - Batch: 10/141, Loss: 1.7144, Perplexity: 5.5531, Time: 0.30s
2025-04-25 23:55:28,135 - INFO - Batch: 20/141, Loss: 1.7214, Perplexity: 5.5925, Time: 0.37s
2025-04-25 23:55:28,374 - INFO - Batch: 30/141, Loss: 1.7106, Perplexity: 5.5324, Time: 0.24s
2025-04-25 23:55:28,714 - INFO - Batch: 40/141, Loss: 1.7037, Perplexity: 5.4940, Time: 0.34s
2025-04-25 23:55:28,947 - INFO - Batch: 50/141, Loss: 1.7169, Perplexity: 5.5671, Time: 0.23s
2025-04-25 23:55:29,206 - INFO - Batch: 60/141, Loss: 1.7168, Perplexity: 5.5665, Time: 0.26s
2025-04-25 23:55:29,421 - INFO - Batch: 70/141, Loss: 1.7073, Perplexity: 5.5141, Time: 0.22s
2025-04-25 23:55:29,642 - INFO - Batch: 80/141, Loss: 1.7232, Perplexity: 5.6026, Time: 0.22s
2025-04-25 23:55:29,862 - INFO - Batch: 90/141, Loss: 1.6976, Perplexity: 5.4611, Time: 0.22s
2025-04-25 23:55:30,074 - INFO - Batch: 100/141, Loss: 1.7107, Perplexity: 5.5330, Time: 0.21s
2025-04-25 23:55:30,294 - INFO - Batch: 110/141, Loss: 1.7053, Perplexity: 5.5032, Time: 0.22s
2025-04-25 23:55:30,587 - INFO - Batch: 120/141, Loss: 1.7177, Perplexity: 5.5715, Time: 0.29s
2025-04-25 23:55:30,811 - INFO - Batch: 130/141, Loss: 1.7169, Perplexity: 5.5672, Time: 0.22s
2025-04-25 23:55:31,034 - INFO - Batch: 140/141, Loss: 1.7147, Perplexity: 5.5549, Time: 0.22s
2025-04-25 23:55:31,629 - INFO - Validation  Loss: 1.8799, Perplexity: 6.5526, Time: 0.56s
2025-04-25 23:55:31,629 - INFO - Epoch 130/300 - Train Loss: 1.7109, Val Loss: 1.8799
2025-04-25 23:55:31,898 - INFO - Batch: 10/141, Loss: 1.7165, Perplexity: 5.5652, Time: 0.27s
2025-04-25 23:55:32,109 - INFO - Batch: 20/141, Loss: 1.7044, Perplexity: 5.4981, Time: 0.21s
2025-04-25 23:55:32,328 - INFO - Batch: 30/141, Loss: 1.7043, Perplexity: 5.4977, Time: 0.22s
2025-04-25 23:55:32,554 - INFO - Batch: 40/141, Loss: 1.7140, Perplexity: 5.5509, Time: 0.23s
2025-04-25 23:55:32,767 - INFO - Batch: 50/141, Loss: 1.7023, Perplexity: 5.4866, Time: 0.21s
2025-04-25 23:55:32,978 - INFO - Batch: 60/141, Loss: 1.7227, Perplexity: 5.5998, Time: 0.21s
2025-04-25 23:55:33,202 - INFO - Batch: 70/141, Loss: 1.7067, Perplexity: 5.5110, Time: 0.22s
2025-04-25 23:55:33,416 - INFO - Batch: 80/141, Loss: 1.7123, Perplexity: 5.5416, Time: 0.21s
2025-04-25 23:55:33,694 - INFO - Batch: 90/141, Loss: 1.7232, Perplexity: 5.6026, Time: 0.28s
2025-04-25 23:55:33,906 - INFO - Batch: 100/141, Loss: 1.7113, Perplexity: 5.5364, Time: 0.21s
2025-04-25 23:55:34,120 - INFO - Batch: 110/141, Loss: 1.7226, Perplexity: 5.5992, Time: 0.21s
2025-04-25 23:55:34,340 - INFO - Batch: 120/141, Loss: 1.7144, Perplexity: 5.5535, Time: 0.22s
2025-04-25 23:55:34,579 - INFO - Batch: 130/141, Loss: 1.7216, Perplexity: 5.5933, Time: 0.24s
2025-04-25 23:55:34,796 - INFO - Batch: 140/141, Loss: 1.7112, Perplexity: 5.5354, Time: 0.22s
2025-04-25 23:55:35,455 - INFO - Validation  Loss: 1.8774, Perplexity: 6.5363, Time: 0.63s
2025-04-25 23:55:35,455 - INFO - Epoch 131/300 - Train Loss: 1.7108, Val Loss: 1.8774
2025-04-25 23:55:35,680 - INFO - Batch: 10/141, Loss: 1.7180, Perplexity: 5.5733, Time: 0.23s
2025-04-25 23:55:35,929 - INFO - Batch: 20/141, Loss: 1.7212, Perplexity: 5.5912, Time: 0.25s
2025-04-25 23:55:36,157 - INFO - Batch: 30/141, Loss: 1.7075, Perplexity: 5.5150, Time: 0.23s
2025-04-25 23:55:36,385 - INFO - Batch: 40/141, Loss: 1.7018, Perplexity: 5.4838, Time: 0.23s
2025-04-25 23:55:36,665 - INFO - Batch: 50/141, Loss: 1.7113, Perplexity: 5.5363, Time: 0.28s
2025-04-25 23:55:36,876 - INFO - Batch: 60/141, Loss: 1.7092, Perplexity: 5.5248, Time: 0.21s
2025-04-25 23:55:37,088 - INFO - Batch: 70/141, Loss: 1.7154, Perplexity: 5.5588, Time: 0.21s
2025-04-25 23:55:37,309 - INFO - Batch: 80/141, Loss: 1.6995, Perplexity: 5.4714, Time: 0.22s
2025-04-25 23:55:37,524 - INFO - Batch: 90/141, Loss: 1.7102, Perplexity: 5.5300, Time: 0.21s
2025-04-25 23:55:37,780 - INFO - Batch: 100/141, Loss: 1.7170, Perplexity: 5.5676, Time: 0.26s
2025-04-25 23:55:38,024 - INFO - Batch: 110/141, Loss: 1.7021, Perplexity: 5.4856, Time: 0.24s
2025-04-25 23:55:38,240 - INFO - Batch: 120/141, Loss: 1.6981, Perplexity: 5.4636, Time: 0.22s
2025-04-25 23:55:38,510 - INFO - Batch: 130/141, Loss: 1.7074, Perplexity: 5.5149, Time: 0.27s
2025-04-25 23:55:38,727 - INFO - Batch: 140/141, Loss: 1.7068, Perplexity: 5.5112, Time: 0.22s
2025-04-25 23:55:39,315 - INFO - Validation  Loss: 1.8782, Perplexity: 6.5420, Time: 0.56s
2025-04-25 23:55:39,315 - INFO - Epoch 132/300 - Train Loss: 1.7108, Val Loss: 1.8782
2025-04-25 23:55:39,533 - INFO - Batch: 10/141, Loss: 1.6969, Perplexity: 5.4568, Time: 0.22s
2025-04-25 23:55:39,807 - INFO - Batch: 20/141, Loss: 1.7180, Perplexity: 5.5733, Time: 0.27s
2025-04-25 23:55:40,030 - INFO - Batch: 30/141, Loss: 1.7183, Perplexity: 5.5749, Time: 0.22s
2025-04-25 23:55:40,272 - INFO - Batch: 40/141, Loss: 1.7135, Perplexity: 5.5485, Time: 0.24s
2025-04-25 23:55:40,498 - INFO - Batch: 50/141, Loss: 1.6977, Perplexity: 5.4615, Time: 0.23s
2025-04-25 23:55:40,736 - INFO - Batch: 60/141, Loss: 1.7292, Perplexity: 5.6359, Time: 0.24s
2025-04-25 23:55:40,957 - INFO - Batch: 70/141, Loss: 1.7041, Perplexity: 5.4963, Time: 0.22s
2025-04-25 23:55:41,174 - INFO - Batch: 80/141, Loss: 1.7070, Perplexity: 5.5122, Time: 0.22s
2025-04-25 23:55:41,393 - INFO - Batch: 90/141, Loss: 1.7061, Perplexity: 5.5074, Time: 0.22s
2025-04-25 23:55:41,679 - INFO - Batch: 100/141, Loss: 1.7327, Perplexity: 5.6560, Time: 0.29s
2025-04-25 23:55:41,897 - INFO - Batch: 110/141, Loss: 1.7066, Perplexity: 5.5101, Time: 0.22s
2025-04-25 23:55:42,115 - INFO - Batch: 120/141, Loss: 1.7113, Perplexity: 5.5360, Time: 0.22s
2025-04-25 23:55:42,345 - INFO - Batch: 130/141, Loss: 1.6978, Perplexity: 5.4621, Time: 0.23s
2025-04-25 23:55:42,619 - INFO - Batch: 140/141, Loss: 1.7107, Perplexity: 5.5330, Time: 0.27s
2025-04-25 23:55:43,315 - INFO - Validation  Loss: 1.8788, Perplexity: 6.5459, Time: 0.67s
2025-04-25 23:55:43,315 - INFO - Epoch 133/300 - Train Loss: 1.7106, Val Loss: 1.8788
2025-04-25 23:55:43,540 - INFO - Batch: 10/141, Loss: 1.7047, Perplexity: 5.4996, Time: 0.22s
2025-04-25 23:55:43,767 - INFO - Batch: 20/141, Loss: 1.7106, Perplexity: 5.5320, Time: 0.23s
2025-04-25 23:55:43,988 - INFO - Batch: 30/141, Loss: 1.7145, Perplexity: 5.5541, Time: 0.22s
2025-04-25 23:55:44,198 - INFO - Batch: 40/141, Loss: 1.7177, Perplexity: 5.5716, Time: 0.21s
2025-04-25 23:55:44,412 - INFO - Batch: 50/141, Loss: 1.7071, Perplexity: 5.5127, Time: 0.21s
2025-04-25 23:55:44,691 - INFO - Batch: 60/141, Loss: 1.7081, Perplexity: 5.5183, Time: 0.28s
2025-04-25 23:55:44,909 - INFO - Batch: 70/141, Loss: 1.7158, Perplexity: 5.5612, Time: 0.22s
2025-04-25 23:55:45,129 - INFO - Batch: 80/141, Loss: 1.7114, Perplexity: 5.5365, Time: 0.22s
2025-04-25 23:55:45,345 - INFO - Batch: 90/141, Loss: 1.7064, Perplexity: 5.5093, Time: 0.22s
2025-04-25 23:55:45,563 - INFO - Batch: 100/141, Loss: 1.7202, Perplexity: 5.5854, Time: 0.22s
2025-04-25 23:55:45,775 - INFO - Batch: 110/141, Loss: 1.7037, Perplexity: 5.4943, Time: 0.21s
2025-04-25 23:55:45,992 - INFO - Batch: 120/141, Loss: 1.7083, Perplexity: 5.5198, Time: 0.22s
2025-04-25 23:55:46,208 - INFO - Batch: 130/141, Loss: 1.7135, Perplexity: 5.5486, Time: 0.22s
2025-04-25 23:55:46,481 - INFO - Batch: 140/141, Loss: 1.7154, Perplexity: 5.5588, Time: 0.27s
2025-04-25 23:55:47,119 - INFO - Validation  Loss: 1.8773, Perplexity: 6.5359, Time: 0.61s
2025-04-25 23:55:47,119 - INFO - Epoch 134/300 - Train Loss: 1.7104, Val Loss: 1.8773
2025-04-25 23:55:47,340 - INFO - Batch: 10/141, Loss: 1.7079, Perplexity: 5.5176, Time: 0.22s
2025-04-25 23:55:47,550 - INFO - Batch: 20/141, Loss: 1.7106, Perplexity: 5.5322, Time: 0.21s
2025-04-25 23:55:47,827 - INFO - Batch: 30/141, Loss: 1.6985, Perplexity: 5.4655, Time: 0.28s
2025-04-25 23:55:48,042 - INFO - Batch: 40/141, Loss: 1.7065, Perplexity: 5.5094, Time: 0.21s
2025-04-25 23:55:48,254 - INFO - Batch: 50/141, Loss: 1.6991, Perplexity: 5.4692, Time: 0.21s
2025-04-25 23:55:48,466 - INFO - Batch: 60/141, Loss: 1.7024, Perplexity: 5.4872, Time: 0.21s
2025-04-25 23:55:48,688 - INFO - Batch: 70/141, Loss: 1.7013, Perplexity: 5.4809, Time: 0.22s
2025-04-25 23:55:48,903 - INFO - Batch: 80/141, Loss: 1.7094, Perplexity: 5.5254, Time: 0.21s
2025-04-25 23:55:49,130 - INFO - Batch: 90/141, Loss: 1.7307, Perplexity: 5.6444, Time: 0.23s
2025-04-25 23:55:49,361 - INFO - Batch: 100/141, Loss: 1.7114, Perplexity: 5.5366, Time: 0.23s
2025-04-25 23:55:49,639 - INFO - Batch: 110/141, Loss: 1.7056, Perplexity: 5.5049, Time: 0.28s
2025-04-25 23:55:49,854 - INFO - Batch: 120/141, Loss: 1.7194, Perplexity: 5.5812, Time: 0.21s
2025-04-25 23:55:50,071 - INFO - Batch: 130/141, Loss: 1.7145, Perplexity: 5.5538, Time: 0.22s
2025-04-25 23:55:50,295 - INFO - Batch: 140/141, Loss: 1.6952, Perplexity: 5.4477, Time: 0.22s
2025-04-25 23:55:50,943 - INFO - Validation  Loss: 1.8789, Perplexity: 6.5460, Time: 0.62s
2025-04-25 23:55:50,943 - INFO - Epoch 135/300 - Train Loss: 1.7104, Val Loss: 1.8789
2025-04-25 23:55:51,159 - INFO - Batch: 10/141, Loss: 1.7019, Perplexity: 5.4842, Time: 0.22s
2025-04-25 23:55:51,382 - INFO - Batch: 20/141, Loss: 1.7081, Perplexity: 5.5183, Time: 0.22s
2025-04-25 23:55:51,609 - INFO - Batch: 30/141, Loss: 1.7014, Perplexity: 5.4816, Time: 0.23s
2025-04-25 23:55:51,826 - INFO - Batch: 40/141, Loss: 1.7024, Perplexity: 5.4871, Time: 0.22s
2025-04-25 23:55:52,053 - INFO - Batch: 50/141, Loss: 1.7130, Perplexity: 5.5457, Time: 0.23s
2025-04-25 23:55:52,305 - INFO - Batch: 60/141, Loss: 1.7215, Perplexity: 5.5928, Time: 0.25s
2025-04-25 23:55:52,624 - INFO - Batch: 70/141, Loss: 1.6986, Perplexity: 5.4661, Time: 0.32s
2025-04-25 23:55:52,850 - INFO - Batch: 80/141, Loss: 1.7127, Perplexity: 5.5442, Time: 0.23s
2025-04-25 23:55:53,068 - INFO - Batch: 90/141, Loss: 1.7154, Perplexity: 5.5590, Time: 0.22s
2025-04-25 23:55:53,288 - INFO - Batch: 100/141, Loss: 1.7120, Perplexity: 5.5403, Time: 0.22s
2025-04-25 23:55:53,509 - INFO - Batch: 110/141, Loss: 1.7042, Perplexity: 5.4971, Time: 0.22s
2025-04-25 23:55:53,769 - INFO - Batch: 120/141, Loss: 1.7124, Perplexity: 5.5422, Time: 0.26s
2025-04-25 23:55:53,974 - INFO - Batch: 130/141, Loss: 1.7086, Perplexity: 5.5210, Time: 0.20s
2025-04-25 23:55:54,181 - INFO - Batch: 140/141, Loss: 1.7073, Perplexity: 5.5141, Time: 0.21s
2025-04-25 23:55:54,797 - INFO - Validation  Loss: 1.8781, Perplexity: 6.5409, Time: 0.59s
2025-04-25 23:55:54,797 - INFO - Epoch 136/300 - Train Loss: 1.7102, Val Loss: 1.8781
2025-04-25 23:55:55,003 - INFO - Batch: 10/141, Loss: 1.7146, Perplexity: 5.5545, Time: 0.21s
2025-04-25 23:55:55,217 - INFO - Batch: 20/141, Loss: 1.7046, Perplexity: 5.4994, Time: 0.21s
2025-04-25 23:55:55,430 - INFO - Batch: 30/141, Loss: 1.7089, Perplexity: 5.5227, Time: 0.21s
2025-04-25 23:55:55,707 - INFO - Batch: 40/141, Loss: 1.7056, Perplexity: 5.5045, Time: 0.28s
2025-04-25 23:55:55,917 - INFO - Batch: 50/141, Loss: 1.7267, Perplexity: 5.6220, Time: 0.21s
2025-04-25 23:55:56,134 - INFO - Batch: 60/141, Loss: 1.7101, Perplexity: 5.5297, Time: 0.22s
2025-04-25 23:55:56,347 - INFO - Batch: 70/141, Loss: 1.7197, Perplexity: 5.5830, Time: 0.21s
2025-04-25 23:55:56,558 - INFO - Batch: 80/141, Loss: 1.7144, Perplexity: 5.5534, Time: 0.21s
2025-04-25 23:55:56,787 - INFO - Batch: 90/141, Loss: 1.7016, Perplexity: 5.4828, Time: 0.23s
2025-04-25 23:55:57,007 - INFO - Batch: 100/141, Loss: 1.7160, Perplexity: 5.5620, Time: 0.22s
2025-04-25 23:55:57,291 - INFO - Batch: 110/141, Loss: 1.6939, Perplexity: 5.4405, Time: 0.28s
2025-04-25 23:55:57,521 - INFO - Batch: 120/141, Loss: 1.6961, Perplexity: 5.4528, Time: 0.23s
2025-04-25 23:55:57,745 - INFO - Batch: 130/141, Loss: 1.7141, Perplexity: 5.5518, Time: 0.22s
2025-04-25 23:55:57,962 - INFO - Batch: 140/141, Loss: 1.7203, Perplexity: 5.5861, Time: 0.22s
2025-04-25 23:55:58,607 - INFO - Validation  Loss: 1.8781, Perplexity: 6.5409, Time: 0.62s
2025-04-25 23:55:58,607 - INFO - Epoch 137/300 - Train Loss: 1.7101, Val Loss: 1.8781
2025-04-25 23:55:58,820 - INFO - Batch: 10/141, Loss: 1.7139, Perplexity: 5.5504, Time: 0.21s
2025-04-25 23:55:59,031 - INFO - Batch: 20/141, Loss: 1.7018, Perplexity: 5.4836, Time: 0.21s
2025-04-25 23:55:59,249 - INFO - Batch: 30/141, Loss: 1.7120, Perplexity: 5.5398, Time: 0.22s
2025-04-25 23:55:59,465 - INFO - Batch: 40/141, Loss: 1.7140, Perplexity: 5.5510, Time: 0.22s
2025-04-25 23:55:59,683 - INFO - Batch: 50/141, Loss: 1.6916, Perplexity: 5.4280, Time: 0.22s
2025-04-25 23:55:59,895 - INFO - Batch: 60/141, Loss: 1.7219, Perplexity: 5.5952, Time: 0.21s
2025-04-25 23:56:00,119 - INFO - Batch: 70/141, Loss: 1.7073, Perplexity: 5.5139, Time: 0.22s
2025-04-25 23:56:00,402 - INFO - Batch: 80/141, Loss: 1.7138, Perplexity: 5.5500, Time: 0.28s
2025-04-25 23:56:00,656 - INFO - Batch: 90/141, Loss: 1.7073, Perplexity: 5.5139, Time: 0.25s
2025-04-25 23:56:00,879 - INFO - Batch: 100/141, Loss: 1.6952, Perplexity: 5.4477, Time: 0.22s
2025-04-25 23:56:01,110 - INFO - Batch: 110/141, Loss: 1.7144, Perplexity: 5.5536, Time: 0.23s
2025-04-25 23:56:01,328 - INFO - Batch: 120/141, Loss: 1.7232, Perplexity: 5.6024, Time: 0.22s
2025-04-25 23:56:01,558 - INFO - Batch: 130/141, Loss: 1.7001, Perplexity: 5.4746, Time: 0.23s
2025-04-25 23:56:01,823 - INFO - Batch: 140/141, Loss: 1.6989, Perplexity: 5.4680, Time: 0.26s
2025-04-25 23:56:02,500 - INFO - Validation  Loss: 1.8784, Perplexity: 6.5429, Time: 0.65s
2025-04-25 23:56:02,500 - INFO - Epoch 138/300 - Train Loss: 1.7101, Val Loss: 1.8784
2025-04-25 23:56:02,733 - INFO - Batch: 10/141, Loss: 1.7255, Perplexity: 5.6155, Time: 0.23s
2025-04-25 23:56:02,957 - INFO - Batch: 20/141, Loss: 1.7134, Perplexity: 5.5477, Time: 0.22s
2025-04-25 23:56:03,176 - INFO - Batch: 30/141, Loss: 1.7111, Perplexity: 5.5348, Time: 0.22s
2025-04-25 23:56:03,398 - INFO - Batch: 40/141, Loss: 1.7080, Perplexity: 5.5178, Time: 0.22s
2025-04-25 23:56:03,684 - INFO - Batch: 50/141, Loss: 1.7185, Perplexity: 5.5760, Time: 0.29s
2025-04-25 23:56:03,904 - INFO - Batch: 60/141, Loss: 1.7188, Perplexity: 5.5780, Time: 0.22s
2025-04-25 23:56:04,119 - INFO - Batch: 70/141, Loss: 1.7234, Perplexity: 5.6037, Time: 0.21s
2025-04-25 23:56:04,345 - INFO - Batch: 80/141, Loss: 1.7125, Perplexity: 5.5430, Time: 0.23s
2025-04-25 23:56:04,568 - INFO - Batch: 90/141, Loss: 1.7045, Perplexity: 5.4987, Time: 0.22s
2025-04-25 23:56:04,791 - INFO - Batch: 100/141, Loss: 1.7046, Perplexity: 5.4994, Time: 0.22s
2025-04-25 23:56:05,018 - INFO - Batch: 110/141, Loss: 1.7095, Perplexity: 5.5259, Time: 0.23s
2025-04-25 23:56:05,318 - INFO - Batch: 120/141, Loss: 1.7120, Perplexity: 5.5403, Time: 0.30s
2025-04-25 23:56:05,545 - INFO - Batch: 130/141, Loss: 1.6999, Perplexity: 5.4733, Time: 0.23s
2025-04-25 23:56:05,856 - INFO - Batch: 140/141, Loss: 1.7265, Perplexity: 5.6210, Time: 0.31s
2025-04-25 23:56:06,473 - INFO - Validation  Loss: 1.8776, Perplexity: 6.5376, Time: 0.59s
2025-04-25 23:56:06,473 - INFO - Epoch 139/300 - Train Loss: 1.7099, Val Loss: 1.8776
2025-04-25 23:56:06,780 - INFO - Batch: 10/141, Loss: 1.7140, Perplexity: 5.5511, Time: 0.31s
2025-04-25 23:56:07,002 - INFO - Batch: 20/141, Loss: 1.7065, Perplexity: 5.5098, Time: 0.22s
2025-04-25 23:56:07,224 - INFO - Batch: 30/141, Loss: 1.7061, Perplexity: 5.5077, Time: 0.22s
2025-04-25 23:56:07,507 - INFO - Batch: 40/141, Loss: 1.7073, Perplexity: 5.5142, Time: 0.28s
2025-04-25 23:56:07,811 - INFO - Batch: 50/141, Loss: 1.7050, Perplexity: 5.5014, Time: 0.30s
2025-04-25 23:56:08,098 - INFO - Batch: 60/141, Loss: 1.7126, Perplexity: 5.5433, Time: 0.29s
2025-04-25 23:56:08,344 - INFO - Batch: 70/141, Loss: 1.7136, Perplexity: 5.5492, Time: 0.25s
2025-04-25 23:56:08,582 - INFO - Batch: 80/141, Loss: 1.7139, Perplexity: 5.5506, Time: 0.24s
2025-04-25 23:56:08,961 - INFO - Batch: 90/141, Loss: 1.7207, Perplexity: 5.5887, Time: 0.38s
2025-04-25 23:56:09,193 - INFO - Batch: 100/141, Loss: 1.7264, Perplexity: 5.6202, Time: 0.23s
2025-04-25 23:56:09,462 - INFO - Batch: 110/141, Loss: 1.7187, Perplexity: 5.5773, Time: 0.27s
2025-04-25 23:56:09,824 - INFO - Batch: 120/141, Loss: 1.7001, Perplexity: 5.4747, Time: 0.36s
2025-04-25 23:56:10,104 - INFO - Batch: 130/141, Loss: 1.7184, Perplexity: 5.5758, Time: 0.28s
2025-04-25 23:56:10,370 - INFO - Batch: 140/141, Loss: 1.7098, Perplexity: 5.5277, Time: 0.27s
2025-04-25 23:56:11,029 - INFO - Validation  Loss: 1.8788, Perplexity: 6.5455, Time: 0.62s
2025-04-25 23:56:11,029 - INFO - Epoch 140/300 - Train Loss: 1.7099, Val Loss: 1.8788
2025-04-25 23:56:11,250 - INFO - Batch: 10/141, Loss: 1.7268, Perplexity: 5.6228, Time: 0.22s
2025-04-25 23:56:11,472 - INFO - Batch: 20/141, Loss: 1.7088, Perplexity: 5.5222, Time: 0.22s
2025-04-25 23:56:11,694 - INFO - Batch: 30/141, Loss: 1.7085, Perplexity: 5.5207, Time: 0.22s
2025-04-25 23:56:11,930 - INFO - Batch: 40/141, Loss: 1.7038, Perplexity: 5.4945, Time: 0.24s
2025-04-25 23:56:12,157 - INFO - Batch: 50/141, Loss: 1.7054, Perplexity: 5.5037, Time: 0.23s
2025-04-25 23:56:12,438 - INFO - Batch: 60/141, Loss: 1.7115, Perplexity: 5.5372, Time: 0.28s
2025-04-25 23:56:12,663 - INFO - Batch: 70/141, Loss: 1.7006, Perplexity: 5.4771, Time: 0.22s
2025-04-25 23:56:12,902 - INFO - Batch: 80/141, Loss: 1.7177, Perplexity: 5.5717, Time: 0.24s
2025-04-25 23:56:13,144 - INFO - Batch: 90/141, Loss: 1.7113, Perplexity: 5.5364, Time: 0.24s
2025-04-25 23:56:13,379 - INFO - Batch: 100/141, Loss: 1.7129, Perplexity: 5.5453, Time: 0.23s
2025-04-25 23:56:13,612 - INFO - Batch: 110/141, Loss: 1.7088, Perplexity: 5.5226, Time: 0.23s
2025-04-25 23:56:13,857 - INFO - Batch: 120/141, Loss: 1.7096, Perplexity: 5.5269, Time: 0.24s
2025-04-25 23:56:14,161 - INFO - Batch: 130/141, Loss: 1.7267, Perplexity: 5.6220, Time: 0.30s
2025-04-25 23:56:14,393 - INFO - Batch: 140/141, Loss: 1.7134, Perplexity: 5.5479, Time: 0.23s
2025-04-25 23:56:15,040 - INFO - Validation  Loss: 1.8771, Perplexity: 6.5345, Time: 0.61s
2025-04-25 23:56:15,040 - INFO - Epoch 141/300 - Train Loss: 1.7097, Val Loss: 1.8771
2025-04-25 23:56:15,286 - INFO - Batch: 10/141, Loss: 1.6978, Perplexity: 5.4619, Time: 0.25s
2025-04-25 23:56:15,595 - INFO - Batch: 20/141, Loss: 1.7046, Perplexity: 5.4992, Time: 0.31s
2025-04-25 23:56:15,866 - INFO - Batch: 30/141, Loss: 1.7106, Perplexity: 5.5323, Time: 0.27s
2025-04-25 23:56:16,117 - INFO - Batch: 40/141, Loss: 1.7068, Perplexity: 5.5115, Time: 0.25s
2025-04-25 23:56:16,358 - INFO - Batch: 50/141, Loss: 1.7080, Perplexity: 5.5177, Time: 0.24s
2025-04-25 23:56:16,597 - INFO - Batch: 60/141, Loss: 1.7041, Perplexity: 5.4963, Time: 0.24s
2025-04-25 23:56:16,859 - INFO - Batch: 70/141, Loss: 1.7125, Perplexity: 5.5426, Time: 0.26s
2025-04-25 23:56:17,100 - INFO - Batch: 80/141, Loss: 1.7052, Perplexity: 5.5025, Time: 0.24s
2025-04-25 23:56:17,341 - INFO - Batch: 90/141, Loss: 1.6964, Perplexity: 5.4542, Time: 0.24s
2025-04-25 23:56:17,639 - INFO - Batch: 100/141, Loss: 1.7173, Perplexity: 5.5697, Time: 0.30s
2025-04-25 23:56:17,895 - INFO - Batch: 110/141, Loss: 1.7041, Perplexity: 5.4965, Time: 0.26s
2025-04-25 23:56:18,135 - INFO - Batch: 120/141, Loss: 1.7144, Perplexity: 5.5534, Time: 0.24s
2025-04-25 23:56:18,410 - INFO - Batch: 130/141, Loss: 1.6969, Perplexity: 5.4572, Time: 0.28s
2025-04-25 23:56:18,659 - INFO - Batch: 140/141, Loss: 1.7116, Perplexity: 5.5376, Time: 0.25s
2025-04-25 23:56:19,343 - INFO - Validation  Loss: 1.8772, Perplexity: 6.5352, Time: 0.65s
2025-04-25 23:56:19,343 - INFO - Epoch 142/300 - Train Loss: 1.7097, Val Loss: 1.8772
2025-04-25 23:56:19,561 - INFO - Batch: 10/141, Loss: 1.7206, Perplexity: 5.5879, Time: 0.22s
2025-04-25 23:56:19,793 - INFO - Batch: 20/141, Loss: 1.7180, Perplexity: 5.5734, Time: 0.23s
2025-04-25 23:56:20,043 - INFO - Batch: 30/141, Loss: 1.7133, Perplexity: 5.5474, Time: 0.25s
2025-04-25 23:56:20,286 - INFO - Batch: 40/141, Loss: 1.7037, Perplexity: 5.4942, Time: 0.24s
2025-04-25 23:56:20,508 - INFO - Batch: 50/141, Loss: 1.7119, Perplexity: 5.5394, Time: 0.22s
2025-04-25 23:56:20,793 - INFO - Batch: 60/141, Loss: 1.6980, Perplexity: 5.4629, Time: 0.28s
2025-04-25 23:56:21,007 - INFO - Batch: 70/141, Loss: 1.7159, Perplexity: 5.5618, Time: 0.21s
2025-04-25 23:56:21,232 - INFO - Batch: 80/141, Loss: 1.7107, Perplexity: 5.5329, Time: 0.22s
2025-04-25 23:56:21,460 - INFO - Batch: 90/141, Loss: 1.7041, Perplexity: 5.4966, Time: 0.23s
2025-04-25 23:56:21,707 - INFO - Batch: 100/141, Loss: 1.7220, Perplexity: 5.5958, Time: 0.25s
2025-04-25 23:56:21,952 - INFO - Batch: 110/141, Loss: 1.7140, Perplexity: 5.5508, Time: 0.24s
2025-04-25 23:56:22,182 - INFO - Batch: 120/141, Loss: 1.7046, Perplexity: 5.4993, Time: 0.23s
2025-04-25 23:56:22,417 - INFO - Batch: 130/141, Loss: 1.6903, Perplexity: 5.4211, Time: 0.23s
2025-04-25 23:56:22,739 - INFO - Batch: 140/141, Loss: 1.7125, Perplexity: 5.5430, Time: 0.32s
2025-04-25 23:56:23,340 - INFO - Validation  Loss: 1.8768, Perplexity: 6.5323, Time: 0.57s
2025-04-25 23:56:23,341 - INFO - Epoch 143/300 - Train Loss: 1.7095, Val Loss: 1.8768
2025-04-25 23:56:23,565 - INFO - Batch: 10/141, Loss: 1.7238, Perplexity: 5.6060, Time: 0.22s
2025-04-25 23:56:23,799 - INFO - Batch: 20/141, Loss: 1.7007, Perplexity: 5.4780, Time: 0.23s
2025-04-25 23:56:24,095 - INFO - Batch: 30/141, Loss: 1.7074, Perplexity: 5.5148, Time: 0.30s
2025-04-25 23:56:24,323 - INFO - Batch: 40/141, Loss: 1.6993, Perplexity: 5.4699, Time: 0.23s
2025-04-25 23:56:24,558 - INFO - Batch: 50/141, Loss: 1.7068, Perplexity: 5.5111, Time: 0.23s
2025-04-25 23:56:24,789 - INFO - Batch: 60/141, Loss: 1.7110, Perplexity: 5.5346, Time: 0.23s
2025-04-25 23:56:25,019 - INFO - Batch: 70/141, Loss: 1.7103, Perplexity: 5.5304, Time: 0.23s
2025-04-25 23:56:25,252 - INFO - Batch: 80/141, Loss: 1.7161, Perplexity: 5.5626, Time: 0.23s
2025-04-25 23:56:25,482 - INFO - Batch: 90/141, Loss: 1.7150, Perplexity: 5.5566, Time: 0.23s
2025-04-25 23:56:25,717 - INFO - Batch: 100/141, Loss: 1.6969, Perplexity: 5.4568, Time: 0.23s
2025-04-25 23:56:26,106 - INFO - Batch: 110/141, Loss: 1.6949, Perplexity: 5.4462, Time: 0.39s
2025-04-25 23:56:26,362 - INFO - Batch: 120/141, Loss: 1.7072, Perplexity: 5.5134, Time: 0.26s
2025-04-25 23:56:26,631 - INFO - Batch: 130/141, Loss: 1.7050, Perplexity: 5.5012, Time: 0.27s
2025-04-25 23:56:26,870 - INFO - Batch: 140/141, Loss: 1.7018, Perplexity: 5.4840, Time: 0.24s
2025-04-25 23:56:27,528 - INFO - Validation  Loss: 1.8774, Perplexity: 6.5362, Time: 0.62s
2025-04-25 23:56:27,528 - INFO - Epoch 144/300 - Train Loss: 1.7094, Val Loss: 1.8774
2025-04-25 23:56:27,745 - INFO - Batch: 10/141, Loss: 1.7356, Perplexity: 5.6725, Time: 0.22s
2025-04-25 23:56:27,964 - INFO - Batch: 20/141, Loss: 1.7151, Perplexity: 5.5572, Time: 0.22s
2025-04-25 23:56:28,187 - INFO - Batch: 30/141, Loss: 1.7268, Perplexity: 5.6226, Time: 0.22s
2025-04-25 23:56:28,411 - INFO - Batch: 40/141, Loss: 1.7197, Perplexity: 5.5828, Time: 0.22s
2025-04-25 23:56:28,632 - INFO - Batch: 50/141, Loss: 1.7029, Perplexity: 5.4899, Time: 0.22s
2025-04-25 23:56:28,856 - INFO - Batch: 60/141, Loss: 1.7243, Perplexity: 5.6087, Time: 0.22s
2025-04-25 23:56:29,137 - INFO - Batch: 70/141, Loss: 1.7099, Perplexity: 5.5283, Time: 0.28s
2025-04-25 23:56:29,358 - INFO - Batch: 80/141, Loss: 1.7082, Perplexity: 5.5188, Time: 0.22s
2025-04-25 23:56:29,579 - INFO - Batch: 90/141, Loss: 1.7083, Perplexity: 5.5193, Time: 0.22s
2025-04-25 23:56:29,801 - INFO - Batch: 100/141, Loss: 1.7185, Perplexity: 5.5760, Time: 0.22s
2025-04-25 23:56:30,027 - INFO - Batch: 110/141, Loss: 1.7120, Perplexity: 5.5403, Time: 0.23s
2025-04-25 23:56:30,254 - INFO - Batch: 120/141, Loss: 1.7100, Perplexity: 5.5289, Time: 0.23s
2025-04-25 23:56:30,476 - INFO - Batch: 130/141, Loss: 1.6963, Perplexity: 5.4536, Time: 0.22s
2025-04-25 23:56:30,699 - INFO - Batch: 140/141, Loss: 1.7029, Perplexity: 5.4899, Time: 0.22s
2025-04-25 23:56:31,339 - INFO - Validation  Loss: 1.8779, Perplexity: 6.5400, Time: 0.61s
2025-04-25 23:56:31,339 - INFO - Epoch 145/300 - Train Loss: 1.7094, Val Loss: 1.8779
2025-04-25 23:56:31,568 - INFO - Batch: 10/141, Loss: 1.7016, Perplexity: 5.4828, Time: 0.23s
2025-04-25 23:56:31,790 - INFO - Batch: 20/141, Loss: 1.7061, Perplexity: 5.5073, Time: 0.22s
2025-04-25 23:56:32,016 - INFO - Batch: 30/141, Loss: 1.7041, Perplexity: 5.4962, Time: 0.23s
2025-04-25 23:56:32,300 - INFO - Batch: 40/141, Loss: 1.7274, Perplexity: 5.6261, Time: 0.28s
2025-04-25 23:56:32,534 - INFO - Batch: 50/141, Loss: 1.6991, Perplexity: 5.4693, Time: 0.23s
2025-04-25 23:56:32,753 - INFO - Batch: 60/141, Loss: 1.7073, Perplexity: 5.5141, Time: 0.22s
2025-04-25 23:56:32,970 - INFO - Batch: 70/141, Loss: 1.7188, Perplexity: 5.5781, Time: 0.22s
2025-04-25 23:56:33,197 - INFO - Batch: 80/141, Loss: 1.7052, Perplexity: 5.5024, Time: 0.23s
2025-04-25 23:56:33,428 - INFO - Batch: 90/141, Loss: 1.7044, Perplexity: 5.4980, Time: 0.23s
2025-04-25 23:56:33,648 - INFO - Batch: 100/141, Loss: 1.6945, Perplexity: 5.4441, Time: 0.22s
2025-04-25 23:56:33,866 - INFO - Batch: 110/141, Loss: 1.7189, Perplexity: 5.5782, Time: 0.22s
2025-04-25 23:56:34,146 - INFO - Batch: 120/141, Loss: 1.7187, Perplexity: 5.5772, Time: 0.28s
2025-04-25 23:56:34,362 - INFO - Batch: 130/141, Loss: 1.7114, Perplexity: 5.5368, Time: 0.22s
2025-04-25 23:56:34,588 - INFO - Batch: 140/141, Loss: 1.7099, Perplexity: 5.5286, Time: 0.23s
2025-04-25 23:56:35,205 - INFO - Validation  Loss: 1.8772, Perplexity: 6.5353, Time: 0.59s
2025-04-25 23:56:35,205 - INFO - Epoch 146/300 - Train Loss: 1.7093, Val Loss: 1.8772
2025-04-25 23:56:35,482 - INFO - Batch: 10/141, Loss: 1.7027, Perplexity: 5.4886, Time: 0.28s
2025-04-25 23:56:35,737 - INFO - Batch: 20/141, Loss: 1.7093, Perplexity: 5.5253, Time: 0.25s
2025-04-25 23:56:35,964 - INFO - Batch: 30/141, Loss: 1.7118, Perplexity: 5.5388, Time: 0.23s
2025-04-25 23:56:36,193 - INFO - Batch: 40/141, Loss: 1.7033, Perplexity: 5.4920, Time: 0.23s
2025-04-25 23:56:36,420 - INFO - Batch: 50/141, Loss: 1.7060, Perplexity: 5.5070, Time: 0.23s
2025-04-25 23:56:36,650 - INFO - Batch: 60/141, Loss: 1.7012, Perplexity: 5.4806, Time: 0.23s
2025-04-25 23:56:36,875 - INFO - Batch: 70/141, Loss: 1.7148, Perplexity: 5.5556, Time: 0.22s
2025-04-25 23:56:37,171 - INFO - Batch: 80/141, Loss: 1.7000, Perplexity: 5.4741, Time: 0.30s
2025-04-25 23:56:37,401 - INFO - Batch: 90/141, Loss: 1.7137, Perplexity: 5.5495, Time: 0.23s
2025-04-25 23:56:37,632 - INFO - Batch: 100/141, Loss: 1.7238, Perplexity: 5.6061, Time: 0.23s
2025-04-25 23:56:37,852 - INFO - Batch: 110/141, Loss: 1.7076, Perplexity: 5.5157, Time: 0.22s
2025-04-25 23:56:38,091 - INFO - Batch: 120/141, Loss: 1.7203, Perplexity: 5.5860, Time: 0.24s
2025-04-25 23:56:38,317 - INFO - Batch: 130/141, Loss: 1.6995, Perplexity: 5.4713, Time: 0.23s
2025-04-25 23:56:38,554 - INFO - Batch: 140/141, Loss: 1.7003, Perplexity: 5.4753, Time: 0.24s
2025-04-25 23:56:39,235 - INFO - Validation  Loss: 1.8777, Perplexity: 6.5385, Time: 0.65s
2025-04-25 23:56:39,235 - INFO - Epoch 147/300 - Train Loss: 1.7092, Val Loss: 1.8777
2025-04-25 23:56:39,461 - INFO - Batch: 10/141, Loss: 1.7111, Perplexity: 5.5349, Time: 0.23s
2025-04-25 23:56:39,691 - INFO - Batch: 20/141, Loss: 1.7023, Perplexity: 5.4868, Time: 0.23s
2025-04-25 23:56:39,923 - INFO - Batch: 30/141, Loss: 1.7040, Perplexity: 5.4960, Time: 0.23s
2025-04-25 23:56:40,159 - INFO - Batch: 40/141, Loss: 1.7117, Perplexity: 5.5385, Time: 0.24s
2025-04-25 23:56:40,446 - INFO - Batch: 50/141, Loss: 1.7085, Perplexity: 5.5205, Time: 0.29s
2025-04-25 23:56:40,679 - INFO - Batch: 60/141, Loss: 1.7174, Perplexity: 5.5700, Time: 0.23s
2025-04-25 23:56:40,912 - INFO - Batch: 70/141, Loss: 1.7216, Perplexity: 5.5932, Time: 0.23s
2025-04-25 23:56:41,154 - INFO - Batch: 80/141, Loss: 1.7202, Perplexity: 5.5859, Time: 0.24s
2025-04-25 23:56:41,397 - INFO - Batch: 90/141, Loss: 1.7050, Perplexity: 5.5013, Time: 0.24s
2025-04-25 23:56:41,626 - INFO - Batch: 100/141, Loss: 1.6979, Perplexity: 5.4626, Time: 0.23s
2025-04-25 23:56:41,899 - INFO - Batch: 110/141, Loss: 1.7149, Perplexity: 5.5560, Time: 0.27s
2025-04-25 23:56:42,144 - INFO - Batch: 120/141, Loss: 1.7240, Perplexity: 5.6068, Time: 0.24s
2025-04-25 23:56:42,461 - INFO - Batch: 130/141, Loss: 1.6983, Perplexity: 5.4647, Time: 0.32s
2025-04-25 23:56:42,694 - INFO - Batch: 140/141, Loss: 1.7089, Perplexity: 5.5228, Time: 0.23s
2025-04-25 23:56:43,412 - INFO - Validation  Loss: 1.8773, Perplexity: 6.5356, Time: 0.68s
2025-04-25 23:56:43,412 - INFO - Epoch 148/300 - Train Loss: 1.7091, Val Loss: 1.8773
2025-04-25 23:56:43,709 - INFO - Batch: 10/141, Loss: 1.7013, Perplexity: 5.4808, Time: 0.30s
2025-04-25 23:56:43,977 - INFO - Batch: 20/141, Loss: 1.6925, Perplexity: 5.4330, Time: 0.27s
2025-04-25 23:56:44,212 - INFO - Batch: 30/141, Loss: 1.7067, Perplexity: 5.5105, Time: 0.24s
2025-04-25 23:56:44,541 - INFO - Batch: 40/141, Loss: 1.7222, Perplexity: 5.5967, Time: 0.33s
2025-04-25 23:56:44,771 - INFO - Batch: 50/141, Loss: 1.7133, Perplexity: 5.5470, Time: 0.23s
2025-04-25 23:56:45,035 - INFO - Batch: 60/141, Loss: 1.7195, Perplexity: 5.5819, Time: 0.26s
2025-04-25 23:56:45,311 - INFO - Batch: 70/141, Loss: 1.6980, Perplexity: 5.4631, Time: 0.28s
2025-04-25 23:56:45,550 - INFO - Batch: 80/141, Loss: 1.7159, Perplexity: 5.5615, Time: 0.24s
2025-04-25 23:56:45,892 - INFO - Batch: 90/141, Loss: 1.7088, Perplexity: 5.5223, Time: 0.34s
2025-04-25 23:56:46,118 - INFO - Batch: 100/141, Loss: 1.7030, Perplexity: 5.4906, Time: 0.23s
2025-04-25 23:56:46,392 - INFO - Batch: 110/141, Loss: 1.7105, Perplexity: 5.5315, Time: 0.27s
2025-04-25 23:56:46,626 - INFO - Batch: 120/141, Loss: 1.7116, Perplexity: 5.5377, Time: 0.23s
2025-04-25 23:56:46,920 - INFO - Batch: 130/141, Loss: 1.7124, Perplexity: 5.5424, Time: 0.29s
2025-04-25 23:56:47,144 - INFO - Batch: 140/141, Loss: 1.6966, Perplexity: 5.4556, Time: 0.22s
2025-04-25 23:56:47,906 - INFO - Validation  Loss: 1.8764, Perplexity: 6.5296, Time: 0.73s
2025-04-25 23:56:47,906 - INFO - Epoch 149/300 - Train Loss: 1.7090, Val Loss: 1.8764
2025-04-25 23:56:48,134 - INFO - Batch: 10/141, Loss: 1.7164, Perplexity: 5.5646, Time: 0.23s
2025-04-25 23:56:48,414 - INFO - Batch: 20/141, Loss: 1.6937, Perplexity: 5.4397, Time: 0.28s
2025-04-25 23:56:48,645 - INFO - Batch: 30/141, Loss: 1.7029, Perplexity: 5.4897, Time: 0.23s
2025-04-25 23:56:48,925 - INFO - Batch: 40/141, Loss: 1.7036, Perplexity: 5.4937, Time: 0.28s
2025-04-25 23:56:49,154 - INFO - Batch: 50/141, Loss: 1.7073, Perplexity: 5.5142, Time: 0.23s
2025-04-25 23:56:49,481 - INFO - Batch: 60/141, Loss: 1.7053, Perplexity: 5.5028, Time: 0.33s
2025-04-25 23:56:49,708 - INFO - Batch: 70/141, Loss: 1.7117, Perplexity: 5.5384, Time: 0.23s
2025-04-25 23:56:49,930 - INFO - Batch: 80/141, Loss: 1.7043, Perplexity: 5.4977, Time: 0.22s
2025-04-25 23:56:50,158 - INFO - Batch: 90/141, Loss: 1.7097, Perplexity: 5.5275, Time: 0.23s
2025-04-25 23:56:50,384 - INFO - Batch: 100/141, Loss: 1.7215, Perplexity: 5.5927, Time: 0.23s
2025-04-25 23:56:50,606 - INFO - Batch: 110/141, Loss: 1.7020, Perplexity: 5.4849, Time: 0.22s
2025-04-25 23:56:50,827 - INFO - Batch: 120/141, Loss: 1.7056, Perplexity: 5.5047, Time: 0.22s
2025-04-25 23:56:51,108 - INFO - Batch: 130/141, Loss: 1.7104, Perplexity: 5.5313, Time: 0.28s
2025-04-25 23:56:51,355 - INFO - Batch: 140/141, Loss: 1.7178, Perplexity: 5.5724, Time: 0.25s
2025-04-25 23:56:52,010 - INFO - Validation  Loss: 1.8767, Perplexity: 6.5320, Time: 0.60s
2025-04-25 23:56:52,011 - INFO - Epoch 150/300 - Train Loss: 1.7089, Val Loss: 1.8767
2025-04-25 23:56:52,231 - INFO - Batch: 10/141, Loss: 1.6905, Perplexity: 5.4221, Time: 0.22s
2025-04-25 23:56:52,521 - INFO - Batch: 20/141, Loss: 1.7092, Perplexity: 5.5244, Time: 0.29s
2025-04-25 23:56:52,742 - INFO - Batch: 30/141, Loss: 1.7002, Perplexity: 5.4748, Time: 0.22s
2025-04-25 23:56:52,964 - INFO - Batch: 40/141, Loss: 1.7261, Perplexity: 5.6187, Time: 0.22s
2025-04-25 23:56:53,184 - INFO - Batch: 50/141, Loss: 1.7068, Perplexity: 5.5115, Time: 0.22s
2025-04-25 23:56:53,403 - INFO - Batch: 60/141, Loss: 1.7211, Perplexity: 5.5907, Time: 0.22s
2025-04-25 23:56:53,628 - INFO - Batch: 70/141, Loss: 1.6968, Perplexity: 5.4567, Time: 0.22s
2025-04-25 23:56:53,847 - INFO - Batch: 80/141, Loss: 1.7183, Perplexity: 5.5751, Time: 0.22s
2025-04-25 23:56:54,069 - INFO - Batch: 90/141, Loss: 1.7098, Perplexity: 5.5276, Time: 0.22s
2025-04-25 23:56:54,358 - INFO - Batch: 100/141, Loss: 1.6958, Perplexity: 5.4512, Time: 0.29s
2025-04-25 23:56:54,575 - INFO - Batch: 110/141, Loss: 1.6962, Perplexity: 5.4531, Time: 0.22s
2025-04-25 23:56:54,795 - INFO - Batch: 120/141, Loss: 1.7072, Perplexity: 5.5135, Time: 0.22s
2025-04-25 23:56:55,013 - INFO - Batch: 130/141, Loss: 1.7125, Perplexity: 5.5430, Time: 0.22s
2025-04-25 23:56:55,240 - INFO - Batch: 140/141, Loss: 1.7154, Perplexity: 5.5587, Time: 0.23s
2025-04-25 23:56:55,895 - INFO - Validation  Loss: 1.8771, Perplexity: 6.5348, Time: 0.62s
2025-04-25 23:56:55,896 - INFO - Epoch 151/300 - Train Loss: 1.7089, Val Loss: 1.8771
2025-04-25 23:56:56,117 - INFO - Batch: 10/141, Loss: 1.7166, Perplexity: 5.5655, Time: 0.22s
2025-04-25 23:56:56,349 - INFO - Batch: 20/141, Loss: 1.7210, Perplexity: 5.5900, Time: 0.23s
2025-04-25 23:56:56,656 - INFO - Batch: 30/141, Loss: 1.7000, Perplexity: 5.4741, Time: 0.31s
2025-04-25 23:56:56,941 - INFO - Batch: 40/141, Loss: 1.7094, Perplexity: 5.5255, Time: 0.29s
2025-04-25 23:56:57,233 - INFO - Batch: 50/141, Loss: 1.7149, Perplexity: 5.5560, Time: 0.29s
2025-04-25 23:56:57,476 - INFO - Batch: 60/141, Loss: 1.7003, Perplexity: 5.4755, Time: 0.24s
2025-04-25 23:56:57,795 - INFO - Batch: 70/141, Loss: 1.7150, Perplexity: 5.5569, Time: 0.32s
2025-04-25 23:56:58,066 - INFO - Batch: 80/141, Loss: 1.6982, Perplexity: 5.4640, Time: 0.27s
2025-04-25 23:56:58,299 - INFO - Batch: 90/141, Loss: 1.7153, Perplexity: 5.5581, Time: 0.23s
2025-04-25 23:56:58,528 - INFO - Batch: 100/141, Loss: 1.7135, Perplexity: 5.5485, Time: 0.23s
2025-04-25 23:56:58,760 - INFO - Batch: 110/141, Loss: 1.7166, Perplexity: 5.5654, Time: 0.23s
2025-04-25 23:56:58,989 - INFO - Batch: 120/141, Loss: 1.7076, Perplexity: 5.5159, Time: 0.23s
2025-04-25 23:56:59,223 - INFO - Batch: 130/141, Loss: 1.7275, Perplexity: 5.6266, Time: 0.23s
2025-04-25 23:56:59,526 - INFO - Batch: 140/141, Loss: 1.7079, Perplexity: 5.5176, Time: 0.30s
2025-04-25 23:57:00,122 - INFO - Validation  Loss: 1.8770, Perplexity: 6.5341, Time: 0.56s
2025-04-25 23:57:00,123 - INFO - Epoch 152/300 - Train Loss: 1.7088, Val Loss: 1.8770
2025-04-25 23:57:00,363 - INFO - Batch: 10/141, Loss: 1.7086, Perplexity: 5.5215, Time: 0.24s
2025-04-25 23:57:00,704 - INFO - Batch: 20/141, Loss: 1.6943, Perplexity: 5.4428, Time: 0.34s
2025-04-25 23:57:01,082 - INFO - Batch: 30/141, Loss: 1.7081, Perplexity: 5.5183, Time: 0.38s
2025-04-25 23:57:01,363 - INFO - Batch: 40/141, Loss: 1.7002, Perplexity: 5.4749, Time: 0.28s
2025-04-25 23:57:01,613 - INFO - Batch: 50/141, Loss: 1.7185, Perplexity: 5.5759, Time: 0.25s
2025-04-25 23:57:01,847 - INFO - Batch: 60/141, Loss: 1.7070, Perplexity: 5.5124, Time: 0.23s
2025-04-25 23:57:02,080 - INFO - Batch: 70/141, Loss: 1.7000, Perplexity: 5.4737, Time: 0.23s
2025-04-25 23:57:02,308 - INFO - Batch: 80/141, Loss: 1.7209, Perplexity: 5.5898, Time: 0.23s
2025-04-25 23:57:02,534 - INFO - Batch: 90/141, Loss: 1.7066, Perplexity: 5.5102, Time: 0.23s
2025-04-25 23:57:02,759 - INFO - Batch: 100/141, Loss: 1.7141, Perplexity: 5.5518, Time: 0.22s
2025-04-25 23:57:03,042 - INFO - Batch: 110/141, Loss: 1.7029, Perplexity: 5.4899, Time: 0.28s
2025-04-25 23:57:03,275 - INFO - Batch: 120/141, Loss: 1.6968, Perplexity: 5.4562, Time: 0.23s
2025-04-25 23:57:03,502 - INFO - Batch: 130/141, Loss: 1.7041, Perplexity: 5.4962, Time: 0.23s
2025-04-25 23:57:03,730 - INFO - Batch: 140/141, Loss: 1.7005, Perplexity: 5.4768, Time: 0.23s
2025-04-25 23:57:04,402 - INFO - Validation  Loss: 1.8776, Perplexity: 6.5378, Time: 0.64s
2025-04-25 23:57:04,402 - INFO - Epoch 153/300 - Train Loss: 1.7087, Val Loss: 1.8776
2025-04-25 23:57:04,640 - INFO - Batch: 10/141, Loss: 1.7121, Perplexity: 5.5403, Time: 0.24s
2025-04-25 23:57:04,869 - INFO - Batch: 20/141, Loss: 1.6979, Perplexity: 5.4623, Time: 0.23s
2025-04-25 23:57:05,098 - INFO - Batch: 30/141, Loss: 1.7012, Perplexity: 5.4807, Time: 0.23s
2025-04-25 23:57:05,332 - INFO - Batch: 40/141, Loss: 1.7189, Perplexity: 5.5783, Time: 0.23s
2025-04-25 23:57:05,568 - INFO - Batch: 50/141, Loss: 1.7121, Perplexity: 5.5408, Time: 0.24s
2025-04-25 23:57:05,822 - INFO - Batch: 60/141, Loss: 1.7190, Perplexity: 5.5792, Time: 0.25s
2025-04-25 23:57:06,051 - INFO - Batch: 70/141, Loss: 1.7074, Perplexity: 5.5145, Time: 0.23s
2025-04-25 23:57:06,329 - INFO - Batch: 80/141, Loss: 1.7132, Perplexity: 5.5464, Time: 0.28s
2025-04-25 23:57:06,557 - INFO - Batch: 90/141, Loss: 1.6943, Perplexity: 5.4428, Time: 0.23s
2025-04-25 23:57:06,779 - INFO - Batch: 100/141, Loss: 1.7159, Perplexity: 5.5618, Time: 0.22s
2025-04-25 23:57:06,998 - INFO - Batch: 110/141, Loss: 1.7160, Perplexity: 5.5622, Time: 0.22s
2025-04-25 23:57:07,217 - INFO - Batch: 120/141, Loss: 1.6998, Perplexity: 5.4727, Time: 0.22s
2025-04-25 23:57:07,436 - INFO - Batch: 130/141, Loss: 1.7048, Perplexity: 5.5000, Time: 0.22s
2025-04-25 23:57:07,660 - INFO - Batch: 140/141, Loss: 1.7088, Perplexity: 5.5226, Time: 0.22s
2025-04-25 23:57:08,315 - INFO - Validation  Loss: 1.8771, Perplexity: 6.5348, Time: 0.62s
2025-04-25 23:57:08,315 - INFO - Epoch 154/300 - Train Loss: 1.7086, Val Loss: 1.8771
2025-04-25 23:57:08,544 - INFO - Batch: 10/141, Loss: 1.7045, Perplexity: 5.4987, Time: 0.23s
2025-04-25 23:57:08,768 - INFO - Batch: 20/141, Loss: 1.7070, Perplexity: 5.5122, Time: 0.22s
2025-04-25 23:57:08,987 - INFO - Batch: 30/141, Loss: 1.7110, Perplexity: 5.5343, Time: 0.22s
2025-04-25 23:57:09,265 - INFO - Batch: 40/141, Loss: 1.7044, Perplexity: 5.4979, Time: 0.28s
2025-04-25 23:57:09,484 - INFO - Batch: 50/141, Loss: 1.7092, Perplexity: 5.5244, Time: 0.22s
2025-04-25 23:57:09,704 - INFO - Batch: 60/141, Loss: 1.7174, Perplexity: 5.5698, Time: 0.22s
2025-04-25 23:57:09,918 - INFO - Batch: 70/141, Loss: 1.6991, Perplexity: 5.4691, Time: 0.21s
2025-04-25 23:57:10,136 - INFO - Batch: 80/141, Loss: 1.7034, Perplexity: 5.4926, Time: 0.22s
2025-04-25 23:57:10,362 - INFO - Batch: 90/141, Loss: 1.7129, Perplexity: 5.5453, Time: 0.23s
2025-04-25 23:57:10,583 - INFO - Batch: 100/141, Loss: 1.6994, Perplexity: 5.4708, Time: 0.22s
2025-04-25 23:57:10,830 - INFO - Batch: 110/141, Loss: 1.7001, Perplexity: 5.4746, Time: 0.25s
2025-04-25 23:57:11,106 - INFO - Batch: 120/141, Loss: 1.7126, Perplexity: 5.5434, Time: 0.28s
2025-04-25 23:57:11,334 - INFO - Batch: 130/141, Loss: 1.7075, Perplexity: 5.5150, Time: 0.23s
2025-04-25 23:57:11,556 - INFO - Batch: 140/141, Loss: 1.6981, Perplexity: 5.4634, Time: 0.22s
2025-04-25 23:57:12,175 - INFO - Validation  Loss: 1.8773, Perplexity: 6.5359, Time: 0.59s
2025-04-25 23:57:12,175 - INFO - Epoch 155/300 - Train Loss: 1.7085, Val Loss: 1.8773
2025-04-25 23:57:12,475 - INFO - Batch: 10/141, Loss: 1.6907, Perplexity: 5.4232, Time: 0.30s
2025-04-25 23:57:12,697 - INFO - Batch: 20/141, Loss: 1.7083, Perplexity: 5.5193, Time: 0.22s
2025-04-25 23:57:12,932 - INFO - Batch: 30/141, Loss: 1.7172, Perplexity: 5.5688, Time: 0.23s
2025-04-25 23:57:13,145 - INFO - Batch: 40/141, Loss: 1.7097, Perplexity: 5.5270, Time: 0.21s
2025-04-25 23:57:13,361 - INFO - Batch: 50/141, Loss: 1.7277, Perplexity: 5.6279, Time: 0.22s
2025-04-25 23:57:13,576 - INFO - Batch: 60/141, Loss: 1.7099, Perplexity: 5.5282, Time: 0.21s
2025-04-25 23:57:13,798 - INFO - Batch: 70/141, Loss: 1.7066, Perplexity: 5.5103, Time: 0.22s
2025-04-25 23:57:14,080 - INFO - Batch: 80/141, Loss: 1.7063, Perplexity: 5.5087, Time: 0.28s
2025-04-25 23:57:14,297 - INFO - Batch: 90/141, Loss: 1.7023, Perplexity: 5.4868, Time: 0.22s
2025-04-25 23:57:14,516 - INFO - Batch: 100/141, Loss: 1.7044, Perplexity: 5.4979, Time: 0.22s
2025-04-25 23:57:14,736 - INFO - Batch: 110/141, Loss: 1.7004, Perplexity: 5.4762, Time: 0.22s
2025-04-25 23:57:14,954 - INFO - Batch: 120/141, Loss: 1.6964, Perplexity: 5.4542, Time: 0.22s
2025-04-25 23:57:15,179 - INFO - Batch: 130/141, Loss: 1.7053, Perplexity: 5.5030, Time: 0.22s
2025-04-25 23:57:15,396 - INFO - Batch: 140/141, Loss: 1.6985, Perplexity: 5.4659, Time: 0.22s
2025-04-25 23:57:16,038 - INFO - Validation  Loss: 1.8766, Perplexity: 6.5310, Time: 0.62s
2025-04-25 23:57:16,038 - INFO - Epoch 156/300 - Train Loss: 1.7084, Val Loss: 1.8766
2025-04-25 23:57:16,259 - INFO - Batch: 10/141, Loss: 1.7020, Perplexity: 5.4847, Time: 0.22s
2025-04-25 23:57:16,478 - INFO - Batch: 20/141, Loss: 1.7032, Perplexity: 5.4914, Time: 0.22s
2025-04-25 23:57:16,736 - INFO - Batch: 30/141, Loss: 1.6967, Perplexity: 5.4557, Time: 0.26s
2025-04-25 23:57:16,954 - INFO - Batch: 40/141, Loss: 1.7131, Perplexity: 5.5461, Time: 0.22s
2025-04-25 23:57:17,226 - INFO - Batch: 50/141, Loss: 1.7159, Perplexity: 5.5615, Time: 0.27s
2025-04-25 23:57:17,443 - INFO - Batch: 60/141, Loss: 1.7119, Perplexity: 5.5396, Time: 0.22s
2025-04-25 23:57:17,661 - INFO - Batch: 70/141, Loss: 1.6959, Perplexity: 5.4515, Time: 0.22s
2025-04-25 23:57:17,883 - INFO - Batch: 80/141, Loss: 1.7129, Perplexity: 5.5449, Time: 0.22s
2025-04-25 23:57:18,096 - INFO - Batch: 90/141, Loss: 1.7104, Perplexity: 5.5310, Time: 0.21s
2025-04-25 23:57:18,311 - INFO - Batch: 100/141, Loss: 1.7027, Perplexity: 5.4889, Time: 0.21s
2025-04-25 23:57:18,529 - INFO - Batch: 110/141, Loss: 1.7045, Perplexity: 5.4985, Time: 0.22s
2025-04-25 23:57:18,750 - INFO - Batch: 120/141, Loss: 1.7172, Perplexity: 5.5690, Time: 0.22s
2025-04-25 23:57:19,021 - INFO - Batch: 130/141, Loss: 1.7124, Perplexity: 5.5421, Time: 0.27s
2025-04-25 23:57:19,234 - INFO - Batch: 140/141, Loss: 1.7078, Perplexity: 5.5166, Time: 0.21s
2025-04-25 23:57:19,831 - INFO - Validation  Loss: 1.8763, Perplexity: 6.5296, Time: 0.57s
2025-04-25 23:57:19,831 - INFO - Epoch 157/300 - Train Loss: 1.7083, Val Loss: 1.8763
2025-04-25 23:57:20,047 - INFO - Batch: 10/141, Loss: 1.6955, Perplexity: 5.4495, Time: 0.22s
2025-04-25 23:57:20,325 - INFO - Batch: 20/141, Loss: 1.7088, Perplexity: 5.5221, Time: 0.28s
2025-04-25 23:57:20,540 - INFO - Batch: 30/141, Loss: 1.6816, Perplexity: 5.3743, Time: 0.22s
2025-04-25 23:57:20,759 - INFO - Batch: 40/141, Loss: 1.7069, Perplexity: 5.5117, Time: 0.22s
2025-04-25 23:57:20,971 - INFO - Batch: 50/141, Loss: 1.7226, Perplexity: 5.5989, Time: 0.21s
2025-04-25 23:57:21,188 - INFO - Batch: 60/141, Loss: 1.6973, Perplexity: 5.4592, Time: 0.22s
2025-04-25 23:57:21,404 - INFO - Batch: 70/141, Loss: 1.7094, Perplexity: 5.5258, Time: 0.22s
2025-04-25 23:57:21,617 - INFO - Batch: 80/141, Loss: 1.7018, Perplexity: 5.4838, Time: 0.21s
2025-04-25 23:57:21,888 - INFO - Batch: 90/141, Loss: 1.7005, Perplexity: 5.4766, Time: 0.27s
2025-04-25 23:57:22,100 - INFO - Batch: 100/141, Loss: 1.7047, Perplexity: 5.4995, Time: 0.21s
2025-04-25 23:57:22,310 - INFO - Batch: 110/141, Loss: 1.7206, Perplexity: 5.5878, Time: 0.21s
2025-04-25 23:57:22,529 - INFO - Batch: 120/141, Loss: 1.7283, Perplexity: 5.6309, Time: 0.22s
2025-04-25 23:57:22,751 - INFO - Batch: 130/141, Loss: 1.7029, Perplexity: 5.4897, Time: 0.22s
2025-04-25 23:57:22,975 - INFO - Batch: 140/141, Loss: 1.7200, Perplexity: 5.5848, Time: 0.22s
2025-04-25 23:57:23,609 - INFO - Validation  Loss: 1.8770, Perplexity: 6.5337, Time: 0.61s
2025-04-25 23:57:23,610 - INFO - Epoch 158/300 - Train Loss: 1.7082, Val Loss: 1.8770
2025-04-25 23:57:23,823 - INFO - Batch: 10/141, Loss: 1.7214, Perplexity: 5.5926, Time: 0.21s
2025-04-25 23:57:24,041 - INFO - Batch: 20/141, Loss: 1.7104, Perplexity: 5.5311, Time: 0.22s
2025-04-25 23:57:24,252 - INFO - Batch: 30/141, Loss: 1.7176, Perplexity: 5.5710, Time: 0.21s
2025-04-25 23:57:24,471 - INFO - Batch: 40/141, Loss: 1.7021, Perplexity: 5.4854, Time: 0.22s
2025-04-25 23:57:24,683 - INFO - Batch: 50/141, Loss: 1.7057, Perplexity: 5.5055, Time: 0.21s
2025-04-25 23:57:24,958 - INFO - Batch: 60/141, Loss: 1.7214, Perplexity: 5.5922, Time: 0.28s
2025-04-25 23:57:25,175 - INFO - Batch: 70/141, Loss: 1.7112, Perplexity: 5.5357, Time: 0.22s
2025-04-25 23:57:25,393 - INFO - Batch: 80/141, Loss: 1.7171, Perplexity: 5.5685, Time: 0.22s
2025-04-25 23:57:25,610 - INFO - Batch: 90/141, Loss: 1.7189, Perplexity: 5.5783, Time: 0.22s
2025-04-25 23:57:25,825 - INFO - Batch: 100/141, Loss: 1.7007, Perplexity: 5.4779, Time: 0.22s
2025-04-25 23:57:26,053 - INFO - Batch: 110/141, Loss: 1.7181, Perplexity: 5.5737, Time: 0.23s
2025-04-25 23:57:26,269 - INFO - Batch: 120/141, Loss: 1.6979, Perplexity: 5.4626, Time: 0.22s
2025-04-25 23:57:26,488 - INFO - Batch: 130/141, Loss: 1.7130, Perplexity: 5.5458, Time: 0.22s
2025-04-25 23:57:26,767 - INFO - Batch: 140/141, Loss: 1.7054, Perplexity: 5.5038, Time: 0.28s
2025-04-25 23:57:27,352 - INFO - Validation  Loss: 1.8760, Perplexity: 6.5273, Time: 0.56s
2025-04-25 23:57:27,352 - INFO - Epoch 159/300 - Train Loss: 1.7081, Val Loss: 1.8760
2025-04-25 23:57:27,568 - INFO - Batch: 10/141, Loss: 1.7144, Perplexity: 5.5535, Time: 0.22s
2025-04-25 23:57:27,787 - INFO - Batch: 20/141, Loss: 1.7038, Perplexity: 5.4949, Time: 0.22s
2025-04-25 23:57:28,060 - INFO - Batch: 30/141, Loss: 1.7082, Perplexity: 5.5188, Time: 0.27s
2025-04-25 23:57:28,276 - INFO - Batch: 40/141, Loss: 1.7098, Perplexity: 5.5280, Time: 0.22s
2025-04-25 23:57:28,493 - INFO - Batch: 50/141, Loss: 1.7059, Perplexity: 5.5063, Time: 0.22s
2025-04-25 23:57:28,704 - INFO - Batch: 60/141, Loss: 1.7084, Perplexity: 5.5202, Time: 0.21s
2025-04-25 23:57:28,956 - INFO - Batch: 70/141, Loss: 1.7141, Perplexity: 5.5518, Time: 0.25s
2025-04-25 23:57:29,184 - INFO - Batch: 80/141, Loss: 1.7075, Perplexity: 5.5149, Time: 0.23s
2025-04-25 23:57:29,407 - INFO - Batch: 90/141, Loss: 1.7110, Perplexity: 5.5344, Time: 0.22s
2025-04-25 23:57:29,691 - INFO - Batch: 100/141, Loss: 1.7125, Perplexity: 5.5426, Time: 0.28s
2025-04-25 23:57:29,906 - INFO - Batch: 110/141, Loss: 1.7073, Perplexity: 5.5138, Time: 0.22s
2025-04-25 23:57:30,131 - INFO - Batch: 120/141, Loss: 1.7014, Perplexity: 5.4819, Time: 0.22s
2025-04-25 23:57:30,345 - INFO - Batch: 130/141, Loss: 1.7184, Perplexity: 5.5759, Time: 0.21s
2025-04-25 23:57:30,600 - INFO - Batch: 140/141, Loss: 1.7087, Perplexity: 5.5216, Time: 0.25s
2025-04-25 23:57:31,254 - INFO - Validation  Loss: 1.8759, Perplexity: 6.5266, Time: 0.63s
2025-04-25 23:57:31,254 - INFO - Epoch 160/300 - Train Loss: 1.7081, Val Loss: 1.8759
2025-04-25 23:57:31,474 - INFO - Batch: 10/141, Loss: 1.7004, Perplexity: 5.4761, Time: 0.22s
2025-04-25 23:57:31,692 - INFO - Batch: 20/141, Loss: 1.6937, Perplexity: 5.4396, Time: 0.22s
2025-04-25 23:57:31,908 - INFO - Batch: 30/141, Loss: 1.6989, Perplexity: 5.4679, Time: 0.22s
2025-04-25 23:57:32,131 - INFO - Batch: 40/141, Loss: 1.7190, Perplexity: 5.5791, Time: 0.22s
2025-04-25 23:57:32,343 - INFO - Batch: 50/141, Loss: 1.7186, Perplexity: 5.5766, Time: 0.21s
2025-04-25 23:57:32,567 - INFO - Batch: 60/141, Loss: 1.7293, Perplexity: 5.6366, Time: 0.22s
2025-04-25 23:57:32,849 - INFO - Batch: 70/141, Loss: 1.7113, Perplexity: 5.5360, Time: 0.28s
2025-04-25 23:57:33,067 - INFO - Batch: 80/141, Loss: 1.7096, Perplexity: 5.5270, Time: 0.22s
2025-04-25 23:57:33,283 - INFO - Batch: 90/141, Loss: 1.7064, Perplexity: 5.5092, Time: 0.22s
2025-04-25 23:57:33,502 - INFO - Batch: 100/141, Loss: 1.7128, Perplexity: 5.5442, Time: 0.22s
2025-04-25 23:57:33,717 - INFO - Batch: 110/141, Loss: 1.7115, Perplexity: 5.5372, Time: 0.22s
2025-04-25 23:57:33,932 - INFO - Batch: 120/141, Loss: 1.7017, Perplexity: 5.4834, Time: 0.21s
2025-04-25 23:57:34,151 - INFO - Batch: 130/141, Loss: 1.6909, Perplexity: 5.4243, Time: 0.22s
2025-04-25 23:57:34,372 - INFO - Batch: 140/141, Loss: 1.7189, Perplexity: 5.5784, Time: 0.22s
2025-04-25 23:57:35,020 - INFO - Validation  Loss: 1.8762, Perplexity: 6.5284, Time: 0.56s
2025-04-25 23:57:35,020 - INFO - Epoch 161/300 - Train Loss: 1.7080, Val Loss: 1.8762
2025-04-25 23:57:35,247 - INFO - Batch: 10/141, Loss: 1.6977, Perplexity: 5.4613, Time: 0.23s
2025-04-25 23:57:35,469 - INFO - Batch: 20/141, Loss: 1.7010, Perplexity: 5.4795, Time: 0.22s
2025-04-25 23:57:35,742 - INFO - Batch: 30/141, Loss: 1.7038, Perplexity: 5.4946, Time: 0.27s
2025-04-25 23:57:35,955 - INFO - Batch: 40/141, Loss: 1.7104, Perplexity: 5.5309, Time: 0.21s
2025-04-25 23:57:36,174 - INFO - Batch: 50/141, Loss: 1.6945, Perplexity: 5.4442, Time: 0.22s
2025-04-25 23:57:36,392 - INFO - Batch: 60/141, Loss: 1.7170, Perplexity: 5.5678, Time: 0.22s
2025-04-25 23:57:36,605 - INFO - Batch: 70/141, Loss: 1.7006, Perplexity: 5.4772, Time: 0.21s
2025-04-25 23:57:36,818 - INFO - Batch: 80/141, Loss: 1.7059, Perplexity: 5.5061, Time: 0.21s
2025-04-25 23:57:37,034 - INFO - Batch: 90/141, Loss: 1.7185, Perplexity: 5.5759, Time: 0.22s
2025-04-25 23:57:37,250 - INFO - Batch: 100/141, Loss: 1.7079, Perplexity: 5.5171, Time: 0.22s
2025-04-25 23:57:37,528 - INFO - Batch: 110/141, Loss: 1.7068, Perplexity: 5.5112, Time: 0.28s
2025-04-25 23:57:37,741 - INFO - Batch: 120/141, Loss: 1.7100, Perplexity: 5.5291, Time: 0.21s
2025-04-25 23:57:37,960 - INFO - Batch: 130/141, Loss: 1.6921, Perplexity: 5.4308, Time: 0.22s
2025-04-25 23:57:38,189 - INFO - Batch: 140/141, Loss: 1.7120, Perplexity: 5.5402, Time: 0.23s
2025-04-25 23:57:38,858 - INFO - Validation  Loss: 1.8771, Perplexity: 6.5347, Time: 0.64s
2025-04-25 23:57:38,858 - INFO - Epoch 162/300 - Train Loss: 1.7080, Val Loss: 1.8771
2025-04-25 23:57:39,083 - INFO - Batch: 10/141, Loss: 1.7009, Perplexity: 5.4787, Time: 0.23s
2025-04-25 23:57:39,298 - INFO - Batch: 20/141, Loss: 1.7116, Perplexity: 5.5379, Time: 0.21s
2025-04-25 23:57:39,517 - INFO - Batch: 30/141, Loss: 1.7088, Perplexity: 5.5225, Time: 0.22s
2025-04-25 23:57:39,728 - INFO - Batch: 40/141, Loss: 1.7101, Perplexity: 5.5296, Time: 0.21s
2025-04-25 23:57:39,940 - INFO - Batch: 50/141, Loss: 1.7182, Perplexity: 5.5746, Time: 0.21s
2025-04-25 23:57:40,202 - INFO - Batch: 60/141, Loss: 1.7233, Perplexity: 5.6028, Time: 0.26s
2025-04-25 23:57:40,488 - INFO - Batch: 70/141, Loss: 1.7053, Perplexity: 5.5028, Time: 0.29s
2025-04-25 23:57:40,776 - INFO - Batch: 80/141, Loss: 1.7148, Perplexity: 5.5556, Time: 0.29s
2025-04-25 23:57:40,993 - INFO - Batch: 90/141, Loss: 1.6999, Perplexity: 5.4734, Time: 0.22s
2025-04-25 23:57:41,214 - INFO - Batch: 100/141, Loss: 1.7121, Perplexity: 5.5408, Time: 0.22s
2025-04-25 23:57:41,478 - INFO - Batch: 110/141, Loss: 1.6930, Perplexity: 5.4360, Time: 0.26s
2025-04-25 23:57:41,697 - INFO - Batch: 120/141, Loss: 1.7138, Perplexity: 5.5503, Time: 0.22s
2025-04-25 23:57:41,916 - INFO - Batch: 130/141, Loss: 1.7081, Perplexity: 5.5185, Time: 0.22s
2025-04-25 23:57:42,148 - INFO - Batch: 140/141, Loss: 1.7138, Perplexity: 5.5501, Time: 0.23s
2025-04-25 23:57:42,849 - INFO - Validation  Loss: 1.8766, Perplexity: 6.5311, Time: 0.67s
2025-04-25 23:57:42,849 - INFO - Epoch 163/300 - Train Loss: 1.7079, Val Loss: 1.8766
2025-04-25 23:57:43,093 - INFO - Batch: 10/141, Loss: 1.7365, Perplexity: 5.6773, Time: 0.24s
2025-04-25 23:57:43,309 - INFO - Batch: 20/141, Loss: 1.7059, Perplexity: 5.5065, Time: 0.22s
2025-04-25 23:57:43,532 - INFO - Batch: 30/141, Loss: 1.7167, Perplexity: 5.5661, Time: 0.22s
2025-04-25 23:57:43,820 - INFO - Batch: 40/141, Loss: 1.7002, Perplexity: 5.4751, Time: 0.29s
2025-04-25 23:57:44,064 - INFO - Batch: 50/141, Loss: 1.7001, Perplexity: 5.4745, Time: 0.24s
2025-04-25 23:57:44,327 - INFO - Batch: 60/141, Loss: 1.7024, Perplexity: 5.4869, Time: 0.26s
2025-04-25 23:57:44,555 - INFO - Batch: 70/141, Loss: 1.7047, Perplexity: 5.4997, Time: 0.23s
2025-04-25 23:57:44,810 - INFO - Batch: 80/141, Loss: 1.7010, Perplexity: 5.4795, Time: 0.26s
2025-04-25 23:57:45,039 - INFO - Batch: 90/141, Loss: 1.7071, Perplexity: 5.5132, Time: 0.23s
2025-04-25 23:57:45,272 - INFO - Batch: 100/141, Loss: 1.6933, Perplexity: 5.4373, Time: 0.23s
2025-04-25 23:57:45,508 - INFO - Batch: 110/141, Loss: 1.7135, Perplexity: 5.5486, Time: 0.24s
2025-04-25 23:57:45,791 - INFO - Batch: 120/141, Loss: 1.7137, Perplexity: 5.5493, Time: 0.28s
2025-04-25 23:57:46,009 - INFO - Batch: 130/141, Loss: 1.7134, Perplexity: 5.5476, Time: 0.22s
2025-04-25 23:57:46,241 - INFO - Batch: 140/141, Loss: 1.7113, Perplexity: 5.5364, Time: 0.23s
2025-04-25 23:57:46,908 - INFO - Validation  Loss: 1.8763, Perplexity: 6.5294, Time: 0.64s
2025-04-25 23:57:46,908 - INFO - Epoch 164/300 - Train Loss: 1.7078, Val Loss: 1.8763
2025-04-25 23:57:47,190 - INFO - Batch: 10/141, Loss: 1.7022, Perplexity: 5.4862, Time: 0.28s
2025-04-25 23:57:47,404 - INFO - Batch: 20/141, Loss: 1.7055, Perplexity: 5.5042, Time: 0.21s
2025-04-25 23:57:47,617 - INFO - Batch: 30/141, Loss: 1.7107, Perplexity: 5.5327, Time: 0.21s
2025-04-25 23:57:47,830 - INFO - Batch: 40/141, Loss: 1.7151, Perplexity: 5.5570, Time: 0.21s
2025-04-25 23:57:48,081 - INFO - Batch: 50/141, Loss: 1.7071, Perplexity: 5.5131, Time: 0.25s
2025-04-25 23:57:48,303 - INFO - Batch: 60/141, Loss: 1.7045, Perplexity: 5.4984, Time: 0.22s
2025-04-25 23:57:48,590 - INFO - Batch: 70/141, Loss: 1.7043, Perplexity: 5.4975, Time: 0.29s
2025-04-25 23:57:48,817 - INFO - Batch: 80/141, Loss: 1.6995, Perplexity: 5.4712, Time: 0.23s
2025-04-25 23:57:49,112 - INFO - Batch: 90/141, Loss: 1.6956, Perplexity: 5.4499, Time: 0.29s
2025-04-25 23:57:49,354 - INFO - Batch: 100/141, Loss: 1.7208, Perplexity: 5.5892, Time: 0.24s
2025-04-25 23:57:49,579 - INFO - Batch: 110/141, Loss: 1.7023, Perplexity: 5.4868, Time: 0.23s
2025-04-25 23:57:49,806 - INFO - Batch: 120/141, Loss: 1.6989, Perplexity: 5.4680, Time: 0.23s
2025-04-25 23:57:50,033 - INFO - Batch: 130/141, Loss: 1.7030, Perplexity: 5.4904, Time: 0.23s
2025-04-25 23:57:50,267 - INFO - Batch: 140/141, Loss: 1.7035, Perplexity: 5.4929, Time: 0.23s
2025-04-25 23:57:50,919 - INFO - Validation  Loss: 1.8764, Perplexity: 6.5301, Time: 0.62s
2025-04-25 23:57:50,919 - INFO - Epoch 165/300 - Train Loss: 1.7077, Val Loss: 1.8764
2025-04-25 23:57:51,151 - INFO - Batch: 10/141, Loss: 1.7213, Perplexity: 5.5917, Time: 0.23s
2025-04-25 23:57:51,376 - INFO - Batch: 20/141, Loss: 1.7149, Perplexity: 5.5561, Time: 0.22s
2025-04-25 23:57:51,595 - INFO - Batch: 30/141, Loss: 1.7080, Perplexity: 5.5179, Time: 0.22s
2025-04-25 23:57:51,861 - INFO - Batch: 40/141, Loss: 1.7023, Perplexity: 5.4865, Time: 0.27s
2025-04-25 23:57:52,141 - INFO - Batch: 50/141, Loss: 1.7129, Perplexity: 5.5448, Time: 0.28s
2025-04-25 23:57:52,368 - INFO - Batch: 60/141, Loss: 1.7120, Perplexity: 5.5398, Time: 0.23s
2025-04-25 23:57:52,588 - INFO - Batch: 70/141, Loss: 1.7071, Perplexity: 5.5128, Time: 0.22s
2025-04-25 23:57:52,820 - INFO - Batch: 80/141, Loss: 1.7139, Perplexity: 5.5507, Time: 0.23s
2025-04-25 23:57:53,079 - INFO - Batch: 90/141, Loss: 1.7046, Perplexity: 5.4992, Time: 0.26s
2025-04-25 23:57:53,313 - INFO - Batch: 100/141, Loss: 1.7072, Perplexity: 5.5135, Time: 0.23s
2025-04-25 23:57:53,533 - INFO - Batch: 110/141, Loss: 1.7038, Perplexity: 5.4946, Time: 0.22s
2025-04-25 23:57:53,771 - INFO - Batch: 120/141, Loss: 1.7156, Perplexity: 5.5602, Time: 0.24s
2025-04-25 23:57:54,056 - INFO - Batch: 130/141, Loss: 1.7172, Perplexity: 5.5692, Time: 0.29s
2025-04-25 23:57:54,285 - INFO - Batch: 140/141, Loss: 1.7160, Perplexity: 5.5620, Time: 0.23s
2025-04-25 23:57:54,882 - INFO - Validation  Loss: 1.8760, Perplexity: 6.5274, Time: 0.57s
2025-04-25 23:57:54,882 - INFO - Epoch 166/300 - Train Loss: 1.7076, Val Loss: 1.8760
2025-04-25 23:57:55,107 - INFO - Batch: 10/141, Loss: 1.7058, Perplexity: 5.5059, Time: 0.22s
2025-04-25 23:57:55,401 - INFO - Batch: 20/141, Loss: 1.6952, Perplexity: 5.4479, Time: 0.29s
2025-04-25 23:57:55,622 - INFO - Batch: 30/141, Loss: 1.7063, Perplexity: 5.5087, Time: 0.22s
2025-04-25 23:57:55,851 - INFO - Batch: 40/141, Loss: 1.7113, Perplexity: 5.5362, Time: 0.23s
2025-04-25 23:57:56,068 - INFO - Batch: 50/141, Loss: 1.7022, Perplexity: 5.4858, Time: 0.22s
2025-04-25 23:57:56,313 - INFO - Batch: 60/141, Loss: 1.7136, Perplexity: 5.5490, Time: 0.25s
2025-04-25 23:57:56,537 - INFO - Batch: 70/141, Loss: 1.7339, Perplexity: 5.6627, Time: 0.22s
2025-04-25 23:57:56,758 - INFO - Batch: 80/141, Loss: 1.7033, Perplexity: 5.4918, Time: 0.22s
2025-04-25 23:57:56,985 - INFO - Batch: 90/141, Loss: 1.7128, Perplexity: 5.5446, Time: 0.23s
2025-04-25 23:57:57,280 - INFO - Batch: 100/141, Loss: 1.7071, Perplexity: 5.5129, Time: 0.29s
2025-04-25 23:57:57,504 - INFO - Batch: 110/141, Loss: 1.7013, Perplexity: 5.4811, Time: 0.22s
2025-04-25 23:57:57,726 - INFO - Batch: 120/141, Loss: 1.7093, Perplexity: 5.5252, Time: 0.22s
2025-04-25 23:57:57,959 - INFO - Batch: 130/141, Loss: 1.7163, Perplexity: 5.5639, Time: 0.23s
2025-04-25 23:57:58,182 - INFO - Batch: 140/141, Loss: 1.6877, Perplexity: 5.4070, Time: 0.22s
2025-04-25 23:57:58,858 - INFO - Validation  Loss: 1.8758, Perplexity: 6.5258, Time: 0.65s
2025-04-25 23:57:58,859 - INFO - Epoch 167/300 - Train Loss: 1.7076, Val Loss: 1.8758
2025-04-25 23:57:59,089 - INFO - Batch: 10/141, Loss: 1.7031, Perplexity: 5.4908, Time: 0.23s
2025-04-25 23:57:59,324 - INFO - Batch: 20/141, Loss: 1.6968, Perplexity: 5.4567, Time: 0.23s
2025-04-25 23:57:59,546 - INFO - Batch: 30/141, Loss: 1.6991, Perplexity: 5.4689, Time: 0.22s
2025-04-25 23:57:59,768 - INFO - Batch: 40/141, Loss: 1.6968, Perplexity: 5.4564, Time: 0.22s
2025-04-25 23:57:59,995 - INFO - Batch: 50/141, Loss: 1.6953, Perplexity: 5.4480, Time: 0.23s
2025-04-25 23:58:00,295 - INFO - Batch: 60/141, Loss: 1.7034, Perplexity: 5.4926, Time: 0.30s
2025-04-25 23:58:00,522 - INFO - Batch: 70/141, Loss: 1.7090, Perplexity: 5.5232, Time: 0.23s
2025-04-25 23:58:00,742 - INFO - Batch: 80/141, Loss: 1.7083, Perplexity: 5.5194, Time: 0.22s
2025-04-25 23:58:00,966 - INFO - Batch: 90/141, Loss: 1.6863, Perplexity: 5.3993, Time: 0.22s
2025-04-25 23:58:01,191 - INFO - Batch: 100/141, Loss: 1.6923, Perplexity: 5.4320, Time: 0.23s
2025-04-25 23:58:01,423 - INFO - Batch: 110/141, Loss: 1.7020, Perplexity: 5.4847, Time: 0.23s
2025-04-25 23:58:01,651 - INFO - Batch: 120/141, Loss: 1.7135, Perplexity: 5.5485, Time: 0.23s
2025-04-25 23:58:01,888 - INFO - Batch: 130/141, Loss: 1.7083, Perplexity: 5.5195, Time: 0.24s
2025-04-25 23:58:02,170 - INFO - Batch: 140/141, Loss: 1.7009, Perplexity: 5.4787, Time: 0.28s
2025-04-25 23:58:02,770 - INFO - Validation  Loss: 1.8761, Perplexity: 6.5282, Time: 0.57s
2025-04-25 23:58:02,770 - INFO - Epoch 168/300 - Train Loss: 1.7074, Val Loss: 1.8761
2025-04-25 23:58:02,999 - INFO - Batch: 10/141, Loss: 1.6981, Perplexity: 5.4634, Time: 0.23s
2025-04-25 23:58:03,219 - INFO - Batch: 20/141, Loss: 1.6974, Perplexity: 5.4597, Time: 0.22s
2025-04-25 23:58:03,516 - INFO - Batch: 30/141, Loss: 1.7084, Perplexity: 5.5200, Time: 0.30s
2025-04-25 23:58:03,737 - INFO - Batch: 40/141, Loss: 1.7085, Perplexity: 5.5207, Time: 0.22s
2025-04-25 23:58:03,962 - INFO - Batch: 50/141, Loss: 1.7183, Perplexity: 5.5752, Time: 0.23s
2025-04-25 23:58:04,183 - INFO - Batch: 60/141, Loss: 1.6932, Perplexity: 5.4367, Time: 0.22s
2025-04-25 23:58:04,412 - INFO - Batch: 70/141, Loss: 1.7089, Perplexity: 5.5228, Time: 0.23s
2025-04-25 23:58:04,633 - INFO - Batch: 80/141, Loss: 1.6953, Perplexity: 5.4483, Time: 0.22s
2025-04-25 23:58:04,858 - INFO - Batch: 90/141, Loss: 1.6998, Perplexity: 5.4728, Time: 0.22s
2025-04-25 23:58:05,145 - INFO - Batch: 100/141, Loss: 1.7128, Perplexity: 5.5445, Time: 0.29s
2025-04-25 23:58:05,373 - INFO - Batch: 110/141, Loss: 1.7041, Perplexity: 5.4963, Time: 0.23s
2025-04-25 23:58:05,593 - INFO - Batch: 120/141, Loss: 1.7131, Perplexity: 5.5463, Time: 0.22s
2025-04-25 23:58:05,823 - INFO - Batch: 130/141, Loss: 1.7102, Perplexity: 5.5300, Time: 0.23s
2025-04-25 23:58:06,050 - INFO - Batch: 140/141, Loss: 1.7095, Perplexity: 5.5260, Time: 0.23s
2025-04-25 23:58:06,744 - INFO - Validation  Loss: 1.8759, Perplexity: 6.5268, Time: 0.66s
2025-04-25 23:58:06,745 - INFO - Epoch 169/300 - Train Loss: 1.7074, Val Loss: 1.8759
2025-04-25 23:58:06,977 - INFO - Batch: 10/141, Loss: 1.7089, Perplexity: 5.5230, Time: 0.23s
2025-04-25 23:58:07,201 - INFO - Batch: 20/141, Loss: 1.7049, Perplexity: 5.5007, Time: 0.22s
2025-04-25 23:58:07,426 - INFO - Batch: 30/141, Loss: 1.7121, Perplexity: 5.5405, Time: 0.23s
2025-04-25 23:58:07,644 - INFO - Batch: 40/141, Loss: 1.7013, Perplexity: 5.4810, Time: 0.22s
2025-04-25 23:58:07,866 - INFO - Batch: 50/141, Loss: 1.7034, Perplexity: 5.4925, Time: 0.22s
2025-04-25 23:58:08,130 - INFO - Batch: 60/141, Loss: 1.7041, Perplexity: 5.4966, Time: 0.26s
2025-04-25 23:58:08,410 - INFO - Batch: 70/141, Loss: 1.7006, Perplexity: 5.4772, Time: 0.28s
2025-04-25 23:58:08,632 - INFO - Batch: 80/141, Loss: 1.7061, Perplexity: 5.5076, Time: 0.22s
2025-04-25 23:58:08,851 - INFO - Batch: 90/141, Loss: 1.6971, Perplexity: 5.4582, Time: 0.22s
2025-04-25 23:58:09,071 - INFO - Batch: 100/141, Loss: 1.6947, Perplexity: 5.4452, Time: 0.22s
2025-04-25 23:58:09,295 - INFO - Batch: 110/141, Loss: 1.6988, Perplexity: 5.4674, Time: 0.22s
2025-04-25 23:58:09,522 - INFO - Batch: 120/141, Loss: 1.6958, Perplexity: 5.4509, Time: 0.23s
2025-04-25 23:58:09,745 - INFO - Batch: 130/141, Loss: 1.7028, Perplexity: 5.4894, Time: 0.22s
2025-04-25 23:58:09,964 - INFO - Batch: 140/141, Loss: 1.7155, Perplexity: 5.5597, Time: 0.22s
2025-04-25 23:58:10,626 - INFO - Validation  Loss: 1.8763, Perplexity: 6.5293, Time: 0.63s
2025-04-25 23:58:10,626 - INFO - Epoch 170/300 - Train Loss: 1.7074, Val Loss: 1.8763
2025-04-25 23:58:10,847 - INFO - Batch: 10/141, Loss: 1.7118, Perplexity: 5.5389, Time: 0.22s
2025-04-25 23:58:11,083 - INFO - Batch: 20/141, Loss: 1.7265, Perplexity: 5.6211, Time: 0.24s
2025-04-25 23:58:11,349 - INFO - Batch: 30/141, Loss: 1.7056, Perplexity: 5.5046, Time: 0.27s
2025-04-25 23:58:11,722 - INFO - Batch: 40/141, Loss: 1.7103, Perplexity: 5.5304, Time: 0.37s
2025-04-25 23:58:12,039 - INFO - Batch: 50/141, Loss: 1.7105, Perplexity: 5.5316, Time: 0.32s
2025-04-25 23:58:12,280 - INFO - Batch: 60/141, Loss: 1.7064, Perplexity: 5.5089, Time: 0.24s
2025-04-25 23:58:12,556 - INFO - Batch: 70/141, Loss: 1.6997, Perplexity: 5.4722, Time: 0.28s
2025-04-25 23:58:12,822 - INFO - Batch: 80/141, Loss: 1.7060, Perplexity: 5.5071, Time: 0.27s
2025-04-25 23:58:13,079 - INFO - Batch: 90/141, Loss: 1.7146, Perplexity: 5.5544, Time: 0.26s
2025-04-25 23:58:13,305 - INFO - Batch: 100/141, Loss: 1.6989, Perplexity: 5.4682, Time: 0.23s
2025-04-25 23:58:13,606 - INFO - Batch: 110/141, Loss: 1.7030, Perplexity: 5.4901, Time: 0.30s
2025-04-25 23:58:13,854 - INFO - Batch: 120/141, Loss: 1.6856, Perplexity: 5.3955, Time: 0.25s
2025-04-25 23:58:14,084 - INFO - Batch: 130/141, Loss: 1.7034, Perplexity: 5.4925, Time: 0.23s
2025-04-25 23:58:14,312 - INFO - Batch: 140/141, Loss: 1.7039, Perplexity: 5.4952, Time: 0.23s
2025-04-25 23:58:15,114 - INFO - Validation  Loss: 1.8755, Perplexity: 6.5238, Time: 0.75s
2025-04-25 23:58:15,114 - INFO - Epoch 171/300 - Train Loss: 1.7072, Val Loss: 1.8755
2025-04-25 23:58:15,346 - INFO - Batch: 10/141, Loss: 1.7084, Perplexity: 5.5199, Time: 0.23s
2025-04-25 23:58:15,574 - INFO - Batch: 20/141, Loss: 1.7097, Perplexity: 5.5272, Time: 0.23s
2025-04-25 23:58:15,807 - INFO - Batch: 30/141, Loss: 1.7090, Perplexity: 5.5236, Time: 0.23s
2025-04-25 23:58:16,042 - INFO - Batch: 40/141, Loss: 1.6991, Perplexity: 5.4691, Time: 0.23s
2025-04-25 23:58:16,268 - INFO - Batch: 50/141, Loss: 1.7043, Perplexity: 5.4974, Time: 0.23s
2025-04-25 23:58:16,492 - INFO - Batch: 60/141, Loss: 1.7195, Perplexity: 5.5818, Time: 0.22s
2025-04-25 23:58:16,730 - INFO - Batch: 70/141, Loss: 1.7094, Perplexity: 5.5256, Time: 0.24s
2025-04-25 23:58:17,010 - INFO - Batch: 80/141, Loss: 1.7300, Perplexity: 5.6407, Time: 0.28s
2025-04-25 23:58:17,238 - INFO - Batch: 90/141, Loss: 1.7054, Perplexity: 5.5034, Time: 0.23s
2025-04-25 23:58:17,462 - INFO - Batch: 100/141, Loss: 1.7035, Perplexity: 5.4933, Time: 0.22s
2025-04-25 23:58:17,692 - INFO - Batch: 110/141, Loss: 1.6982, Perplexity: 5.4640, Time: 0.23s
2025-04-25 23:58:17,918 - INFO - Batch: 120/141, Loss: 1.7068, Perplexity: 5.5111, Time: 0.23s
2025-04-25 23:58:18,156 - INFO - Batch: 130/141, Loss: 1.6940, Perplexity: 5.4414, Time: 0.24s
2025-04-25 23:58:18,378 - INFO - Batch: 140/141, Loss: 1.7210, Perplexity: 5.5902, Time: 0.22s
2025-04-25 23:58:19,037 - INFO - Validation  Loss: 1.8761, Perplexity: 6.5280, Time: 0.62s
2025-04-25 23:58:19,037 - INFO - Epoch 172/300 - Train Loss: 1.7071, Val Loss: 1.8761
2025-04-25 23:58:19,261 - INFO - Batch: 10/141, Loss: 1.7126, Perplexity: 5.5433, Time: 0.22s
2025-04-25 23:58:19,486 - INFO - Batch: 20/141, Loss: 1.6983, Perplexity: 5.4645, Time: 0.22s
2025-04-25 23:58:19,710 - INFO - Batch: 30/141, Loss: 1.6990, Perplexity: 5.4684, Time: 0.22s
2025-04-25 23:58:19,935 - INFO - Batch: 40/141, Loss: 1.7098, Perplexity: 5.5278, Time: 0.22s
2025-04-25 23:58:20,221 - INFO - Batch: 50/141, Loss: 1.7060, Perplexity: 5.5067, Time: 0.29s
2025-04-25 23:58:20,443 - INFO - Batch: 60/141, Loss: 1.6981, Perplexity: 5.4635, Time: 0.22s
2025-04-25 23:58:20,682 - INFO - Batch: 70/141, Loss: 1.7193, Perplexity: 5.5808, Time: 0.24s
2025-04-25 23:58:20,979 - INFO - Batch: 80/141, Loss: 1.7174, Perplexity: 5.5700, Time: 0.30s
2025-04-25 23:58:21,274 - INFO - Batch: 90/141, Loss: 1.7115, Perplexity: 5.5372, Time: 0.30s
2025-04-25 23:58:21,580 - INFO - Batch: 100/141, Loss: 1.7152, Perplexity: 5.5577, Time: 0.31s
2025-04-25 23:58:21,811 - INFO - Batch: 110/141, Loss: 1.7197, Perplexity: 5.5827, Time: 0.23s
2025-04-25 23:58:22,101 - INFO - Batch: 120/141, Loss: 1.6984, Perplexity: 5.4653, Time: 0.29s
2025-04-25 23:58:22,351 - INFO - Batch: 130/141, Loss: 1.6930, Perplexity: 5.4357, Time: 0.25s
2025-04-25 23:58:22,578 - INFO - Batch: 140/141, Loss: 1.7002, Perplexity: 5.4750, Time: 0.23s
2025-04-25 23:58:23,240 - INFO - Validation  Loss: 1.8760, Perplexity: 6.5275, Time: 0.63s
2025-04-25 23:58:23,240 - INFO - Epoch 173/300 - Train Loss: 1.7070, Val Loss: 1.8760
2025-04-25 23:58:23,552 - INFO - Batch: 10/141, Loss: 1.6992, Perplexity: 5.4694, Time: 0.31s
2025-04-25 23:58:23,844 - INFO - Batch: 20/141, Loss: 1.7144, Perplexity: 5.5534, Time: 0.29s
2025-04-25 23:58:24,135 - INFO - Batch: 30/141, Loss: 1.7032, Perplexity: 5.4916, Time: 0.29s
2025-04-25 23:58:24,368 - INFO - Batch: 40/141, Loss: 1.7032, Perplexity: 5.4917, Time: 0.23s
2025-04-25 23:58:24,647 - INFO - Batch: 50/141, Loss: 1.7000, Perplexity: 5.4738, Time: 0.28s
2025-04-25 23:58:24,870 - INFO - Batch: 60/141, Loss: 1.7145, Perplexity: 5.5538, Time: 0.22s
2025-04-25 23:58:25,089 - INFO - Batch: 70/141, Loss: 1.7086, Perplexity: 5.5213, Time: 0.22s
2025-04-25 23:58:25,330 - INFO - Batch: 80/141, Loss: 1.7091, Perplexity: 5.5238, Time: 0.24s
2025-04-25 23:58:25,658 - INFO - Batch: 90/141, Loss: 1.6976, Perplexity: 5.4608, Time: 0.33s
2025-04-25 23:58:25,878 - INFO - Batch: 100/141, Loss: 1.7093, Perplexity: 5.5253, Time: 0.22s
2025-04-25 23:58:26,105 - INFO - Batch: 110/141, Loss: 1.7088, Perplexity: 5.5222, Time: 0.23s
2025-04-25 23:58:26,334 - INFO - Batch: 120/141, Loss: 1.7080, Perplexity: 5.5182, Time: 0.23s
2025-04-25 23:58:26,553 - INFO - Batch: 130/141, Loss: 1.7100, Perplexity: 5.5290, Time: 0.22s
2025-04-25 23:58:26,772 - INFO - Batch: 140/141, Loss: 1.6911, Perplexity: 5.4253, Time: 0.22s
2025-04-25 23:58:27,420 - INFO - Validation  Loss: 1.8759, Perplexity: 6.5265, Time: 0.62s
2025-04-25 23:58:27,420 - INFO - Epoch 174/300 - Train Loss: 1.7070, Val Loss: 1.8759
2025-04-25 23:58:27,639 - INFO - Batch: 10/141, Loss: 1.7123, Perplexity: 5.5419, Time: 0.22s
2025-04-25 23:58:27,863 - INFO - Batch: 20/141, Loss: 1.7170, Perplexity: 5.5678, Time: 0.22s
2025-04-25 23:58:28,104 - INFO - Batch: 30/141, Loss: 1.7044, Perplexity: 5.4983, Time: 0.24s
2025-04-25 23:58:28,322 - INFO - Batch: 40/141, Loss: 1.7134, Perplexity: 5.5477, Time: 0.22s
2025-04-25 23:58:28,595 - INFO - Batch: 50/141, Loss: 1.6922, Perplexity: 5.4316, Time: 0.27s
2025-04-25 23:58:28,822 - INFO - Batch: 60/141, Loss: 1.7021, Perplexity: 5.4854, Time: 0.23s
2025-04-25 23:58:29,042 - INFO - Batch: 70/141, Loss: 1.7162, Perplexity: 5.5632, Time: 0.22s
2025-04-25 23:58:29,272 - INFO - Batch: 80/141, Loss: 1.7090, Perplexity: 5.5235, Time: 0.23s
2025-04-25 23:58:29,491 - INFO - Batch: 90/141, Loss: 1.6977, Perplexity: 5.4616, Time: 0.22s
2025-04-25 23:58:29,712 - INFO - Batch: 100/141, Loss: 1.7193, Perplexity: 5.5806, Time: 0.22s
2025-04-25 23:58:29,960 - INFO - Batch: 110/141, Loss: 1.7140, Perplexity: 5.5513, Time: 0.25s
2025-04-25 23:58:30,189 - INFO - Batch: 120/141, Loss: 1.7036, Perplexity: 5.4937, Time: 0.23s
2025-04-25 23:58:30,484 - INFO - Batch: 130/141, Loss: 1.6951, Perplexity: 5.4472, Time: 0.29s
2025-04-25 23:58:30,711 - INFO - Batch: 140/141, Loss: 1.7163, Perplexity: 5.5637, Time: 0.23s
2025-04-25 23:58:31,315 - INFO - Validation  Loss: 1.8766, Perplexity: 6.5314, Time: 0.57s
2025-04-25 23:58:31,315 - INFO - Epoch 175/300 - Train Loss: 1.7070, Val Loss: 1.8766
2025-04-25 23:58:31,542 - INFO - Batch: 10/141, Loss: 1.7142, Perplexity: 5.5523, Time: 0.23s
2025-04-25 23:58:31,830 - INFO - Batch: 20/141, Loss: 1.7171, Perplexity: 5.5682, Time: 0.29s
2025-04-25 23:58:32,062 - INFO - Batch: 30/141, Loss: 1.7006, Perplexity: 5.4773, Time: 0.23s
2025-04-25 23:58:32,293 - INFO - Batch: 40/141, Loss: 1.7138, Perplexity: 5.5500, Time: 0.23s
2025-04-25 23:58:32,526 - INFO - Batch: 50/141, Loss: 1.6797, Perplexity: 5.3639, Time: 0.23s
2025-04-25 23:58:32,769 - INFO - Batch: 60/141, Loss: 1.7216, Perplexity: 5.5936, Time: 0.24s
2025-04-25 23:58:33,007 - INFO - Batch: 70/141, Loss: 1.7029, Perplexity: 5.4900, Time: 0.24s
2025-04-25 23:58:33,329 - INFO - Batch: 80/141, Loss: 1.7043, Perplexity: 5.4973, Time: 0.32s
2025-04-25 23:58:33,569 - INFO - Batch: 90/141, Loss: 1.7054, Perplexity: 5.5038, Time: 0.24s
2025-04-25 23:58:33,920 - INFO - Batch: 100/141, Loss: 1.6941, Perplexity: 5.4419, Time: 0.35s
2025-04-25 23:58:34,183 - INFO - Batch: 110/141, Loss: 1.7196, Perplexity: 5.5822, Time: 0.26s
2025-04-25 23:58:34,444 - INFO - Batch: 120/141, Loss: 1.7139, Perplexity: 5.5505, Time: 0.26s
2025-04-25 23:58:34,705 - INFO - Batch: 130/141, Loss: 1.7114, Perplexity: 5.5366, Time: 0.26s
2025-04-25 23:58:34,979 - INFO - Batch: 140/141, Loss: 1.7230, Perplexity: 5.6013, Time: 0.27s
2025-04-25 23:58:35,794 - INFO - Validation  Loss: 1.8763, Perplexity: 6.5292, Time: 0.78s
2025-04-25 23:58:35,794 - INFO - Epoch 176/300 - Train Loss: 1.7069, Val Loss: 1.8763
2025-04-25 23:58:36,069 - INFO - Batch: 10/141, Loss: 1.7015, Perplexity: 5.4819, Time: 0.28s
2025-04-25 23:58:36,334 - INFO - Batch: 20/141, Loss: 1.7048, Perplexity: 5.5004, Time: 0.26s
2025-04-25 23:58:36,593 - INFO - Batch: 30/141, Loss: 1.7266, Perplexity: 5.6212, Time: 0.26s
2025-04-25 23:58:36,860 - INFO - Batch: 40/141, Loss: 1.6989, Perplexity: 5.4680, Time: 0.27s
2025-04-25 23:58:37,130 - INFO - Batch: 50/141, Loss: 1.6891, Perplexity: 5.4147, Time: 0.27s
2025-04-25 23:58:37,466 - INFO - Batch: 60/141, Loss: 1.7083, Perplexity: 5.5193, Time: 0.34s
2025-04-25 23:58:37,689 - INFO - Batch: 70/141, Loss: 1.7040, Perplexity: 5.4960, Time: 0.22s
2025-04-25 23:58:37,920 - INFO - Batch: 80/141, Loss: 1.7020, Perplexity: 5.4847, Time: 0.23s
2025-04-25 23:58:38,151 - INFO - Batch: 90/141, Loss: 1.6825, Perplexity: 5.3788, Time: 0.23s
2025-04-25 23:58:38,385 - INFO - Batch: 100/141, Loss: 1.7107, Perplexity: 5.5331, Time: 0.23s
2025-04-25 23:58:38,625 - INFO - Batch: 110/141, Loss: 1.7403, Perplexity: 5.6992, Time: 0.24s
2025-04-25 23:58:38,853 - INFO - Batch: 120/141, Loss: 1.7121, Perplexity: 5.5404, Time: 0.23s
2025-04-25 23:58:39,076 - INFO - Batch: 130/141, Loss: 1.6890, Perplexity: 5.4143, Time: 0.22s
2025-04-25 23:58:39,372 - INFO - Batch: 140/141, Loss: 1.7108, Perplexity: 5.5334, Time: 0.30s
2025-04-25 23:58:40,053 - INFO - Validation  Loss: 1.8763, Perplexity: 6.5293, Time: 0.65s
2025-04-25 23:58:40,053 - INFO - Epoch 177/300 - Train Loss: 1.7068, Val Loss: 1.8763
2025-04-25 23:58:40,284 - INFO - Batch: 10/141, Loss: 1.7061, Perplexity: 5.5072, Time: 0.23s
2025-04-25 23:58:40,533 - INFO - Batch: 20/141, Loss: 1.7163, Perplexity: 5.5642, Time: 0.25s
2025-04-25 23:58:40,828 - INFO - Batch: 30/141, Loss: 1.7126, Perplexity: 5.5433, Time: 0.29s
2025-04-25 23:58:41,056 - INFO - Batch: 40/141, Loss: 1.7161, Perplexity: 5.5627, Time: 0.23s
2025-04-25 23:58:41,295 - INFO - Batch: 50/141, Loss: 1.6915, Perplexity: 5.4278, Time: 0.24s
2025-04-25 23:58:41,540 - INFO - Batch: 60/141, Loss: 1.7144, Perplexity: 5.5532, Time: 0.24s
2025-04-25 23:58:41,810 - INFO - Batch: 70/141, Loss: 1.7040, Perplexity: 5.4961, Time: 0.27s
2025-04-25 23:58:42,049 - INFO - Batch: 80/141, Loss: 1.7132, Perplexity: 5.5466, Time: 0.24s
2025-04-25 23:58:42,284 - INFO - Batch: 90/141, Loss: 1.7186, Perplexity: 5.5767, Time: 0.23s
2025-04-25 23:58:42,515 - INFO - Batch: 100/141, Loss: 1.7044, Perplexity: 5.4983, Time: 0.23s
2025-04-25 23:58:42,812 - INFO - Batch: 110/141, Loss: 1.7062, Perplexity: 5.5080, Time: 0.30s
2025-04-25 23:58:43,079 - INFO - Batch: 120/141, Loss: 1.6966, Perplexity: 5.4553, Time: 0.27s
2025-04-25 23:58:43,367 - INFO - Batch: 130/141, Loss: 1.6915, Perplexity: 5.4278, Time: 0.29s
2025-04-25 23:58:43,627 - INFO - Batch: 140/141, Loss: 1.7055, Perplexity: 5.5039, Time: 0.26s
2025-04-25 23:58:44,517 - INFO - Validation  Loss: 1.8758, Perplexity: 6.5258, Time: 0.85s
2025-04-25 23:58:44,518 - INFO - Epoch 178/300 - Train Loss: 1.7067, Val Loss: 1.8758
2025-04-25 23:58:44,817 - INFO - Batch: 10/141, Loss: 1.6966, Perplexity: 5.4554, Time: 0.30s
2025-04-25 23:58:45,151 - INFO - Batch: 20/141, Loss: 1.7123, Perplexity: 5.5417, Time: 0.33s
2025-04-25 23:58:45,439 - INFO - Batch: 30/141, Loss: 1.7006, Perplexity: 5.4773, Time: 0.29s
2025-04-25 23:58:45,767 - INFO - Batch: 40/141, Loss: 1.6962, Perplexity: 5.4534, Time: 0.33s
2025-04-25 23:58:46,010 - INFO - Batch: 50/141, Loss: 1.7142, Perplexity: 5.5524, Time: 0.24s
2025-04-25 23:58:46,307 - INFO - Batch: 60/141, Loss: 1.7073, Perplexity: 5.5139, Time: 0.30s
2025-04-25 23:58:46,631 - INFO - Batch: 70/141, Loss: 1.7121, Perplexity: 5.5406, Time: 0.32s
2025-04-25 23:58:46,887 - INFO - Batch: 80/141, Loss: 1.7134, Perplexity: 5.5478, Time: 0.26s
2025-04-25 23:58:47,127 - INFO - Batch: 90/141, Loss: 1.6924, Perplexity: 5.4324, Time: 0.24s
2025-04-25 23:58:47,356 - INFO - Batch: 100/141, Loss: 1.7118, Perplexity: 5.5391, Time: 0.23s
2025-04-25 23:58:47,585 - INFO - Batch: 110/141, Loss: 1.6964, Perplexity: 5.4542, Time: 0.23s
2025-04-25 23:58:47,906 - INFO - Batch: 120/141, Loss: 1.7039, Perplexity: 5.4954, Time: 0.32s
2025-04-25 23:58:48,153 - INFO - Batch: 130/141, Loss: 1.7051, Perplexity: 5.5019, Time: 0.25s
2025-04-25 23:58:48,392 - INFO - Batch: 140/141, Loss: 1.6970, Perplexity: 5.4576, Time: 0.24s
2025-04-25 23:58:49,192 - INFO - Validation  Loss: 1.8763, Perplexity: 6.5295, Time: 0.77s
2025-04-25 23:58:49,192 - INFO - Epoch 179/300 - Train Loss: 1.7066, Val Loss: 1.8763
2025-04-25 23:58:49,435 - INFO - Batch: 10/141, Loss: 1.7019, Perplexity: 5.4843, Time: 0.24s
2025-04-25 23:58:49,706 - INFO - Batch: 20/141, Loss: 1.7050, Perplexity: 5.5012, Time: 0.27s
2025-04-25 23:58:49,935 - INFO - Batch: 30/141, Loss: 1.7061, Perplexity: 5.5073, Time: 0.23s
2025-04-25 23:58:50,240 - INFO - Batch: 40/141, Loss: 1.7021, Perplexity: 5.4856, Time: 0.30s
2025-04-25 23:58:50,467 - INFO - Batch: 50/141, Loss: 1.6959, Perplexity: 5.4516, Time: 0.23s
2025-04-25 23:58:50,690 - INFO - Batch: 60/141, Loss: 1.6990, Perplexity: 5.4684, Time: 0.22s
2025-04-25 23:58:50,933 - INFO - Batch: 70/141, Loss: 1.7012, Perplexity: 5.4804, Time: 0.24s
2025-04-25 23:58:51,174 - INFO - Batch: 80/141, Loss: 1.6920, Perplexity: 5.4303, Time: 0.24s
2025-04-25 23:58:51,401 - INFO - Batch: 90/141, Loss: 1.6976, Perplexity: 5.4610, Time: 0.23s
2025-04-25 23:58:51,628 - INFO - Batch: 100/141, Loss: 1.7153, Perplexity: 5.5582, Time: 0.23s
2025-04-25 23:58:51,858 - INFO - Batch: 110/141, Loss: 1.7032, Perplexity: 5.4915, Time: 0.23s
2025-04-25 23:58:52,138 - INFO - Batch: 120/141, Loss: 1.7101, Perplexity: 5.5294, Time: 0.28s
2025-04-25 23:58:52,361 - INFO - Batch: 130/141, Loss: 1.7128, Perplexity: 5.5445, Time: 0.22s
2025-04-25 23:58:52,579 - INFO - Batch: 140/141, Loss: 1.7177, Perplexity: 5.5716, Time: 0.22s
2025-04-25 23:58:53,257 - INFO - Validation  Loss: 1.8761, Perplexity: 6.5277, Time: 0.65s
2025-04-25 23:58:53,257 - INFO - Epoch 180/300 - Train Loss: 1.7065, Val Loss: 1.8761
2025-04-25 23:58:53,482 - INFO - Batch: 10/141, Loss: 1.7089, Perplexity: 5.5229, Time: 0.22s
2025-04-25 23:58:53,706 - INFO - Batch: 20/141, Loss: 1.7177, Perplexity: 5.5717, Time: 0.22s
2025-04-25 23:58:53,929 - INFO - Batch: 30/141, Loss: 1.7106, Perplexity: 5.5322, Time: 0.22s
2025-04-25 23:58:54,152 - INFO - Batch: 40/141, Loss: 1.6852, Perplexity: 5.3934, Time: 0.22s
2025-04-25 23:58:54,375 - INFO - Batch: 50/141, Loss: 1.6981, Perplexity: 5.4637, Time: 0.22s
2025-04-25 23:58:54,597 - INFO - Batch: 60/141, Loss: 1.7189, Perplexity: 5.5784, Time: 0.22s
2025-04-25 23:58:54,822 - INFO - Batch: 70/141, Loss: 1.7078, Perplexity: 5.5167, Time: 0.22s
2025-04-25 23:58:55,128 - INFO - Batch: 80/141, Loss: 1.7025, Perplexity: 5.4876, Time: 0.31s
2025-04-25 23:58:55,355 - INFO - Batch: 90/141, Loss: 1.7131, Perplexity: 5.5461, Time: 0.23s
2025-04-25 23:58:55,587 - INFO - Batch: 100/141, Loss: 1.7080, Perplexity: 5.5181, Time: 0.23s
2025-04-25 23:58:55,889 - INFO - Batch: 110/141, Loss: 1.7121, Perplexity: 5.5407, Time: 0.30s
2025-04-25 23:58:56,195 - INFO - Batch: 120/141, Loss: 1.7006, Perplexity: 5.4772, Time: 0.31s
2025-04-25 23:58:56,499 - INFO - Batch: 130/141, Loss: 1.6985, Perplexity: 5.4660, Time: 0.30s
2025-04-25 23:58:56,734 - INFO - Batch: 140/141, Loss: 1.6970, Perplexity: 5.4578, Time: 0.24s
2025-04-25 23:58:57,519 - INFO - Validation  Loss: 1.8764, Perplexity: 6.5299, Time: 0.75s
2025-04-25 23:58:57,519 - INFO - Epoch 181/300 - Train Loss: 1.7065, Val Loss: 1.8764
2025-04-25 23:58:57,751 - INFO - Batch: 10/141, Loss: 1.7193, Perplexity: 5.5804, Time: 0.23s
2025-04-25 23:58:57,975 - INFO - Batch: 20/141, Loss: 1.7069, Perplexity: 5.5117, Time: 0.22s
2025-04-25 23:58:58,206 - INFO - Batch: 30/141, Loss: 1.7048, Perplexity: 5.5003, Time: 0.23s
2025-04-25 23:58:58,437 - INFO - Batch: 40/141, Loss: 1.7087, Perplexity: 5.5216, Time: 0.23s
2025-04-25 23:58:58,723 - INFO - Batch: 50/141, Loss: 1.7101, Perplexity: 5.5296, Time: 0.29s
2025-04-25 23:58:58,970 - INFO - Batch: 60/141, Loss: 1.7115, Perplexity: 5.5372, Time: 0.25s
2025-04-25 23:58:59,243 - INFO - Batch: 70/141, Loss: 1.7163, Perplexity: 5.5641, Time: 0.27s
2025-04-25 23:58:59,479 - INFO - Batch: 80/141, Loss: 1.7130, Perplexity: 5.5453, Time: 0.24s
2025-04-25 23:58:59,806 - INFO - Batch: 90/141, Loss: 1.7189, Perplexity: 5.5781, Time: 0.33s
2025-04-25 23:59:00,054 - INFO - Batch: 100/141, Loss: 1.7071, Perplexity: 5.5130, Time: 0.25s
2025-04-25 23:59:00,300 - INFO - Batch: 110/141, Loss: 1.7084, Perplexity: 5.5200, Time: 0.25s
2025-04-25 23:59:00,585 - INFO - Batch: 120/141, Loss: 1.6986, Perplexity: 5.4664, Time: 0.28s
2025-04-25 23:59:00,816 - INFO - Batch: 130/141, Loss: 1.7091, Perplexity: 5.5239, Time: 0.23s
2025-04-25 23:59:01,050 - INFO - Batch: 140/141, Loss: 1.7035, Perplexity: 5.4931, Time: 0.23s
2025-04-25 23:59:01,701 - INFO - Validation  Loss: 1.8770, Perplexity: 6.5341, Time: 0.62s
2025-04-25 23:59:01,701 - INFO - Epoch 182/300 - Train Loss: 1.7064, Val Loss: 1.8770
2025-04-25 23:59:01,984 - INFO - Batch: 10/141, Loss: 1.7040, Perplexity: 5.4957, Time: 0.28s
2025-04-25 23:59:02,242 - INFO - Batch: 20/141, Loss: 1.7037, Perplexity: 5.4942, Time: 0.26s
2025-04-25 23:59:02,474 - INFO - Batch: 30/141, Loss: 1.7188, Perplexity: 5.5777, Time: 0.23s
2025-04-25 23:59:02,699 - INFO - Batch: 40/141, Loss: 1.7056, Perplexity: 5.5046, Time: 0.23s
2025-04-25 23:59:02,929 - INFO - Batch: 50/141, Loss: 1.6992, Perplexity: 5.4694, Time: 0.23s
2025-04-25 23:59:03,213 - INFO - Batch: 60/141, Loss: 1.7040, Perplexity: 5.4961, Time: 0.28s
2025-04-25 23:59:03,555 - INFO - Batch: 70/141, Loss: 1.7097, Perplexity: 5.5275, Time: 0.34s
2025-04-25 23:59:03,797 - INFO - Batch: 80/141, Loss: 1.7029, Perplexity: 5.4900, Time: 0.24s
2025-04-25 23:59:04,101 - INFO - Batch: 90/141, Loss: 1.7118, Perplexity: 5.5391, Time: 0.30s
2025-04-25 23:59:04,337 - INFO - Batch: 100/141, Loss: 1.7047, Perplexity: 5.4998, Time: 0.24s
2025-04-25 23:59:04,567 - INFO - Batch: 110/141, Loss: 1.6940, Perplexity: 5.4410, Time: 0.23s
2025-04-25 23:59:04,813 - INFO - Batch: 120/141, Loss: 1.7071, Perplexity: 5.5131, Time: 0.25s
2025-04-25 23:59:05,051 - INFO - Batch: 130/141, Loss: 1.7192, Perplexity: 5.5801, Time: 0.24s
2025-04-25 23:59:05,297 - INFO - Batch: 140/141, Loss: 1.7071, Perplexity: 5.5129, Time: 0.25s
2025-04-25 23:59:05,975 - INFO - Validation  Loss: 1.8761, Perplexity: 6.5280, Time: 0.64s
2025-04-25 23:59:05,975 - INFO - Epoch 183/300 - Train Loss: 1.7063, Val Loss: 1.8761
2025-04-25 23:59:06,217 - INFO - Batch: 10/141, Loss: 1.6968, Perplexity: 5.4567, Time: 0.24s
2025-04-25 23:59:06,463 - INFO - Batch: 20/141, Loss: 1.7073, Perplexity: 5.5141, Time: 0.25s
2025-04-25 23:59:06,691 - INFO - Batch: 30/141, Loss: 1.6862, Perplexity: 5.3989, Time: 0.23s
2025-04-25 23:59:06,916 - INFO - Batch: 40/141, Loss: 1.7068, Perplexity: 5.5115, Time: 0.22s
2025-04-25 23:59:07,184 - INFO - Batch: 50/141, Loss: 1.7057, Perplexity: 5.5055, Time: 0.27s
2025-04-25 23:59:07,510 - INFO - Batch: 60/141, Loss: 1.6854, Perplexity: 5.3947, Time: 0.33s
2025-04-25 23:59:07,735 - INFO - Batch: 70/141, Loss: 1.6926, Perplexity: 5.4335, Time: 0.22s
2025-04-25 23:59:08,009 - INFO - Batch: 80/141, Loss: 1.6954, Perplexity: 5.4487, Time: 0.27s
2025-04-25 23:59:08,232 - INFO - Batch: 90/141, Loss: 1.7138, Perplexity: 5.5499, Time: 0.22s
2025-04-25 23:59:08,556 - INFO - Batch: 100/141, Loss: 1.7131, Perplexity: 5.5459, Time: 0.32s
2025-04-25 23:59:08,807 - INFO - Batch: 110/141, Loss: 1.7060, Perplexity: 5.5070, Time: 0.25s
2025-04-25 23:59:09,065 - INFO - Batch: 120/141, Loss: 1.7100, Perplexity: 5.5288, Time: 0.26s
2025-04-25 23:59:09,391 - INFO - Batch: 130/141, Loss: 1.7077, Perplexity: 5.5163, Time: 0.33s
2025-04-25 23:59:09,610 - INFO - Batch: 140/141, Loss: 1.7074, Perplexity: 5.5149, Time: 0.22s
2025-04-25 23:59:10,218 - INFO - Validation  Loss: 1.8755, Perplexity: 6.5241, Time: 0.58s
2025-04-25 23:59:10,218 - INFO - Epoch 184/300 - Train Loss: 1.7063, Val Loss: 1.8755
2025-04-25 23:59:10,450 - INFO - Batch: 10/141, Loss: 1.6949, Perplexity: 5.4463, Time: 0.23s
2025-04-25 23:59:10,726 - INFO - Batch: 20/141, Loss: 1.7023, Perplexity: 5.4864, Time: 0.28s
2025-04-25 23:59:11,025 - INFO - Batch: 30/141, Loss: 1.7054, Perplexity: 5.5037, Time: 0.30s
2025-04-25 23:59:11,274 - INFO - Batch: 40/141, Loss: 1.7033, Perplexity: 5.4923, Time: 0.25s
2025-04-25 23:59:11,575 - INFO - Batch: 50/141, Loss: 1.7137, Perplexity: 5.5492, Time: 0.30s
2025-04-25 23:59:11,882 - INFO - Batch: 60/141, Loss: 1.7003, Perplexity: 5.4754, Time: 0.31s
2025-04-25 23:59:12,101 - INFO - Batch: 70/141, Loss: 1.7146, Perplexity: 5.5546, Time: 0.22s
2025-04-25 23:59:12,323 - INFO - Batch: 80/141, Loss: 1.7045, Perplexity: 5.4988, Time: 0.22s
2025-04-25 23:59:12,558 - INFO - Batch: 90/141, Loss: 1.7127, Perplexity: 5.5440, Time: 0.23s
2025-04-25 23:59:12,873 - INFO - Batch: 100/141, Loss: 1.7123, Perplexity: 5.5416, Time: 0.31s
2025-04-25 23:59:13,100 - INFO - Batch: 110/141, Loss: 1.7067, Perplexity: 5.5105, Time: 0.23s
2025-04-25 23:59:13,340 - INFO - Batch: 120/141, Loss: 1.7010, Perplexity: 5.4793, Time: 0.24s
2025-04-25 23:59:13,563 - INFO - Batch: 130/141, Loss: 1.7152, Perplexity: 5.5580, Time: 0.22s
2025-04-25 23:59:13,786 - INFO - Batch: 140/141, Loss: 1.7101, Perplexity: 5.5296, Time: 0.22s
2025-04-25 23:59:14,475 - INFO - Validation  Loss: 1.8759, Perplexity: 6.5265, Time: 0.66s
2025-04-25 23:59:14,475 - INFO - Epoch 185/300 - Train Loss: 1.7062, Val Loss: 1.8759
2025-04-25 23:59:14,709 - INFO - Batch: 10/141, Loss: 1.7033, Perplexity: 5.4921, Time: 0.23s
2025-04-25 23:59:14,935 - INFO - Batch: 20/141, Loss: 1.7271, Perplexity: 5.6246, Time: 0.23s
2025-04-25 23:59:15,175 - INFO - Batch: 30/141, Loss: 1.7039, Perplexity: 5.4956, Time: 0.24s
2025-04-25 23:59:15,406 - INFO - Batch: 40/141, Loss: 1.6902, Perplexity: 5.4207, Time: 0.23s
2025-04-25 23:59:15,645 - INFO - Batch: 50/141, Loss: 1.6899, Perplexity: 5.4187, Time: 0.24s
2025-04-25 23:59:15,878 - INFO - Batch: 60/141, Loss: 1.6893, Perplexity: 5.4157, Time: 0.23s
2025-04-25 23:59:16,261 - INFO - Batch: 70/141, Loss: 1.7192, Perplexity: 5.5802, Time: 0.38s
2025-04-25 23:59:16,488 - INFO - Batch: 80/141, Loss: 1.7150, Perplexity: 5.5568, Time: 0.23s
2025-04-25 23:59:16,745 - INFO - Batch: 90/141, Loss: 1.7005, Perplexity: 5.4765, Time: 0.26s
2025-04-25 23:59:17,021 - INFO - Batch: 100/141, Loss: 1.7063, Perplexity: 5.5085, Time: 0.28s
2025-04-25 23:59:17,287 - INFO - Batch: 110/141, Loss: 1.6932, Perplexity: 5.4368, Time: 0.27s
2025-04-25 23:59:17,524 - INFO - Batch: 120/141, Loss: 1.7078, Perplexity: 5.5167, Time: 0.24s
2025-04-25 23:59:17,750 - INFO - Batch: 130/141, Loss: 1.6943, Perplexity: 5.4431, Time: 0.23s
2025-04-25 23:59:18,044 - INFO - Batch: 140/141, Loss: 1.7056, Perplexity: 5.5048, Time: 0.29s
2025-04-25 23:59:18,662 - INFO - Validation  Loss: 1.8753, Perplexity: 6.5231, Time: 0.59s
2025-04-25 23:59:18,662 - INFO - Epoch 186/300 - Train Loss: 1.7060, Val Loss: 1.8753
2025-04-25 23:59:18,887 - INFO - Batch: 10/141, Loss: 1.7102, Perplexity: 5.5301, Time: 0.23s
2025-04-25 23:59:19,108 - INFO - Batch: 20/141, Loss: 1.7109, Perplexity: 5.5340, Time: 0.22s
2025-04-25 23:59:19,393 - INFO - Batch: 30/141, Loss: 1.7230, Perplexity: 5.6011, Time: 0.28s
2025-04-25 23:59:19,615 - INFO - Batch: 40/141, Loss: 1.7059, Perplexity: 5.5066, Time: 0.22s
2025-04-25 23:59:19,839 - INFO - Batch: 50/141, Loss: 1.7138, Perplexity: 5.5498, Time: 0.22s
2025-04-25 23:59:20,063 - INFO - Batch: 60/141, Loss: 1.6883, Perplexity: 5.4105, Time: 0.22s
2025-04-25 23:59:20,291 - INFO - Batch: 70/141, Loss: 1.7024, Perplexity: 5.4868, Time: 0.23s
2025-04-25 23:59:20,522 - INFO - Batch: 80/141, Loss: 1.7047, Perplexity: 5.4999, Time: 0.23s
2025-04-25 23:59:20,751 - INFO - Batch: 90/141, Loss: 1.7163, Perplexity: 5.5640, Time: 0.23s
2025-04-25 23:59:20,973 - INFO - Batch: 100/141, Loss: 1.7056, Perplexity: 5.5047, Time: 0.22s
2025-04-25 23:59:21,257 - INFO - Batch: 110/141, Loss: 1.6947, Perplexity: 5.4449, Time: 0.28s
2025-04-25 23:59:21,487 - INFO - Batch: 120/141, Loss: 1.7132, Perplexity: 5.5469, Time: 0.23s
2025-04-25 23:59:21,727 - INFO - Batch: 130/141, Loss: 1.7090, Perplexity: 5.5232, Time: 0.24s
2025-04-25 23:59:21,954 - INFO - Batch: 140/141, Loss: 1.6993, Perplexity: 5.4700, Time: 0.23s
2025-04-25 23:59:22,681 - INFO - Validation  Loss: 1.8762, Perplexity: 6.5286, Time: 0.69s
2025-04-25 23:59:22,681 - INFO - Epoch 187/300 - Train Loss: 1.7060, Val Loss: 1.8762
2025-04-25 23:59:22,979 - INFO - Batch: 10/141, Loss: 1.7103, Perplexity: 5.5304, Time: 0.30s
2025-04-25 23:59:23,259 - INFO - Batch: 20/141, Loss: 1.7124, Perplexity: 5.5422, Time: 0.28s
2025-04-25 23:59:23,495 - INFO - Batch: 30/141, Loss: 1.7030, Perplexity: 5.4902, Time: 0.24s
2025-04-25 23:59:23,733 - INFO - Batch: 40/141, Loss: 1.7095, Perplexity: 5.5264, Time: 0.24s
2025-04-25 23:59:23,953 - INFO - Batch: 50/141, Loss: 1.7102, Perplexity: 5.5298, Time: 0.22s
2025-04-25 23:59:24,182 - INFO - Batch: 60/141, Loss: 1.7136, Perplexity: 5.5491, Time: 0.23s
2025-04-25 23:59:24,552 - INFO - Batch: 70/141, Loss: 1.7087, Perplexity: 5.5220, Time: 0.37s
2025-04-25 23:59:24,770 - INFO - Batch: 80/141, Loss: 1.7033, Perplexity: 5.4921, Time: 0.22s
2025-04-25 23:59:24,986 - INFO - Batch: 90/141, Loss: 1.7022, Perplexity: 5.4862, Time: 0.22s
2025-04-25 23:59:25,221 - INFO - Batch: 100/141, Loss: 1.7190, Perplexity: 5.5792, Time: 0.23s
2025-04-25 23:59:25,439 - INFO - Batch: 110/141, Loss: 1.6977, Perplexity: 5.4615, Time: 0.22s
2025-04-25 23:59:25,662 - INFO - Batch: 120/141, Loss: 1.6962, Perplexity: 5.4531, Time: 0.22s
2025-04-25 23:59:25,905 - INFO - Batch: 130/141, Loss: 1.6980, Perplexity: 5.4633, Time: 0.24s
2025-04-25 23:59:26,143 - INFO - Batch: 140/141, Loss: 1.7121, Perplexity: 5.5404, Time: 0.24s
2025-04-25 23:59:26,810 - INFO - Validation  Loss: 1.8755, Perplexity: 6.5238, Time: 0.64s
2025-04-25 23:59:26,810 - INFO - Epoch 188/300 - Train Loss: 1.7060, Val Loss: 1.8755
2025-04-25 23:59:27,040 - INFO - Batch: 10/141, Loss: 1.7048, Perplexity: 5.5004, Time: 0.23s
2025-04-25 23:59:27,266 - INFO - Batch: 20/141, Loss: 1.7040, Perplexity: 5.4960, Time: 0.23s
2025-04-25 23:59:27,545 - INFO - Batch: 30/141, Loss: 1.6971, Perplexity: 5.4581, Time: 0.28s
2025-04-25 23:59:27,859 - INFO - Batch: 40/141, Loss: 1.7094, Perplexity: 5.5258, Time: 0.31s
2025-04-25 23:59:28,240 - INFO - Batch: 50/141, Loss: 1.7083, Perplexity: 5.5195, Time: 0.38s
2025-04-25 23:59:28,553 - INFO - Batch: 60/141, Loss: 1.6962, Perplexity: 5.4533, Time: 0.31s
2025-04-25 23:59:28,871 - INFO - Batch: 70/141, Loss: 1.7092, Perplexity: 5.5245, Time: 0.32s
2025-04-25 23:59:29,143 - INFO - Batch: 80/141, Loss: 1.7069, Perplexity: 5.5117, Time: 0.27s
2025-04-25 23:59:29,409 - INFO - Batch: 90/141, Loss: 1.7035, Perplexity: 5.4931, Time: 0.27s
2025-04-25 23:59:29,641 - INFO - Batch: 100/141, Loss: 1.6966, Perplexity: 5.4554, Time: 0.23s
2025-04-25 23:59:29,899 - INFO - Batch: 110/141, Loss: 1.6918, Perplexity: 5.4295, Time: 0.26s
2025-04-25 23:59:30,208 - INFO - Batch: 120/141, Loss: 1.7095, Perplexity: 5.5264, Time: 0.31s
2025-04-25 23:59:30,432 - INFO - Batch: 130/141, Loss: 1.7089, Perplexity: 5.5226, Time: 0.22s
2025-04-25 23:59:30,665 - INFO - Batch: 140/141, Loss: 1.7007, Perplexity: 5.4778, Time: 0.23s
2025-04-25 23:59:31,310 - INFO - Validation  Loss: 1.8760, Perplexity: 6.5273, Time: 0.61s
2025-04-25 23:59:31,310 - INFO - Epoch 189/300 - Train Loss: 1.7058, Val Loss: 1.8760
2025-04-25 23:59:31,601 - INFO - Batch: 10/141, Loss: 1.7101, Perplexity: 5.5294, Time: 0.29s
2025-04-25 23:59:31,816 - INFO - Batch: 20/141, Loss: 1.7168, Perplexity: 5.5664, Time: 0.21s
2025-04-25 23:59:32,034 - INFO - Batch: 30/141, Loss: 1.7029, Perplexity: 5.4897, Time: 0.22s
2025-04-25 23:59:32,257 - INFO - Batch: 40/141, Loss: 1.6957, Perplexity: 5.4505, Time: 0.22s
2025-04-25 23:59:32,484 - INFO - Batch: 50/141, Loss: 1.7064, Perplexity: 5.5088, Time: 0.23s
2025-04-25 23:59:32,723 - INFO - Batch: 60/141, Loss: 1.7102, Perplexity: 5.5301, Time: 0.24s
2025-04-25 23:59:32,949 - INFO - Batch: 70/141, Loss: 1.7043, Perplexity: 5.4977, Time: 0.23s
2025-04-25 23:59:33,233 - INFO - Batch: 80/141, Loss: 1.7123, Perplexity: 5.5418, Time: 0.28s
2025-04-25 23:59:33,484 - INFO - Batch: 90/141, Loss: 1.6989, Perplexity: 5.4681, Time: 0.25s
2025-04-25 23:59:33,707 - INFO - Batch: 100/141, Loss: 1.6956, Perplexity: 5.4497, Time: 0.22s
2025-04-25 23:59:33,928 - INFO - Batch: 110/141, Loss: 1.6899, Perplexity: 5.4191, Time: 0.22s
2025-04-25 23:59:34,149 - INFO - Batch: 120/141, Loss: 1.7169, Perplexity: 5.5675, Time: 0.22s
2025-04-25 23:59:34,367 - INFO - Batch: 130/141, Loss: 1.6977, Perplexity: 5.4612, Time: 0.22s
2025-04-25 23:59:34,585 - INFO - Batch: 140/141, Loss: 1.7056, Perplexity: 5.5048, Time: 0.22s
2025-04-25 23:59:35,315 - INFO - Validation  Loss: 1.8773, Perplexity: 6.5356, Time: 0.70s
2025-04-25 23:59:35,316 - INFO - Epoch 190/300 - Train Loss: 1.7058, Val Loss: 1.8773
2025-04-25 23:59:35,555 - INFO - Batch: 10/141, Loss: 1.7143, Perplexity: 5.5527, Time: 0.24s
2025-04-25 23:59:35,789 - INFO - Batch: 20/141, Loss: 1.7106, Perplexity: 5.5325, Time: 0.23s
2025-04-25 23:59:36,013 - INFO - Batch: 30/141, Loss: 1.7009, Perplexity: 5.4786, Time: 0.22s
2025-04-25 23:59:36,240 - INFO - Batch: 40/141, Loss: 1.6934, Perplexity: 5.4377, Time: 0.23s
2025-04-25 23:59:36,532 - INFO - Batch: 50/141, Loss: 1.6990, Perplexity: 5.4685, Time: 0.29s
2025-04-25 23:59:36,756 - INFO - Batch: 60/141, Loss: 1.7042, Perplexity: 5.4969, Time: 0.22s
2025-04-25 23:59:36,980 - INFO - Batch: 70/141, Loss: 1.7027, Perplexity: 5.4887, Time: 0.22s
2025-04-25 23:59:37,226 - INFO - Batch: 80/141, Loss: 1.7090, Perplexity: 5.5235, Time: 0.25s
2025-04-25 23:59:37,530 - INFO - Batch: 90/141, Loss: 1.7063, Perplexity: 5.5084, Time: 0.30s
2025-04-25 23:59:37,754 - INFO - Batch: 100/141, Loss: 1.7203, Perplexity: 5.5862, Time: 0.22s
2025-04-25 23:59:37,970 - INFO - Batch: 110/141, Loss: 1.7003, Perplexity: 5.4755, Time: 0.22s
2025-04-25 23:59:38,196 - INFO - Batch: 120/141, Loss: 1.7079, Perplexity: 5.5173, Time: 0.23s
2025-04-25 23:59:38,472 - INFO - Batch: 130/141, Loss: 1.6982, Perplexity: 5.4643, Time: 0.28s
2025-04-25 23:59:38,726 - INFO - Batch: 140/141, Loss: 1.7060, Perplexity: 5.5068, Time: 0.25s
2025-04-25 23:59:39,319 - INFO - Validation  Loss: 1.8766, Perplexity: 6.5309, Time: 0.56s
2025-04-25 23:59:39,319 - INFO - Epoch 191/300 - Train Loss: 1.7057, Val Loss: 1.8766
2025-04-25 23:59:39,534 - INFO - Batch: 10/141, Loss: 1.6965, Perplexity: 5.4549, Time: 0.21s
2025-04-25 23:59:39,813 - INFO - Batch: 20/141, Loss: 1.7050, Perplexity: 5.5016, Time: 0.28s
2025-04-25 23:59:40,025 - INFO - Batch: 30/141, Loss: 1.7072, Perplexity: 5.5133, Time: 0.21s
2025-04-25 23:59:40,244 - INFO - Batch: 40/141, Loss: 1.7139, Perplexity: 5.5505, Time: 0.22s
2025-04-25 23:59:40,470 - INFO - Batch: 50/141, Loss: 1.7031, Perplexity: 5.4909, Time: 0.23s
2025-04-25 23:59:40,685 - INFO - Batch: 60/141, Loss: 1.6997, Perplexity: 5.4723, Time: 0.21s
2025-04-25 23:59:40,903 - INFO - Batch: 70/141, Loss: 1.7158, Perplexity: 5.5612, Time: 0.22s
2025-04-25 23:59:41,122 - INFO - Batch: 80/141, Loss: 1.6964, Perplexity: 5.4543, Time: 0.22s
2025-04-25 23:59:41,403 - INFO - Batch: 90/141, Loss: 1.7033, Perplexity: 5.4919, Time: 0.28s
2025-04-25 23:59:41,641 - INFO - Batch: 100/141, Loss: 1.7183, Perplexity: 5.5753, Time: 0.24s
2025-04-25 23:59:41,875 - INFO - Batch: 110/141, Loss: 1.7048, Perplexity: 5.5005, Time: 0.23s
2025-04-25 23:59:42,096 - INFO - Batch: 120/141, Loss: 1.7082, Perplexity: 5.5189, Time: 0.22s
2025-04-25 23:59:42,316 - INFO - Batch: 130/141, Loss: 1.7182, Perplexity: 5.5744, Time: 0.22s
2025-04-25 23:59:42,532 - INFO - Batch: 140/141, Loss: 1.6937, Perplexity: 5.4393, Time: 0.22s
2025-04-25 23:59:43,199 - INFO - Validation  Loss: 1.8762, Perplexity: 6.5289, Time: 0.64s
2025-04-25 23:59:43,199 - INFO - Epoch 192/300 - Train Loss: 1.7056, Val Loss: 1.8762
2025-04-25 23:59:43,413 - INFO - Batch: 10/141, Loss: 1.7082, Perplexity: 5.5188, Time: 0.21s
2025-04-25 23:59:43,641 - INFO - Batch: 20/141, Loss: 1.7032, Perplexity: 5.4918, Time: 0.23s
2025-04-25 23:59:43,919 - INFO - Batch: 30/141, Loss: 1.7146, Perplexity: 5.5543, Time: 0.28s
2025-04-25 23:59:44,163 - INFO - Batch: 40/141, Loss: 1.6975, Perplexity: 5.4601, Time: 0.24s
2025-04-25 23:59:44,391 - INFO - Batch: 50/141, Loss: 1.7090, Perplexity: 5.5236, Time: 0.23s
2025-04-25 23:59:44,674 - INFO - Batch: 60/141, Loss: 1.7111, Perplexity: 5.5349, Time: 0.28s
2025-04-25 23:59:44,894 - INFO - Batch: 70/141, Loss: 1.6991, Perplexity: 5.4693, Time: 0.22s
2025-04-25 23:59:45,151 - INFO - Batch: 80/141, Loss: 1.7023, Perplexity: 5.4864, Time: 0.26s
2025-04-25 23:59:45,430 - INFO - Batch: 90/141, Loss: 1.6945, Perplexity: 5.4441, Time: 0.28s
2025-04-25 23:59:45,762 - INFO - Batch: 100/141, Loss: 1.7085, Perplexity: 5.5205, Time: 0.33s
2025-04-25 23:59:46,004 - INFO - Batch: 110/141, Loss: 1.7117, Perplexity: 5.5384, Time: 0.24s
2025-04-25 23:59:46,237 - INFO - Batch: 120/141, Loss: 1.7026, Perplexity: 5.4882, Time: 0.23s
2025-04-25 23:59:46,466 - INFO - Batch: 130/141, Loss: 1.7128, Perplexity: 5.5445, Time: 0.23s
2025-04-25 23:59:46,801 - INFO - Batch: 140/141, Loss: 1.7082, Perplexity: 5.5189, Time: 0.33s
2025-04-25 23:59:47,434 - INFO - Validation  Loss: 1.8758, Perplexity: 6.5261, Time: 0.60s
2025-04-25 23:59:47,434 - INFO - Epoch 193/300 - Train Loss: 1.7055, Val Loss: 1.8758
2025-04-25 23:59:47,717 - INFO - Batch: 10/141, Loss: 1.7211, Perplexity: 5.5905, Time: 0.28s
2025-04-25 23:59:48,038 - INFO - Batch: 20/141, Loss: 1.7044, Perplexity: 5.4980, Time: 0.32s
2025-04-25 23:59:48,289 - INFO - Batch: 30/141, Loss: 1.7061, Perplexity: 5.5072, Time: 0.25s
2025-04-25 23:59:48,526 - INFO - Batch: 40/141, Loss: 1.6979, Perplexity: 5.4626, Time: 0.24s
2025-04-25 23:59:48,763 - INFO - Batch: 50/141, Loss: 1.7090, Perplexity: 5.5236, Time: 0.24s
2025-04-25 23:59:49,000 - INFO - Batch: 60/141, Loss: 1.7034, Perplexity: 5.4924, Time: 0.24s
2025-04-25 23:59:49,232 - INFO - Batch: 70/141, Loss: 1.7075, Perplexity: 5.5152, Time: 0.23s
2025-04-25 23:59:49,485 - INFO - Batch: 80/141, Loss: 1.7193, Perplexity: 5.5805, Time: 0.25s
2025-04-25 23:59:49,753 - INFO - Batch: 90/141, Loss: 1.7070, Perplexity: 5.5125, Time: 0.27s
2025-04-25 23:59:50,082 - INFO - Batch: 100/141, Loss: 1.6936, Perplexity: 5.4392, Time: 0.33s
2025-04-25 23:59:50,358 - INFO - Batch: 110/141, Loss: 1.7103, Perplexity: 5.5308, Time: 0.28s
2025-04-25 23:59:50,628 - INFO - Batch: 120/141, Loss: 1.7075, Perplexity: 5.5153, Time: 0.27s
2025-04-25 23:59:50,919 - INFO - Batch: 130/141, Loss: 1.7096, Perplexity: 5.5268, Time: 0.29s
2025-04-25 23:59:51,194 - INFO - Batch: 140/141, Loss: 1.6948, Perplexity: 5.4457, Time: 0.28s
2025-04-25 23:59:51,898 - INFO - Validation  Loss: 1.8755, Perplexity: 6.5238, Time: 0.68s
2025-04-25 23:59:51,898 - INFO - Epoch 194/300 - Train Loss: 1.7054, Val Loss: 1.8755
2025-04-25 23:59:52,222 - INFO - Batch: 10/141, Loss: 1.7087, Perplexity: 5.5215, Time: 0.32s
2025-04-25 23:59:52,486 - INFO - Batch: 20/141, Loss: 1.6963, Perplexity: 5.4539, Time: 0.26s
2025-04-25 23:59:52,712 - INFO - Batch: 30/141, Loss: 1.6989, Perplexity: 5.4679, Time: 0.23s
2025-04-25 23:59:52,950 - INFO - Batch: 40/141, Loss: 1.7049, Perplexity: 5.5008, Time: 0.24s
2025-04-25 23:59:53,221 - INFO - Batch: 50/141, Loss: 1.7024, Perplexity: 5.4869, Time: 0.27s
2025-04-25 23:59:53,524 - INFO - Batch: 60/141, Loss: 1.7115, Perplexity: 5.5371, Time: 0.30s
2025-04-25 23:59:53,909 - INFO - Batch: 70/141, Loss: 1.7099, Perplexity: 5.5286, Time: 0.39s
2025-04-25 23:59:54,171 - INFO - Batch: 80/141, Loss: 1.6995, Perplexity: 5.4710, Time: 0.26s
2025-04-25 23:59:54,475 - INFO - Batch: 90/141, Loss: 1.7112, Perplexity: 5.5355, Time: 0.30s
2025-04-25 23:59:54,725 - INFO - Batch: 100/141, Loss: 1.7061, Perplexity: 5.5076, Time: 0.25s
2025-04-25 23:59:54,979 - INFO - Batch: 110/141, Loss: 1.7091, Perplexity: 5.5240, Time: 0.25s
2025-04-25 23:59:55,238 - INFO - Batch: 120/141, Loss: 1.6948, Perplexity: 5.4457, Time: 0.26s
2025-04-25 23:59:55,463 - INFO - Batch: 130/141, Loss: 1.7146, Perplexity: 5.5543, Time: 0.22s
2025-04-25 23:59:55,788 - INFO - Batch: 140/141, Loss: 1.7005, Perplexity: 5.4767, Time: 0.32s
2025-04-25 23:59:56,534 - INFO - Validation  Loss: 1.8757, Perplexity: 6.5255, Time: 0.71s
2025-04-25 23:59:56,534 - INFO - Epoch 195/300 - Train Loss: 1.7053, Val Loss: 1.8757
2025-04-25 23:59:56,854 - INFO - Batch: 10/141, Loss: 1.6924, Perplexity: 5.4327, Time: 0.32s
2025-04-25 23:59:57,144 - INFO - Batch: 20/141, Loss: 1.6904, Perplexity: 5.4215, Time: 0.29s
2025-04-25 23:59:57,445 - INFO - Batch: 30/141, Loss: 1.6928, Perplexity: 5.4348, Time: 0.30s
2025-04-25 23:59:57,671 - INFO - Batch: 40/141, Loss: 1.7099, Perplexity: 5.5283, Time: 0.23s
2025-04-25 23:59:57,902 - INFO - Batch: 50/141, Loss: 1.7131, Perplexity: 5.5461, Time: 0.23s
2025-04-25 23:59:58,133 - INFO - Batch: 60/141, Loss: 1.7073, Perplexity: 5.5141, Time: 0.23s
2025-04-25 23:59:58,356 - INFO - Batch: 70/141, Loss: 1.7034, Perplexity: 5.4928, Time: 0.22s
2025-04-25 23:59:58,599 - INFO - Batch: 80/141, Loss: 1.7058, Perplexity: 5.5057, Time: 0.24s
2025-04-25 23:59:58,855 - INFO - Batch: 90/141, Loss: 1.6926, Perplexity: 5.4333, Time: 0.26s
2025-04-25 23:59:59,130 - INFO - Batch: 100/141, Loss: 1.7140, Perplexity: 5.5509, Time: 0.27s
2025-04-25 23:59:59,464 - INFO - Batch: 110/141, Loss: 1.7057, Perplexity: 5.5053, Time: 0.33s
2025-04-25 23:59:59,679 - INFO - Batch: 120/141, Loss: 1.7078, Perplexity: 5.5170, Time: 0.22s
2025-04-25 23:59:59,898 - INFO - Batch: 130/141, Loss: 1.7043, Perplexity: 5.4976, Time: 0.22s
2025-04-26 00:00:00,127 - INFO - Batch: 140/141, Loss: 1.6935, Perplexity: 5.4382, Time: 0.23s
2025-04-26 00:00:00,970 - INFO - Validation  Loss: 1.8763, Perplexity: 6.5293, Time: 0.82s
2025-04-26 00:00:00,970 - INFO - Epoch 196/300 - Train Loss: 1.7052, Val Loss: 1.8763
2025-04-26 00:00:01,345 - INFO - Batch: 10/141, Loss: 1.7163, Perplexity: 5.5641, Time: 0.37s
2025-04-26 00:00:01,583 - INFO - Batch: 20/141, Loss: 1.7010, Perplexity: 5.4794, Time: 0.24s
2025-04-26 00:00:01,826 - INFO - Batch: 30/141, Loss: 1.7127, Perplexity: 5.5439, Time: 0.24s
2025-04-26 00:00:02,103 - INFO - Batch: 40/141, Loss: 1.6939, Perplexity: 5.4406, Time: 0.28s
2025-04-26 00:00:02,369 - INFO - Batch: 50/141, Loss: 1.7017, Perplexity: 5.4834, Time: 0.27s
2025-04-26 00:00:02,627 - INFO - Batch: 60/141, Loss: 1.7031, Perplexity: 5.4911, Time: 0.26s
2025-04-26 00:00:02,938 - INFO - Batch: 70/141, Loss: 1.7088, Perplexity: 5.5222, Time: 0.31s
2025-04-26 00:00:03,423 - INFO - Batch: 80/141, Loss: 1.7068, Perplexity: 5.5113, Time: 0.48s
2025-04-26 00:00:03,687 - INFO - Batch: 90/141, Loss: 1.7127, Perplexity: 5.5438, Time: 0.26s
2025-04-26 00:00:03,935 - INFO - Batch: 100/141, Loss: 1.7163, Perplexity: 5.5636, Time: 0.25s
2025-04-26 00:00:04,335 - INFO - Batch: 110/141, Loss: 1.7043, Perplexity: 5.4977, Time: 0.40s
2025-04-26 00:00:04,595 - INFO - Batch: 120/141, Loss: 1.7032, Perplexity: 5.4914, Time: 0.26s
2025-04-26 00:00:04,819 - INFO - Batch: 130/141, Loss: 1.7087, Perplexity: 5.5217, Time: 0.22s
2025-04-26 00:00:05,089 - INFO - Batch: 140/141, Loss: 1.7100, Perplexity: 5.5292, Time: 0.27s
2025-04-26 00:00:05,931 - INFO - Validation  Loss: 1.8763, Perplexity: 6.5294, Time: 0.80s
2025-04-26 00:00:05,931 - INFO - Epoch 197/300 - Train Loss: 1.7052, Val Loss: 1.8763
2025-04-26 00:00:06,180 - INFO - Batch: 10/141, Loss: 1.6896, Perplexity: 5.4174, Time: 0.25s
2025-04-26 00:00:06,424 - INFO - Batch: 20/141, Loss: 1.6999, Perplexity: 5.4734, Time: 0.24s
2025-04-26 00:00:06,672 - INFO - Batch: 30/141, Loss: 1.6883, Perplexity: 5.4102, Time: 0.25s
2025-04-26 00:00:06,976 - INFO - Batch: 40/141, Loss: 1.6923, Perplexity: 5.4322, Time: 0.30s
2025-04-26 00:00:07,211 - INFO - Batch: 50/141, Loss: 1.7183, Perplexity: 5.5752, Time: 0.23s
2025-04-26 00:00:07,451 - INFO - Batch: 60/141, Loss: 1.6936, Perplexity: 5.4391, Time: 0.24s
2025-04-26 00:00:07,694 - INFO - Batch: 70/141, Loss: 1.7045, Perplexity: 5.4985, Time: 0.24s
2025-04-26 00:00:07,928 - INFO - Batch: 80/141, Loss: 1.6933, Perplexity: 5.4371, Time: 0.23s
2025-04-26 00:00:08,177 - INFO - Batch: 90/141, Loss: 1.6975, Perplexity: 5.4605, Time: 0.25s
2025-04-26 00:00:08,412 - INFO - Batch: 100/141, Loss: 1.7005, Perplexity: 5.4764, Time: 0.23s
2025-04-26 00:00:08,648 - INFO - Batch: 110/141, Loss: 1.7041, Perplexity: 5.4965, Time: 0.24s
2025-04-26 00:00:08,971 - INFO - Batch: 120/141, Loss: 1.6905, Perplexity: 5.4222, Time: 0.32s
2025-04-26 00:00:09,358 - INFO - Batch: 130/141, Loss: 1.6976, Perplexity: 5.4607, Time: 0.39s
2025-04-26 00:00:09,687 - INFO - Batch: 140/141, Loss: 1.7101, Perplexity: 5.5295, Time: 0.33s
2025-04-26 00:00:10,573 - INFO - Validation  Loss: 1.8760, Perplexity: 6.5274, Time: 0.83s
2025-04-26 00:00:10,573 - INFO - Epoch 198/300 - Train Loss: 1.7051, Val Loss: 1.8760
2025-04-26 00:00:10,957 - INFO - Batch: 10/141, Loss: 1.6925, Perplexity: 5.4332, Time: 0.38s
2025-04-26 00:00:11,325 - INFO - Batch: 20/141, Loss: 1.7075, Perplexity: 5.5150, Time: 0.37s
2025-04-26 00:00:11,676 - INFO - Batch: 30/141, Loss: 1.7028, Perplexity: 5.4891, Time: 0.35s
2025-04-26 00:00:11,992 - INFO - Batch: 40/141, Loss: 1.7063, Perplexity: 5.5086, Time: 0.32s
2025-04-26 00:00:12,307 - INFO - Batch: 50/141, Loss: 1.7028, Perplexity: 5.4891, Time: 0.31s
2025-04-26 00:00:12,599 - INFO - Batch: 60/141, Loss: 1.7127, Perplexity: 5.5442, Time: 0.29s
2025-04-26 00:00:12,938 - INFO - Batch: 70/141, Loss: 1.6959, Perplexity: 5.4518, Time: 0.34s
2025-04-26 00:00:13,240 - INFO - Batch: 80/141, Loss: 1.7069, Perplexity: 5.5121, Time: 0.30s
2025-04-26 00:00:13,634 - INFO - Batch: 90/141, Loss: 1.7039, Perplexity: 5.4956, Time: 0.39s
2025-04-26 00:00:13,930 - INFO - Batch: 100/141, Loss: 1.7048, Perplexity: 5.5005, Time: 0.30s
2025-04-26 00:00:14,224 - INFO - Batch: 110/141, Loss: 1.7003, Perplexity: 5.4756, Time: 0.29s
2025-04-26 00:00:14,504 - INFO - Batch: 120/141, Loss: 1.6969, Perplexity: 5.4573, Time: 0.28s
2025-04-26 00:00:14,794 - INFO - Batch: 130/141, Loss: 1.7108, Perplexity: 5.5337, Time: 0.29s
2025-04-26 00:00:15,066 - INFO - Batch: 140/141, Loss: 1.7061, Perplexity: 5.5073, Time: 0.27s
2025-04-26 00:00:15,990 - INFO - Validation  Loss: 1.8757, Perplexity: 6.5254, Time: 0.89s
2025-04-26 00:00:15,990 - INFO - Epoch 199/300 - Train Loss: 1.7050, Val Loss: 1.8757
2025-04-26 00:00:16,283 - INFO - Batch: 10/141, Loss: 1.7064, Perplexity: 5.5091, Time: 0.29s
2025-04-26 00:00:16,559 - INFO - Batch: 20/141, Loss: 1.7048, Perplexity: 5.5004, Time: 0.28s
2025-04-26 00:00:16,866 - INFO - Batch: 30/141, Loss: 1.6920, Perplexity: 5.4303, Time: 0.31s
2025-04-26 00:00:17,167 - INFO - Batch: 40/141, Loss: 1.7118, Perplexity: 5.5389, Time: 0.30s
2025-04-26 00:00:17,520 - INFO - Batch: 50/141, Loss: 1.7210, Perplexity: 5.5903, Time: 0.35s
2025-04-26 00:00:17,799 - INFO - Batch: 60/141, Loss: 1.7081, Perplexity: 5.5187, Time: 0.28s
2025-04-26 00:00:18,088 - INFO - Batch: 70/141, Loss: 1.7039, Perplexity: 5.4951, Time: 0.29s
2025-04-26 00:00:18,398 - INFO - Batch: 80/141, Loss: 1.7036, Perplexity: 5.4934, Time: 0.31s
2025-04-26 00:00:18,679 - INFO - Batch: 90/141, Loss: 1.6998, Perplexity: 5.4727, Time: 0.28s
2025-04-26 00:00:18,954 - INFO - Batch: 100/141, Loss: 1.6853, Perplexity: 5.3942, Time: 0.27s
2025-04-26 00:00:19,220 - INFO - Batch: 110/141, Loss: 1.7100, Perplexity: 5.5290, Time: 0.27s
2025-04-26 00:00:19,493 - INFO - Batch: 120/141, Loss: 1.7095, Perplexity: 5.5263, Time: 0.27s
2025-04-26 00:00:19,872 - INFO - Batch: 130/141, Loss: 1.6943, Perplexity: 5.4426, Time: 0.38s
2025-04-26 00:00:20,138 - INFO - Batch: 140/141, Loss: 1.6985, Perplexity: 5.4655, Time: 0.27s
2025-04-26 00:00:20,940 - INFO - Validation  Loss: 1.8769, Perplexity: 6.5333, Time: 0.77s
2025-04-26 00:00:20,940 - INFO - Epoch 200/300 - Train Loss: 1.7049, Val Loss: 1.8769
2025-04-26 00:00:21,216 - INFO - Batch: 10/141, Loss: 1.7038, Perplexity: 5.4948, Time: 0.28s
2025-04-26 00:00:21,626 - INFO - Batch: 20/141, Loss: 1.7150, Perplexity: 5.5565, Time: 0.41s
2025-04-26 00:00:21,950 - INFO - Batch: 30/141, Loss: 1.6993, Perplexity: 5.4699, Time: 0.32s
2025-04-26 00:00:22,278 - INFO - Batch: 40/141, Loss: 1.7125, Perplexity: 5.5431, Time: 0.33s
2025-04-26 00:00:22,602 - INFO - Batch: 50/141, Loss: 1.7167, Perplexity: 5.5663, Time: 0.32s
2025-04-26 00:00:22,861 - INFO - Batch: 60/141, Loss: 1.6814, Perplexity: 5.3730, Time: 0.26s
2025-04-26 00:00:23,181 - INFO - Batch: 70/141, Loss: 1.7010, Perplexity: 5.4792, Time: 0.32s
2025-04-26 00:00:23,539 - INFO - Batch: 80/141, Loss: 1.7199, Perplexity: 5.5837, Time: 0.36s
2025-04-26 00:00:23,973 - INFO - Batch: 90/141, Loss: 1.6937, Perplexity: 5.4394, Time: 0.43s
2025-04-26 00:00:24,301 - INFO - Batch: 100/141, Loss: 1.7159, Perplexity: 5.5618, Time: 0.33s
2025-04-26 00:00:24,645 - INFO - Batch: 110/141, Loss: 1.6999, Perplexity: 5.4737, Time: 0.34s
2025-04-26 00:00:24,979 - INFO - Batch: 120/141, Loss: 1.7107, Perplexity: 5.5329, Time: 0.33s
2025-04-26 00:00:25,321 - INFO - Batch: 130/141, Loss: 1.7028, Perplexity: 5.4895, Time: 0.34s
2025-04-26 00:00:25,611 - INFO - Batch: 140/141, Loss: 1.6928, Perplexity: 5.4349, Time: 0.29s
2025-04-26 00:00:26,503 - INFO - Validation  Loss: 1.8765, Perplexity: 6.5307, Time: 0.87s
2025-04-26 00:00:26,503 - INFO - Epoch 201/300 - Train Loss: 1.7048, Val Loss: 1.8765
2025-04-26 00:00:26,885 - INFO - Batch: 10/141, Loss: 1.7109, Perplexity: 5.5338, Time: 0.38s
2025-04-26 00:00:27,275 - INFO - Batch: 20/141, Loss: 1.7077, Perplexity: 5.5165, Time: 0.39s
2025-04-26 00:00:28,288 - INFO - Batch: 30/141, Loss: 1.7064, Perplexity: 5.5093, Time: 1.01s
2025-04-26 00:00:32,094 - INFO - Batch: 40/141, Loss: 1.7077, Perplexity: 5.5165, Time: 3.81s
2025-04-26 00:00:32,366 - INFO - Batch: 50/141, Loss: 1.6970, Perplexity: 5.4574, Time: 0.27s
2025-04-26 00:00:32,728 - INFO - Batch: 60/141, Loss: 1.6953, Perplexity: 5.4480, Time: 0.36s
2025-04-26 00:00:32,999 - INFO - Batch: 70/141, Loss: 1.7011, Perplexity: 5.4797, Time: 0.27s
2025-04-26 00:00:33,226 - INFO - Batch: 80/141, Loss: 1.6961, Perplexity: 5.4527, Time: 0.23s
2025-04-26 00:00:33,488 - INFO - Batch: 90/141, Loss: 1.7047, Perplexity: 5.4995, Time: 0.26s
2025-04-26 00:00:33,745 - INFO - Batch: 100/141, Loss: 1.6985, Perplexity: 5.4656, Time: 0.26s
2025-04-26 00:00:33,983 - INFO - Batch: 110/141, Loss: 1.6995, Perplexity: 5.4710, Time: 0.24s
2025-04-26 00:00:34,220 - INFO - Batch: 120/141, Loss: 1.7171, Perplexity: 5.5681, Time: 0.24s
2025-04-26 00:00:34,454 - INFO - Batch: 130/141, Loss: 1.7180, Perplexity: 5.5732, Time: 0.23s
2025-04-26 00:00:34,756 - INFO - Batch: 140/141, Loss: 1.7081, Perplexity: 5.5186, Time: 0.30s
2025-04-26 00:00:35,511 - INFO - Validation  Loss: 1.8763, Perplexity: 6.5295, Time: 0.72s
2025-04-26 00:00:35,511 - INFO - Epoch 202/300 - Train Loss: 1.7047, Val Loss: 1.8763
2025-04-26 00:00:35,823 - INFO - Batch: 10/141, Loss: 1.7100, Perplexity: 5.5291, Time: 0.31s
2025-04-26 00:00:36,139 - INFO - Batch: 20/141, Loss: 1.7032, Perplexity: 5.4917, Time: 0.32s
2025-04-26 00:00:36,466 - INFO - Batch: 30/141, Loss: 1.7092, Perplexity: 5.5245, Time: 0.33s
2025-04-26 00:00:36,701 - INFO - Batch: 40/141, Loss: 1.7109, Perplexity: 5.5341, Time: 0.24s
2025-04-26 00:00:37,017 - INFO - Batch: 50/141, Loss: 1.7017, Perplexity: 5.4831, Time: 0.32s
2025-04-26 00:00:37,317 - INFO - Batch: 60/141, Loss: 1.7075, Perplexity: 5.5149, Time: 0.30s
2025-04-26 00:00:37,729 - INFO - Batch: 70/141, Loss: 1.7210, Perplexity: 5.5901, Time: 0.41s
2025-04-26 00:00:38,026 - INFO - Batch: 80/141, Loss: 1.6993, Perplexity: 5.4703, Time: 0.30s
2025-04-26 00:00:38,412 - INFO - Batch: 90/141, Loss: 1.7021, Perplexity: 5.4852, Time: 0.39s
2025-04-26 00:00:38,828 - INFO - Batch: 100/141, Loss: 1.6958, Perplexity: 5.4508, Time: 0.42s
2025-04-26 00:00:39,145 - INFO - Batch: 110/141, Loss: 1.7103, Perplexity: 5.5304, Time: 0.32s
2025-04-26 00:00:39,503 - INFO - Batch: 120/141, Loss: 1.6984, Perplexity: 5.4654, Time: 0.36s
2025-04-26 00:00:39,815 - INFO - Batch: 130/141, Loss: 1.7034, Perplexity: 5.4929, Time: 0.31s
2025-04-26 00:00:40,126 - INFO - Batch: 140/141, Loss: 1.7129, Perplexity: 5.5450, Time: 0.31s
2025-04-26 00:00:40,904 - INFO - Validation  Loss: 1.8762, Perplexity: 6.5288, Time: 0.75s
2025-04-26 00:00:40,904 - INFO - Epoch 203/300 - Train Loss: 1.7047, Val Loss: 1.8762
2025-04-26 00:00:41,162 - INFO - Batch: 10/141, Loss: 1.7029, Perplexity: 5.4900, Time: 0.26s
2025-04-26 00:00:41,418 - INFO - Batch: 20/141, Loss: 1.6907, Perplexity: 5.4231, Time: 0.26s
2025-04-26 00:00:41,689 - INFO - Batch: 30/141, Loss: 1.6943, Perplexity: 5.4431, Time: 0.27s
2025-04-26 00:00:41,933 - INFO - Batch: 40/141, Loss: 1.7100, Perplexity: 5.5292, Time: 0.24s
2025-04-26 00:00:42,184 - INFO - Batch: 50/141, Loss: 1.7081, Perplexity: 5.5187, Time: 0.25s
2025-04-26 00:00:42,431 - INFO - Batch: 60/141, Loss: 1.7113, Perplexity: 5.5361, Time: 0.25s
2025-04-26 00:00:42,808 - INFO - Batch: 70/141, Loss: 1.7051, Perplexity: 5.5020, Time: 0.38s
2025-04-26 00:00:43,118 - INFO - Batch: 80/141, Loss: 1.7127, Perplexity: 5.5441, Time: 0.31s
2025-04-26 00:00:43,411 - INFO - Batch: 90/141, Loss: 1.7164, Perplexity: 5.5642, Time: 0.29s
2025-04-26 00:00:43,679 - INFO - Batch: 100/141, Loss: 1.7011, Perplexity: 5.4800, Time: 0.27s
2025-04-26 00:00:43,942 - INFO - Batch: 110/141, Loss: 1.7134, Perplexity: 5.5479, Time: 0.26s
2025-04-26 00:00:44,206 - INFO - Batch: 120/141, Loss: 1.7034, Perplexity: 5.4926, Time: 0.26s
2025-04-26 00:00:44,469 - INFO - Batch: 130/141, Loss: 1.7069, Perplexity: 5.5118, Time: 0.26s
2025-04-26 00:00:44,825 - INFO - Batch: 140/141, Loss: 1.6985, Perplexity: 5.4655, Time: 0.36s
2025-04-26 00:00:45,954 - INFO - Validation  Loss: 1.8763, Perplexity: 6.5294, Time: 1.08s
2025-04-26 00:00:45,954 - INFO - Epoch 204/300 - Train Loss: 1.7046, Val Loss: 1.8763
2025-04-26 00:00:46,297 - INFO - Batch: 10/141, Loss: 1.6962, Perplexity: 5.4532, Time: 0.34s
2025-04-26 00:00:46,608 - INFO - Batch: 20/141, Loss: 1.6984, Perplexity: 5.4650, Time: 0.31s
2025-04-26 00:00:46,915 - INFO - Batch: 30/141, Loss: 1.7021, Perplexity: 5.4852, Time: 0.31s
2025-04-26 00:00:47,296 - INFO - Batch: 40/141, Loss: 1.6928, Perplexity: 5.4347, Time: 0.38s
2025-04-26 00:00:47,606 - INFO - Batch: 50/141, Loss: 1.6986, Perplexity: 5.4661, Time: 0.31s
2025-04-26 00:00:47,865 - INFO - Batch: 60/141, Loss: 1.7024, Perplexity: 5.4868, Time: 0.26s
2025-04-26 00:00:48,130 - INFO - Batch: 70/141, Loss: 1.7077, Perplexity: 5.5161, Time: 0.26s
2025-04-26 00:00:48,390 - INFO - Batch: 80/141, Loss: 1.6847, Perplexity: 5.3906, Time: 0.26s
2025-04-26 00:00:48,677 - INFO - Batch: 90/141, Loss: 1.6975, Perplexity: 5.4601, Time: 0.29s
2025-04-26 00:00:48,927 - INFO - Batch: 100/141, Loss: 1.6953, Perplexity: 5.4483, Time: 0.25s
2025-04-26 00:00:49,271 - INFO - Batch: 110/141, Loss: 1.7083, Perplexity: 5.5195, Time: 0.34s
2025-04-26 00:00:49,535 - INFO - Batch: 120/141, Loss: 1.7106, Perplexity: 5.5320, Time: 0.26s
2025-04-26 00:00:49,776 - INFO - Batch: 130/141, Loss: 1.7093, Perplexity: 5.5249, Time: 0.24s
2025-04-26 00:00:50,018 - INFO - Batch: 140/141, Loss: 1.6963, Perplexity: 5.4536, Time: 0.24s
2025-04-26 00:00:50,869 - INFO - Validation  Loss: 1.8770, Perplexity: 6.5338, Time: 0.82s
2025-04-26 00:00:50,869 - INFO - Epoch 205/300 - Train Loss: 1.7045, Val Loss: 1.8770
2025-04-26 00:00:51,140 - INFO - Batch: 10/141, Loss: 1.7025, Perplexity: 5.4879, Time: 0.27s
2025-04-26 00:00:51,395 - INFO - Batch: 20/141, Loss: 1.7030, Perplexity: 5.4903, Time: 0.26s
2025-04-26 00:00:51,661 - INFO - Batch: 30/141, Loss: 1.7079, Perplexity: 5.5174, Time: 0.27s
2025-04-26 00:00:51,931 - INFO - Batch: 40/141, Loss: 1.6849, Perplexity: 5.3917, Time: 0.27s
2025-04-26 00:00:52,198 - INFO - Batch: 50/141, Loss: 1.7050, Perplexity: 5.5013, Time: 0.27s
2025-04-26 00:00:52,465 - INFO - Batch: 60/141, Loss: 1.7071, Perplexity: 5.5130, Time: 0.27s
2025-04-26 00:00:52,744 - INFO - Batch: 70/141, Loss: 1.6927, Perplexity: 5.4339, Time: 0.28s
2025-04-26 00:00:53,104 - INFO - Batch: 80/141, Loss: 1.7037, Perplexity: 5.4943, Time: 0.36s
2025-04-26 00:00:53,374 - INFO - Batch: 90/141, Loss: 1.6956, Perplexity: 5.4501, Time: 0.27s
2025-04-26 00:00:53,683 - INFO - Batch: 100/141, Loss: 1.6903, Perplexity: 5.4211, Time: 0.31s
2025-04-26 00:00:53,979 - INFO - Batch: 110/141, Loss: 1.7036, Perplexity: 5.4935, Time: 0.30s
2025-04-26 00:00:54,305 - INFO - Batch: 120/141, Loss: 1.6998, Perplexity: 5.4728, Time: 0.33s
2025-04-26 00:00:54,645 - INFO - Batch: 130/141, Loss: 1.7175, Perplexity: 5.5707, Time: 0.34s
2025-04-26 00:00:55,150 - INFO - Batch: 140/141, Loss: 1.7087, Perplexity: 5.5220, Time: 0.27s
2025-04-26 00:00:55,851 - INFO - Validation  Loss: 1.8776, Perplexity: 6.5377, Time: 0.67s
2025-04-26 00:00:55,851 - INFO - Epoch 206/300 - Train Loss: 1.7045, Val Loss: 1.8776
2025-04-26 00:00:56,106 - INFO - Batch: 10/141, Loss: 1.6985, Perplexity: 5.4659, Time: 0.26s
2025-04-26 00:00:56,409 - INFO - Batch: 20/141, Loss: 1.7025, Perplexity: 5.4878, Time: 0.30s
2025-04-26 00:00:56,704 - INFO - Batch: 30/141, Loss: 1.7048, Perplexity: 5.5001, Time: 0.29s
2025-04-26 00:00:57,156 - INFO - Batch: 40/141, Loss: 1.7134, Perplexity: 5.5479, Time: 0.45s
2025-04-26 00:00:57,409 - INFO - Batch: 50/141, Loss: 1.7089, Perplexity: 5.5226, Time: 0.25s
2025-04-26 00:00:57,641 - INFO - Batch: 60/141, Loss: 1.6965, Perplexity: 5.4549, Time: 0.23s
2025-04-26 00:00:57,915 - INFO - Batch: 70/141, Loss: 1.7110, Perplexity: 5.5347, Time: 0.27s
2025-04-26 00:00:58,171 - INFO - Batch: 80/141, Loss: 1.7148, Perplexity: 5.5555, Time: 0.26s
2025-04-26 00:00:58,415 - INFO - Batch: 90/141, Loss: 1.6989, Perplexity: 5.4679, Time: 0.24s
2025-04-26 00:00:58,737 - INFO - Batch: 100/141, Loss: 1.7078, Perplexity: 5.5169, Time: 0.32s
2025-04-26 00:00:58,993 - INFO - Batch: 110/141, Loss: 1.7042, Perplexity: 5.4969, Time: 0.26s
2025-04-26 00:00:59,319 - INFO - Batch: 120/141, Loss: 1.6876, Perplexity: 5.4066, Time: 0.33s
2025-04-26 00:00:59,566 - INFO - Batch: 130/141, Loss: 1.7019, Perplexity: 5.4846, Time: 0.25s
2025-04-26 00:00:59,853 - INFO - Batch: 140/141, Loss: 1.6984, Perplexity: 5.4653, Time: 0.29s
2025-04-26 00:01:00,763 - INFO - Validation  Loss: 1.8770, Perplexity: 6.5338, Time: 0.88s
2025-04-26 00:01:00,763 - INFO - Epoch 207/300 - Train Loss: 1.7044, Val Loss: 1.8770
2025-04-26 00:01:01,170 - INFO - Batch: 10/141, Loss: 1.7085, Perplexity: 5.5205, Time: 0.41s
2025-04-26 00:01:01,696 - INFO - Batch: 20/141, Loss: 1.6995, Perplexity: 5.4710, Time: 0.53s
2025-04-26 00:01:02,351 - INFO - Batch: 30/141, Loss: 1.7087, Perplexity: 5.5216, Time: 0.65s
2025-04-26 00:01:03,817 - INFO - Batch: 40/141, Loss: 1.6873, Perplexity: 5.4049, Time: 1.47s
2025-04-26 00:01:04,180 - INFO - Batch: 50/141, Loss: 1.6992, Perplexity: 5.4698, Time: 0.36s
2025-04-26 00:01:04,690 - INFO - Batch: 60/141, Loss: 1.7158, Perplexity: 5.5610, Time: 0.51s
2025-04-26 00:01:04,948 - INFO - Batch: 70/141, Loss: 1.6963, Perplexity: 5.4539, Time: 0.26s
2025-04-26 00:01:05,300 - INFO - Batch: 80/141, Loss: 1.7141, Perplexity: 5.5518, Time: 0.35s
2025-04-26 00:01:05,699 - INFO - Batch: 90/141, Loss: 1.6914, Perplexity: 5.4270, Time: 0.40s
2025-04-26 00:01:05,950 - INFO - Batch: 100/141, Loss: 1.7043, Perplexity: 5.4977, Time: 0.25s
2025-04-26 00:01:06,185 - INFO - Batch: 110/141, Loss: 1.6893, Perplexity: 5.4155, Time: 0.24s
2025-04-26 00:01:06,420 - INFO - Batch: 120/141, Loss: 1.7046, Perplexity: 5.4994, Time: 0.23s
2025-04-26 00:01:06,682 - INFO - Batch: 130/141, Loss: 1.6972, Perplexity: 5.4589, Time: 0.26s
2025-04-26 00:01:06,930 - INFO - Batch: 140/141, Loss: 1.7169, Perplexity: 5.5672, Time: 0.25s
2025-04-26 00:01:07,603 - INFO - Validation  Loss: 1.8772, Perplexity: 6.5353, Time: 0.64s
2025-04-26 00:01:07,603 - INFO - Epoch 208/300 - Train Loss: 1.7043, Val Loss: 1.8772
2025-04-26 00:01:07,839 - INFO - Batch: 10/141, Loss: 1.7169, Perplexity: 5.5670, Time: 0.24s
2025-04-26 00:01:08,083 - INFO - Batch: 20/141, Loss: 1.7037, Perplexity: 5.4940, Time: 0.24s
2025-04-26 00:01:08,310 - INFO - Batch: 30/141, Loss: 1.7098, Perplexity: 5.5276, Time: 0.23s
2025-04-26 00:01:08,764 - INFO - Batch: 40/141, Loss: 1.7132, Perplexity: 5.5464, Time: 0.45s
2025-04-26 00:01:09,708 - INFO - Batch: 50/141, Loss: 1.7122, Perplexity: 5.5409, Time: 0.94s
2025-04-26 00:01:09,923 - INFO - Batch: 60/141, Loss: 1.7007, Perplexity: 5.4779, Time: 0.21s
2025-04-26 00:01:10,329 - INFO - Batch: 70/141, Loss: 1.6795, Perplexity: 5.3628, Time: 0.41s
2025-04-26 00:01:10,593 - INFO - Batch: 80/141, Loss: 1.6933, Perplexity: 5.4375, Time: 0.26s
2025-04-26 00:01:10,844 - INFO - Batch: 90/141, Loss: 1.7050, Perplexity: 5.5012, Time: 0.25s
2025-04-26 00:01:11,080 - INFO - Batch: 100/141, Loss: 1.7264, Perplexity: 5.6207, Time: 0.24s
2025-04-26 00:01:11,312 - INFO - Batch: 110/141, Loss: 1.7197, Perplexity: 5.5826, Time: 0.23s
2025-04-26 00:01:11,548 - INFO - Batch: 120/141, Loss: 1.7142, Perplexity: 5.5520, Time: 0.24s
2025-04-26 00:01:11,861 - INFO - Batch: 130/141, Loss: 1.7048, Perplexity: 5.5001, Time: 0.31s
2025-04-26 00:01:12,102 - INFO - Batch: 140/141, Loss: 1.6845, Perplexity: 5.3896, Time: 0.24s
2025-04-26 00:01:12,789 - INFO - Validation  Loss: 1.8773, Perplexity: 6.5361, Time: 0.65s
2025-04-26 00:01:12,789 - INFO - Epoch 209/300 - Train Loss: 1.7042, Val Loss: 1.8773
2025-04-26 00:01:13,029 - INFO - Batch: 10/141, Loss: 1.7086, Perplexity: 5.5211, Time: 0.24s
2025-04-26 00:01:13,332 - INFO - Batch: 20/141, Loss: 1.7015, Perplexity: 5.4820, Time: 0.30s
2025-04-26 00:01:13,562 - INFO - Batch: 30/141, Loss: 1.7082, Perplexity: 5.5189, Time: 0.23s
2025-04-26 00:01:13,814 - INFO - Batch: 40/141, Loss: 1.6944, Perplexity: 5.4436, Time: 0.25s
2025-04-26 00:01:14,071 - INFO - Batch: 50/141, Loss: 1.7057, Perplexity: 5.5050, Time: 0.26s
2025-04-26 00:01:14,300 - INFO - Batch: 60/141, Loss: 1.6912, Perplexity: 5.4262, Time: 0.23s
2025-04-26 00:01:14,532 - INFO - Batch: 70/141, Loss: 1.7099, Perplexity: 5.5281, Time: 0.23s
2025-04-26 00:01:14,787 - INFO - Batch: 80/141, Loss: 1.7000, Perplexity: 5.4738, Time: 0.25s
2025-04-26 00:01:15,096 - INFO - Batch: 90/141, Loss: 1.7088, Perplexity: 5.5226, Time: 0.31s
2025-04-26 00:01:15,462 - INFO - Batch: 100/141, Loss: 1.6956, Perplexity: 5.4500, Time: 0.37s
2025-04-26 00:01:15,758 - INFO - Batch: 110/141, Loss: 1.6965, Perplexity: 5.4550, Time: 0.30s
2025-04-26 00:01:16,093 - INFO - Batch: 120/141, Loss: 1.7245, Perplexity: 5.6099, Time: 0.33s
2025-04-26 00:01:16,395 - INFO - Batch: 130/141, Loss: 1.7045, Perplexity: 5.4987, Time: 0.30s
2025-04-26 00:01:16,740 - INFO - Batch: 140/141, Loss: 1.7096, Perplexity: 5.5269, Time: 0.34s
2025-04-26 00:01:17,640 - INFO - Validation  Loss: 1.8766, Perplexity: 6.5314, Time: 0.86s
2025-04-26 00:01:17,640 - INFO - Epoch 210/300 - Train Loss: 1.7042, Val Loss: 1.8766
2025-04-26 00:01:17,937 - INFO - Batch: 10/141, Loss: 1.6838, Perplexity: 5.3860, Time: 0.30s
2025-04-26 00:01:18,219 - INFO - Batch: 20/141, Loss: 1.6964, Perplexity: 5.4545, Time: 0.28s
2025-04-26 00:01:18,567 - INFO - Batch: 30/141, Loss: 1.7100, Perplexity: 5.5290, Time: 0.35s
2025-04-26 00:01:18,921 - INFO - Batch: 40/141, Loss: 1.7056, Perplexity: 5.5045, Time: 0.35s
2025-04-26 00:01:19,554 - INFO - Batch: 50/141, Loss: 1.7100, Perplexity: 5.5288, Time: 0.63s
2025-04-26 00:01:20,393 - INFO - Batch: 60/141, Loss: 1.7098, Perplexity: 5.5280, Time: 0.84s
2025-04-26 00:01:20,691 - INFO - Batch: 70/141, Loss: 1.7089, Perplexity: 5.5226, Time: 0.30s
2025-04-26 00:01:20,957 - INFO - Batch: 80/141, Loss: 1.6872, Perplexity: 5.4044, Time: 0.27s
2025-04-26 00:01:21,300 - INFO - Batch: 90/141, Loss: 1.7113, Perplexity: 5.5363, Time: 0.34s
2025-04-26 00:01:21,592 - INFO - Batch: 100/141, Loss: 1.6978, Perplexity: 5.4622, Time: 0.29s
2025-04-26 00:01:21,885 - INFO - Batch: 110/141, Loss: 1.6937, Perplexity: 5.4393, Time: 0.29s
2025-04-26 00:01:22,122 - INFO - Batch: 120/141, Loss: 1.7217, Perplexity: 5.5939, Time: 0.24s
2025-04-26 00:01:22,364 - INFO - Batch: 130/141, Loss: 1.7172, Perplexity: 5.5689, Time: 0.24s
2025-04-26 00:01:22,739 - INFO - Batch: 140/141, Loss: 1.7154, Perplexity: 5.5588, Time: 0.37s
2025-04-26 00:01:23,489 - INFO - Validation  Loss: 1.8772, Perplexity: 6.5354, Time: 0.71s
2025-04-26 00:01:23,489 - INFO - Epoch 211/300 - Train Loss: 1.7041, Val Loss: 1.8772
2025-04-26 00:01:23,764 - INFO - Batch: 10/141, Loss: 1.7170, Perplexity: 5.5676, Time: 0.27s
2025-04-26 00:01:24,087 - INFO - Batch: 20/141, Loss: 1.7029, Perplexity: 5.4900, Time: 0.32s
2025-04-26 00:01:24,442 - INFO - Batch: 30/141, Loss: 1.7117, Perplexity: 5.5382, Time: 0.35s
2025-04-26 00:01:24,671 - INFO - Batch: 40/141, Loss: 1.7001, Perplexity: 5.4747, Time: 0.23s
2025-04-26 00:01:24,942 - INFO - Batch: 50/141, Loss: 1.6951, Perplexity: 5.4474, Time: 0.27s
2025-04-26 00:01:25,253 - INFO - Batch: 60/141, Loss: 1.6949, Perplexity: 5.4459, Time: 0.31s
2025-04-26 00:01:25,574 - INFO - Batch: 70/141, Loss: 1.7101, Perplexity: 5.5293, Time: 0.32s
2025-04-26 00:01:25,887 - INFO - Batch: 80/141, Loss: 1.7063, Perplexity: 5.5087, Time: 0.31s
2025-04-26 00:01:26,164 - INFO - Batch: 90/141, Loss: 1.7199, Perplexity: 5.5838, Time: 0.28s
2025-04-26 00:01:26,451 - INFO - Batch: 100/141, Loss: 1.6900, Perplexity: 5.4196, Time: 0.29s
2025-04-26 00:01:26,784 - INFO - Batch: 110/141, Loss: 1.7175, Perplexity: 5.5705, Time: 0.33s
2025-04-26 00:01:27,024 - INFO - Batch: 120/141, Loss: 1.7061, Perplexity: 5.5077, Time: 0.24s
2025-04-26 00:01:27,327 - INFO - Batch: 130/141, Loss: 1.6938, Perplexity: 5.4402, Time: 0.30s
2025-04-26 00:01:27,581 - INFO - Batch: 140/141, Loss: 1.7029, Perplexity: 5.4900, Time: 0.25s
2025-04-26 00:01:28,564 - INFO - Validation  Loss: 1.8767, Perplexity: 6.5316, Time: 0.96s
2025-04-26 00:01:28,564 - INFO - Epoch 212/300 - Train Loss: 1.7041, Val Loss: 1.8767
2025-04-26 00:01:28,974 - INFO - Batch: 10/141, Loss: 1.7128, Perplexity: 5.5446, Time: 0.41s
2025-04-26 00:01:29,262 - INFO - Batch: 20/141, Loss: 1.7135, Perplexity: 5.5484, Time: 0.29s
2025-04-26 00:01:29,631 - INFO - Batch: 30/141, Loss: 1.7105, Perplexity: 5.5316, Time: 0.37s
2025-04-26 00:01:30,105 - INFO - Batch: 40/141, Loss: 1.7029, Perplexity: 5.4898, Time: 0.47s
2025-04-26 00:01:30,591 - INFO - Batch: 50/141, Loss: 1.6985, Perplexity: 5.4659, Time: 0.49s
2025-04-26 00:01:31,196 - INFO - Batch: 60/141, Loss: 1.7024, Perplexity: 5.4872, Time: 0.60s
2025-04-26 00:01:31,999 - INFO - Batch: 70/141, Loss: 1.7037, Perplexity: 5.4940, Time: 0.80s
2025-04-26 00:01:32,441 - INFO - Batch: 80/141, Loss: 1.7135, Perplexity: 5.5482, Time: 0.44s
2025-04-26 00:01:32,697 - INFO - Batch: 90/141, Loss: 1.7018, Perplexity: 5.4841, Time: 0.26s
2025-04-26 00:01:33,272 - INFO - Batch: 100/141, Loss: 1.7071, Perplexity: 5.5127, Time: 0.57s
2025-04-26 00:01:33,793 - INFO - Batch: 110/141, Loss: 1.7144, Perplexity: 5.5536, Time: 0.52s
2025-04-26 00:01:34,440 - INFO - Batch: 120/141, Loss: 1.7003, Perplexity: 5.4757, Time: 0.65s
2025-04-26 00:01:34,796 - INFO - Batch: 130/141, Loss: 1.7078, Perplexity: 5.5170, Time: 0.36s
2025-04-26 00:01:35,158 - INFO - Batch: 140/141, Loss: 1.7123, Perplexity: 5.5417, Time: 0.36s
2025-04-26 00:01:36,459 - INFO - Validation  Loss: 1.8767, Perplexity: 6.5319, Time: 1.27s
2025-04-26 00:01:36,460 - INFO - Epoch 213/300 - Train Loss: 1.7040, Val Loss: 1.8767
2025-04-26 00:01:37,343 - INFO - Batch: 10/141, Loss: 1.7042, Perplexity: 5.4971, Time: 0.88s
2025-04-26 00:01:38,107 - INFO - Batch: 20/141, Loss: 1.7001, Perplexity: 5.4744, Time: 0.76s
2025-04-26 00:01:38,697 - INFO - Batch: 30/141, Loss: 1.7008, Perplexity: 5.4781, Time: 0.59s
2025-04-26 00:01:39,441 - INFO - Batch: 40/141, Loss: 1.7074, Perplexity: 5.5147, Time: 0.74s
2025-04-26 00:01:40,149 - INFO - Batch: 50/141, Loss: 1.7086, Perplexity: 5.5211, Time: 0.71s
2025-04-26 00:01:40,780 - INFO - Batch: 60/141, Loss: 1.7123, Perplexity: 5.5415, Time: 0.63s
2025-04-26 00:01:41,414 - INFO - Batch: 70/141, Loss: 1.6991, Perplexity: 5.4693, Time: 0.63s
2025-04-26 00:01:42,078 - INFO - Batch: 80/141, Loss: 1.7104, Perplexity: 5.5313, Time: 0.66s
2025-04-26 00:01:42,647 - INFO - Batch: 90/141, Loss: 1.7057, Perplexity: 5.5050, Time: 0.57s
2025-04-26 00:01:43,398 - INFO - Batch: 100/141, Loss: 1.7010, Perplexity: 5.4793, Time: 0.75s
2025-04-26 00:01:44,229 - INFO - Batch: 110/141, Loss: 1.6987, Perplexity: 5.4668, Time: 0.83s
2025-04-26 00:01:44,856 - INFO - Batch: 120/141, Loss: 1.7106, Perplexity: 5.5322, Time: 0.63s
2025-04-26 00:01:45,507 - INFO - Batch: 130/141, Loss: 1.7069, Perplexity: 5.5121, Time: 0.65s
2025-04-26 00:01:46,460 - INFO - Batch: 140/141, Loss: 1.6997, Perplexity: 5.4726, Time: 0.95s
2025-04-26 00:01:48,153 - INFO - Validation  Loss: 1.8769, Perplexity: 6.5330, Time: 1.59s
2025-04-26 00:01:48,153 - INFO - Epoch 214/300 - Train Loss: 1.7039, Val Loss: 1.8769
2025-04-26 00:01:48,832 - INFO - Batch: 10/141, Loss: 1.6990, Perplexity: 5.4682, Time: 0.68s
2025-04-26 00:01:49,594 - INFO - Batch: 20/141, Loss: 1.6958, Perplexity: 5.4512, Time: 0.76s
2025-04-26 00:01:50,037 - INFO - Batch: 30/141, Loss: 1.7045, Perplexity: 5.4987, Time: 0.44s
2025-04-26 00:01:50,429 - INFO - Batch: 40/141, Loss: 1.7158, Perplexity: 5.5610, Time: 0.39s
2025-04-26 00:01:50,817 - INFO - Batch: 50/141, Loss: 1.6966, Perplexity: 5.4555, Time: 0.39s
2025-04-26 00:01:51,181 - INFO - Batch: 60/141, Loss: 1.7112, Perplexity: 5.5356, Time: 0.36s
2025-04-26 00:01:51,602 - INFO - Batch: 70/141, Loss: 1.6996, Perplexity: 5.4719, Time: 0.42s
2025-04-26 00:01:51,991 - INFO - Batch: 80/141, Loss: 1.7053, Perplexity: 5.5029, Time: 0.39s
2025-04-26 00:01:52,306 - INFO - Batch: 90/141, Loss: 1.7085, Perplexity: 5.5210, Time: 0.32s
2025-04-26 00:01:52,591 - INFO - Batch: 100/141, Loss: 1.7060, Perplexity: 5.5072, Time: 0.28s
2025-04-26 00:01:52,841 - INFO - Batch: 110/141, Loss: 1.7120, Perplexity: 5.5398, Time: 0.25s
2025-04-26 00:01:53,119 - INFO - Batch: 120/141, Loss: 1.7029, Perplexity: 5.4898, Time: 0.28s
2025-04-26 00:01:53,398 - INFO - Batch: 130/141, Loss: 1.7070, Perplexity: 5.5124, Time: 0.28s
2025-04-26 00:01:53,651 - INFO - Batch: 140/141, Loss: 1.7117, Perplexity: 5.5383, Time: 0.25s
2025-04-26 00:01:54,468 - INFO - Validation  Loss: 1.8767, Perplexity: 6.5320, Time: 0.78s
2025-04-26 00:01:54,468 - INFO - Epoch 215/300 - Train Loss: 1.7038, Val Loss: 1.8767
2025-04-26 00:01:54,796 - INFO - Batch: 10/141, Loss: 1.6940, Perplexity: 5.4411, Time: 0.33s
2025-04-26 00:01:55,076 - INFO - Batch: 20/141, Loss: 1.7133, Perplexity: 5.5474, Time: 0.28s
2025-04-26 00:01:55,339 - INFO - Batch: 30/141, Loss: 1.7045, Perplexity: 5.4984, Time: 0.26s
2025-04-26 00:01:55,641 - INFO - Batch: 40/141, Loss: 1.6988, Perplexity: 5.4674, Time: 0.30s
2025-04-26 00:01:55,971 - INFO - Batch: 50/141, Loss: 1.7014, Perplexity: 5.4815, Time: 0.33s
2025-04-26 00:01:56,228 - INFO - Batch: 60/141, Loss: 1.6991, Perplexity: 5.4689, Time: 0.26s
2025-04-26 00:01:56,482 - INFO - Batch: 70/141, Loss: 1.7012, Perplexity: 5.4808, Time: 0.25s
2025-04-26 00:01:56,768 - INFO - Batch: 80/141, Loss: 1.6943, Perplexity: 5.4429, Time: 0.29s
2025-04-26 00:01:57,238 - INFO - Batch: 90/141, Loss: 1.6955, Perplexity: 5.4494, Time: 0.47s
2025-04-26 00:01:57,981 - INFO - Batch: 100/141, Loss: 1.7165, Perplexity: 5.5649, Time: 0.74s
2025-04-26 00:01:58,601 - INFO - Batch: 110/141, Loss: 1.7120, Perplexity: 5.5401, Time: 0.62s
2025-04-26 00:01:59,068 - INFO - Batch: 120/141, Loss: 1.7032, Perplexity: 5.4916, Time: 0.47s
2025-04-26 00:01:59,524 - INFO - Batch: 130/141, Loss: 1.7107, Perplexity: 5.5326, Time: 0.46s
2025-04-26 00:01:59,908 - INFO - Batch: 140/141, Loss: 1.7030, Perplexity: 5.4905, Time: 0.38s
2025-04-26 00:02:00,588 - INFO - Validation  Loss: 1.8776, Perplexity: 6.5380, Time: 0.64s
2025-04-26 00:02:00,588 - INFO - Epoch 216/300 - Train Loss: 1.7038, Val Loss: 1.8776
2025-04-26 00:02:01,416 - INFO - Batch: 10/141, Loss: 1.7050, Perplexity: 5.5015, Time: 0.83s
2025-04-26 00:02:01,655 - INFO - Batch: 20/141, Loss: 1.7064, Perplexity: 5.5089, Time: 0.24s
2025-04-26 00:02:01,949 - INFO - Batch: 30/141, Loss: 1.7184, Perplexity: 5.5757, Time: 0.29s
2025-04-26 00:02:02,199 - INFO - Batch: 40/141, Loss: 1.7029, Perplexity: 5.4900, Time: 0.25s
2025-04-26 00:02:02,439 - INFO - Batch: 50/141, Loss: 1.6952, Perplexity: 5.4475, Time: 0.24s
2025-04-26 00:02:02,774 - INFO - Batch: 60/141, Loss: 1.6936, Perplexity: 5.4388, Time: 0.33s
2025-04-26 00:02:03,044 - INFO - Batch: 70/141, Loss: 1.6972, Perplexity: 5.4589, Time: 0.27s
2025-04-26 00:02:03,308 - INFO - Batch: 80/141, Loss: 1.7111, Perplexity: 5.5348, Time: 0.26s
2025-04-26 00:02:03,812 - INFO - Batch: 90/141, Loss: 1.6994, Perplexity: 5.4709, Time: 0.50s
2025-04-26 00:02:04,585 - INFO - Batch: 100/141, Loss: 1.7088, Perplexity: 5.5221, Time: 0.77s
2025-04-26 00:02:05,044 - INFO - Batch: 110/141, Loss: 1.7160, Perplexity: 5.5624, Time: 0.46s
2025-04-26 00:02:05,301 - INFO - Batch: 120/141, Loss: 1.7124, Perplexity: 5.5423, Time: 0.26s
2025-04-26 00:02:05,557 - INFO - Batch: 130/141, Loss: 1.6951, Perplexity: 5.4475, Time: 0.26s
2025-04-26 00:02:05,827 - INFO - Batch: 140/141, Loss: 1.6943, Perplexity: 5.4429, Time: 0.27s
2025-04-26 00:02:07,105 - INFO - Validation  Loss: 1.8767, Perplexity: 6.5317, Time: 1.25s
2025-04-26 00:02:07,107 - INFO - Epoch 217/300 - Train Loss: 1.7038, Val Loss: 1.8767
2025-04-26 00:02:07,596 - INFO - Batch: 10/141, Loss: 1.7102, Perplexity: 5.5303, Time: 0.49s
2025-04-26 00:02:08,000 - INFO - Batch: 20/141, Loss: 1.6986, Perplexity: 5.4661, Time: 0.40s
2025-04-26 00:02:08,660 - INFO - Batch: 30/141, Loss: 1.7077, Perplexity: 5.5164, Time: 0.66s
2025-04-26 00:02:09,347 - INFO - Batch: 40/141, Loss: 1.7162, Perplexity: 5.5632, Time: 0.69s
2025-04-26 00:02:10,020 - INFO - Batch: 50/141, Loss: 1.6854, Perplexity: 5.3945, Time: 0.67s
2025-04-26 00:02:10,971 - INFO - Batch: 60/141, Loss: 1.7008, Perplexity: 5.4783, Time: 0.95s
2025-04-26 00:02:11,649 - INFO - Batch: 70/141, Loss: 1.7066, Perplexity: 5.5103, Time: 0.68s
2025-04-26 00:02:12,364 - INFO - Batch: 80/141, Loss: 1.7002, Perplexity: 5.4750, Time: 0.71s
2025-04-26 00:02:12,997 - INFO - Batch: 90/141, Loss: 1.6953, Perplexity: 5.4481, Time: 0.63s
2025-04-26 00:02:13,498 - INFO - Batch: 100/141, Loss: 1.7114, Perplexity: 5.5369, Time: 0.50s
2025-04-26 00:02:13,748 - INFO - Batch: 110/141, Loss: 1.6917, Perplexity: 5.4290, Time: 0.25s
2025-04-26 00:02:13,993 - INFO - Batch: 120/141, Loss: 1.7088, Perplexity: 5.5221, Time: 0.24s
2025-04-26 00:02:14,843 - INFO - Batch: 130/141, Loss: 1.7060, Perplexity: 5.5069, Time: 0.85s
2025-04-26 00:02:15,586 - INFO - Batch: 140/141, Loss: 1.7182, Perplexity: 5.5747, Time: 0.74s
2025-04-26 00:02:17,307 - INFO - Validation  Loss: 1.8766, Perplexity: 6.5315, Time: 1.63s
2025-04-26 00:02:17,308 - INFO - Epoch 218/300 - Train Loss: 1.7038, Val Loss: 1.8766
2025-04-26 00:02:18,015 - INFO - Batch: 10/141, Loss: 1.7003, Perplexity: 5.4757, Time: 0.71s
2025-04-26 00:02:18,857 - INFO - Batch: 20/141, Loss: 1.7082, Perplexity: 5.5192, Time: 0.84s
2025-04-26 00:02:19,520 - INFO - Batch: 30/141, Loss: 1.6953, Perplexity: 5.4484, Time: 0.66s
2025-04-26 00:02:20,163 - INFO - Batch: 40/141, Loss: 1.6971, Perplexity: 5.4579, Time: 0.64s
2025-04-26 00:02:20,915 - INFO - Batch: 50/141, Loss: 1.6961, Perplexity: 5.4526, Time: 0.75s
2025-04-26 00:02:21,713 - INFO - Batch: 60/141, Loss: 1.7085, Perplexity: 5.5206, Time: 0.80s
2025-04-26 00:02:22,596 - INFO - Batch: 70/141, Loss: 1.7082, Perplexity: 5.5191, Time: 0.88s
2025-04-26 00:02:23,277 - INFO - Batch: 80/141, Loss: 1.7050, Perplexity: 5.5014, Time: 0.68s
2025-04-26 00:02:23,912 - INFO - Batch: 90/141, Loss: 1.7111, Perplexity: 5.5353, Time: 0.63s
2025-04-26 00:02:24,748 - INFO - Batch: 100/141, Loss: 1.7011, Perplexity: 5.4799, Time: 0.84s
2025-04-26 00:02:25,389 - INFO - Batch: 110/141, Loss: 1.7035, Perplexity: 5.4930, Time: 0.64s
2025-04-26 00:02:26,147 - INFO - Batch: 120/141, Loss: 1.7056, Perplexity: 5.5045, Time: 0.76s
2025-04-26 00:02:26,721 - INFO - Batch: 130/141, Loss: 1.6997, Perplexity: 5.4725, Time: 0.57s
2025-04-26 00:02:27,304 - INFO - Batch: 140/141, Loss: 1.7078, Perplexity: 5.5166, Time: 0.58s
2025-04-26 00:02:29,183 - INFO - Validation  Loss: 1.8762, Perplexity: 6.5284, Time: 1.82s
2025-04-26 00:02:29,183 - INFO - Epoch 219/300 - Train Loss: 1.7037, Val Loss: 1.8762
2025-04-26 00:02:29,777 - INFO - Batch: 10/141, Loss: 1.7042, Perplexity: 5.4972, Time: 0.59s
2025-04-26 00:02:30,337 - INFO - Batch: 20/141, Loss: 1.7053, Perplexity: 5.5030, Time: 0.56s
2025-04-26 00:02:30,913 - INFO - Batch: 30/141, Loss: 1.6983, Perplexity: 5.4647, Time: 0.58s
2025-04-26 00:02:31,772 - INFO - Batch: 40/141, Loss: 1.7051, Perplexity: 5.5018, Time: 0.86s
2025-04-26 00:02:32,405 - INFO - Batch: 50/141, Loss: 1.7006, Perplexity: 5.4771, Time: 0.63s
2025-04-26 00:02:33,211 - INFO - Batch: 60/141, Loss: 1.7019, Perplexity: 5.4841, Time: 0.81s
2025-04-26 00:02:33,781 - INFO - Batch: 70/141, Loss: 1.7189, Perplexity: 5.5786, Time: 0.57s
2025-04-26 00:02:34,382 - INFO - Batch: 80/141, Loss: 1.6977, Perplexity: 5.4611, Time: 0.60s
2025-04-26 00:02:34,949 - INFO - Batch: 90/141, Loss: 1.6943, Perplexity: 5.4427, Time: 0.57s
2025-04-26 00:02:35,605 - INFO - Batch: 100/141, Loss: 1.6969, Perplexity: 5.4568, Time: 0.66s
2025-04-26 00:02:36,332 - INFO - Batch: 110/141, Loss: 1.7028, Perplexity: 5.4896, Time: 0.73s
2025-04-26 00:02:37,020 - INFO - Batch: 120/141, Loss: 1.6970, Perplexity: 5.4577, Time: 0.69s
2025-04-26 00:02:37,684 - INFO - Batch: 130/141, Loss: 1.6993, Perplexity: 5.4702, Time: 0.66s
2025-04-26 00:02:38,449 - INFO - Batch: 140/141, Loss: 1.7131, Perplexity: 5.5461, Time: 0.76s
2025-04-26 00:02:40,011 - INFO - Validation  Loss: 1.8765, Perplexity: 6.5309, Time: 1.48s
2025-04-26 00:02:40,012 - INFO - Epoch 220/300 - Train Loss: 1.7036, Val Loss: 1.8765
2025-04-26 00:02:40,632 - INFO - Batch: 10/141, Loss: 1.7124, Perplexity: 5.5422, Time: 0.62s
2025-04-26 00:02:41,323 - INFO - Batch: 20/141, Loss: 1.7152, Perplexity: 5.5577, Time: 0.69s
2025-04-26 00:02:42,145 - INFO - Batch: 30/141, Loss: 1.7054, Perplexity: 5.5039, Time: 0.82s
2025-04-26 00:02:42,861 - INFO - Batch: 40/141, Loss: 1.7053, Perplexity: 5.5030, Time: 0.72s
2025-04-26 00:02:43,622 - INFO - Batch: 50/141, Loss: 1.6990, Perplexity: 5.4685, Time: 0.76s
2025-04-26 00:02:44,479 - INFO - Batch: 60/141, Loss: 1.7084, Perplexity: 5.5203, Time: 0.86s
2025-04-26 00:02:45,121 - INFO - Batch: 70/141, Loss: 1.7107, Perplexity: 5.5331, Time: 0.64s
2025-04-26 00:02:45,787 - INFO - Batch: 80/141, Loss: 1.7150, Perplexity: 5.5567, Time: 0.67s
2025-04-26 00:02:46,535 - INFO - Batch: 90/141, Loss: 1.7168, Perplexity: 5.5666, Time: 0.75s
2025-04-26 00:02:47,372 - INFO - Batch: 100/141, Loss: 1.6956, Perplexity: 5.4500, Time: 0.84s
2025-04-26 00:02:48,242 - INFO - Batch: 110/141, Loss: 1.7016, Perplexity: 5.4825, Time: 0.87s
2025-04-26 00:02:48,916 - INFO - Batch: 120/141, Loss: 1.6989, Perplexity: 5.4679, Time: 0.67s
2025-04-26 00:02:49,485 - INFO - Batch: 130/141, Loss: 1.7033, Perplexity: 5.4923, Time: 0.57s
2025-04-26 00:02:50,110 - INFO - Batch: 140/141, Loss: 1.6913, Perplexity: 5.4263, Time: 0.62s
2025-04-26 00:02:51,774 - INFO - Validation  Loss: 1.8773, Perplexity: 6.5360, Time: 1.59s
2025-04-26 00:02:51,774 - INFO - Epoch 221/300 - Train Loss: 1.7036, Val Loss: 1.8773
2025-04-26 00:02:52,337 - INFO - Batch: 10/141, Loss: 1.7044, Perplexity: 5.4981, Time: 0.56s
2025-04-26 00:02:52,850 - INFO - Batch: 20/141, Loss: 1.7076, Perplexity: 5.5156, Time: 0.51s
2025-04-26 00:02:53,618 - INFO - Batch: 30/141, Loss: 1.7030, Perplexity: 5.4904, Time: 0.77s
2025-04-26 00:02:54,188 - INFO - Batch: 40/141, Loss: 1.7077, Perplexity: 5.5164, Time: 0.57s
2025-04-26 00:02:54,517 - INFO - Batch: 50/141, Loss: 1.7045, Perplexity: 5.4987, Time: 0.33s
2025-04-26 00:02:54,772 - INFO - Batch: 60/141, Loss: 1.7026, Perplexity: 5.4881, Time: 0.25s
2025-04-26 00:02:55,100 - INFO - Batch: 70/141, Loss: 1.7111, Perplexity: 5.5349, Time: 0.33s
2025-04-26 00:02:55,352 - INFO - Batch: 80/141, Loss: 1.6952, Perplexity: 5.4475, Time: 0.25s
2025-04-26 00:02:55,597 - INFO - Batch: 90/141, Loss: 1.7099, Perplexity: 5.5284, Time: 0.25s
2025-04-26 00:02:55,842 - INFO - Batch: 100/141, Loss: 1.7141, Perplexity: 5.5518, Time: 0.24s
2025-04-26 00:02:56,079 - INFO - Batch: 110/141, Loss: 1.7175, Perplexity: 5.5707, Time: 0.24s
2025-04-26 00:02:56,334 - INFO - Batch: 120/141, Loss: 1.7011, Perplexity: 5.4801, Time: 0.25s
2025-04-26 00:02:56,578 - INFO - Batch: 130/141, Loss: 1.7005, Perplexity: 5.4768, Time: 0.24s
2025-04-26 00:02:56,826 - INFO - Batch: 140/141, Loss: 1.7105, Perplexity: 5.5316, Time: 0.25s
2025-04-26 00:02:57,556 - INFO - Validation  Loss: 1.8776, Perplexity: 6.5380, Time: 0.70s
2025-04-26 00:02:57,556 - INFO - Epoch 222/300 - Train Loss: 1.7035, Val Loss: 1.8776
2025-04-26 00:02:57,813 - INFO - Batch: 10/141, Loss: 1.7058, Perplexity: 5.5056, Time: 0.26s
2025-04-26 00:02:58,699 - INFO - Batch: 20/141, Loss: 1.6930, Perplexity: 5.4360, Time: 0.89s
2025-04-26 00:02:59,313 - INFO - Batch: 30/141, Loss: 1.7004, Perplexity: 5.4762, Time: 0.61s
2025-04-26 00:03:00,045 - INFO - Batch: 40/141, Loss: 1.7089, Perplexity: 5.5227, Time: 0.73s
2025-04-26 00:03:00,650 - INFO - Batch: 50/141, Loss: 1.6963, Perplexity: 5.4539, Time: 0.60s
2025-04-26 00:03:01,358 - INFO - Batch: 60/141, Loss: 1.7138, Perplexity: 5.5498, Time: 0.71s
2025-04-26 00:03:02,043 - INFO - Batch: 70/141, Loss: 1.6972, Perplexity: 5.4585, Time: 0.68s
2025-04-26 00:03:02,619 - INFO - Batch: 80/141, Loss: 1.6866, Perplexity: 5.4009, Time: 0.58s
2025-04-26 00:03:03,236 - INFO - Batch: 90/141, Loss: 1.6912, Perplexity: 5.4260, Time: 0.62s
2025-04-26 00:03:03,898 - INFO - Batch: 100/141, Loss: 1.6978, Perplexity: 5.4619, Time: 0.66s
2025-04-26 00:03:04,475 - INFO - Batch: 110/141, Loss: 1.6927, Perplexity: 5.4342, Time: 0.58s
2025-04-26 00:03:04,885 - INFO - Batch: 120/141, Loss: 1.7056, Perplexity: 5.5048, Time: 0.41s
2025-04-26 00:03:05,201 - INFO - Batch: 130/141, Loss: 1.6924, Perplexity: 5.4323, Time: 0.32s
2025-04-26 00:03:05,491 - INFO - Batch: 140/141, Loss: 1.7015, Perplexity: 5.4823, Time: 0.29s
2025-04-26 00:03:06,208 - INFO - Validation  Loss: 1.8772, Perplexity: 6.5350, Time: 0.68s
2025-04-26 00:03:06,208 - INFO - Epoch 223/300 - Train Loss: 1.7034, Val Loss: 1.8772
2025-04-26 00:03:06,580 - INFO - Batch: 10/141, Loss: 1.7010, Perplexity: 5.4795, Time: 0.37s
2025-04-26 00:03:06,885 - INFO - Batch: 20/141, Loss: 1.7040, Perplexity: 5.4957, Time: 0.30s
2025-04-26 00:03:07,243 - INFO - Batch: 30/141, Loss: 1.7014, Perplexity: 5.4815, Time: 0.36s
2025-04-26 00:03:07,518 - INFO - Batch: 40/141, Loss: 1.6928, Perplexity: 5.4349, Time: 0.27s
2025-04-26 00:03:07,835 - INFO - Batch: 50/141, Loss: 1.7089, Perplexity: 5.5227, Time: 0.32s
2025-04-26 00:03:08,240 - INFO - Batch: 60/141, Loss: 1.7054, Perplexity: 5.5036, Time: 0.40s
2025-04-26 00:03:08,604 - INFO - Batch: 70/141, Loss: 1.7051, Perplexity: 5.5021, Time: 0.36s
2025-04-26 00:03:08,996 - INFO - Batch: 80/141, Loss: 1.6990, Perplexity: 5.4686, Time: 0.39s
2025-04-26 00:03:09,247 - INFO - Batch: 90/141, Loss: 1.6968, Perplexity: 5.4562, Time: 0.25s
2025-04-26 00:03:09,494 - INFO - Batch: 100/141, Loss: 1.6981, Perplexity: 5.4633, Time: 0.25s
2025-04-26 00:03:09,743 - INFO - Batch: 110/141, Loss: 1.7145, Perplexity: 5.5537, Time: 0.25s
2025-04-26 00:03:09,997 - INFO - Batch: 120/141, Loss: 1.7102, Perplexity: 5.5299, Time: 0.25s
2025-04-26 00:03:10,249 - INFO - Batch: 130/141, Loss: 1.7047, Perplexity: 5.4998, Time: 0.25s
2025-04-26 00:03:10,497 - INFO - Batch: 140/141, Loss: 1.6906, Perplexity: 5.4225, Time: 0.25s
2025-04-26 00:03:11,402 - INFO - Validation  Loss: 1.8769, Perplexity: 6.5335, Time: 0.86s
2025-04-26 00:03:11,402 - INFO - Epoch 224/300 - Train Loss: 1.7034, Val Loss: 1.8769
2025-04-26 00:03:11,682 - INFO - Batch: 10/141, Loss: 1.7060, Perplexity: 5.5070, Time: 0.28s
2025-04-26 00:03:11,996 - INFO - Batch: 20/141, Loss: 1.7011, Perplexity: 5.4798, Time: 0.31s
2025-04-26 00:03:12,312 - INFO - Batch: 30/141, Loss: 1.6981, Perplexity: 5.4638, Time: 0.32s
2025-04-26 00:03:12,663 - INFO - Batch: 40/141, Loss: 1.7077, Perplexity: 5.5163, Time: 0.35s
2025-04-26 00:03:13,120 - INFO - Batch: 50/141, Loss: 1.7005, Perplexity: 5.4769, Time: 0.46s
2025-04-26 00:03:13,440 - INFO - Batch: 60/141, Loss: 1.6994, Perplexity: 5.4708, Time: 0.32s
2025-04-26 00:03:13,731 - INFO - Batch: 70/141, Loss: 1.7137, Perplexity: 5.5493, Time: 0.29s
2025-04-26 00:03:14,011 - INFO - Batch: 80/141, Loss: 1.7089, Perplexity: 5.5227, Time: 0.28s
2025-04-26 00:03:14,299 - INFO - Batch: 90/141, Loss: 1.6990, Perplexity: 5.4682, Time: 0.29s
2025-04-26 00:03:14,557 - INFO - Batch: 100/141, Loss: 1.7004, Perplexity: 5.4763, Time: 0.26s
2025-04-26 00:03:14,830 - INFO - Batch: 110/141, Loss: 1.7005, Perplexity: 5.4766, Time: 0.27s
2025-04-26 00:03:15,091 - INFO - Batch: 120/141, Loss: 1.7042, Perplexity: 5.4971, Time: 0.26s
2025-04-26 00:03:15,456 - INFO - Batch: 130/141, Loss: 1.6969, Perplexity: 5.4573, Time: 0.36s
2025-04-26 00:03:15,721 - INFO - Batch: 140/141, Loss: 1.7002, Perplexity: 5.4753, Time: 0.26s
2025-04-26 00:03:16,545 - INFO - Validation  Loss: 1.8768, Perplexity: 6.5326, Time: 0.78s
2025-04-26 00:03:16,545 - INFO - Epoch 225/300 - Train Loss: 1.7034, Val Loss: 1.8768
2025-04-26 00:03:16,881 - INFO - Batch: 10/141, Loss: 1.6968, Perplexity: 5.4567, Time: 0.34s
2025-04-26 00:03:17,179 - INFO - Batch: 20/141, Loss: 1.7126, Perplexity: 5.5436, Time: 0.30s
2025-04-26 00:03:17,444 - INFO - Batch: 30/141, Loss: 1.6978, Perplexity: 5.4617, Time: 0.26s
2025-04-26 00:03:17,709 - INFO - Batch: 40/141, Loss: 1.6929, Perplexity: 5.4351, Time: 0.26s
2025-04-26 00:03:17,988 - INFO - Batch: 50/141, Loss: 1.7212, Perplexity: 5.5913, Time: 0.28s
2025-04-26 00:03:18,256 - INFO - Batch: 60/141, Loss: 1.7196, Perplexity: 5.5825, Time: 0.27s
2025-04-26 00:03:18,512 - INFO - Batch: 70/141, Loss: 1.7139, Perplexity: 5.5504, Time: 0.26s
2025-04-26 00:03:18,783 - INFO - Batch: 80/141, Loss: 1.7049, Perplexity: 5.5006, Time: 0.27s
2025-04-26 00:03:19,109 - INFO - Batch: 90/141, Loss: 1.7078, Perplexity: 5.5167, Time: 0.33s
2025-04-26 00:03:19,428 - INFO - Batch: 100/141, Loss: 1.6962, Perplexity: 5.4531, Time: 0.32s
2025-04-26 00:03:19,899 - INFO - Batch: 110/141, Loss: 1.7152, Perplexity: 5.5580, Time: 0.47s
2025-04-26 00:03:20,249 - INFO - Batch: 120/141, Loss: 1.7149, Perplexity: 5.5561, Time: 0.35s
2025-04-26 00:03:20,574 - INFO - Batch: 130/141, Loss: 1.6994, Perplexity: 5.4709, Time: 0.32s
2025-04-26 00:03:20,872 - INFO - Batch: 140/141, Loss: 1.6845, Perplexity: 5.3898, Time: 0.30s
2025-04-26 00:03:21,932 - INFO - Validation  Loss: 1.8767, Perplexity: 6.5322, Time: 1.02s
2025-04-26 00:03:21,932 - INFO - Epoch 226/300 - Train Loss: 1.7033, Val Loss: 1.8767
2025-04-26 00:03:22,228 - INFO - Batch: 10/141, Loss: 1.7104, Perplexity: 5.5309, Time: 0.30s
2025-04-26 00:03:22,512 - INFO - Batch: 20/141, Loss: 1.7076, Perplexity: 5.5160, Time: 0.28s
2025-04-26 00:03:22,741 - INFO - Batch: 30/141, Loss: 1.7026, Perplexity: 5.4880, Time: 0.23s
2025-04-26 00:03:22,980 - INFO - Batch: 40/141, Loss: 1.6976, Perplexity: 5.4606, Time: 0.24s
2025-04-26 00:03:23,211 - INFO - Batch: 50/141, Loss: 1.6965, Perplexity: 5.4547, Time: 0.23s
2025-04-26 00:03:23,622 - INFO - Batch: 60/141, Loss: 1.7011, Perplexity: 5.4801, Time: 0.41s
2025-04-26 00:03:23,880 - INFO - Batch: 70/141, Loss: 1.7113, Perplexity: 5.5360, Time: 0.26s
2025-04-26 00:03:24,139 - INFO - Batch: 80/141, Loss: 1.7051, Perplexity: 5.5020, Time: 0.26s
2025-04-26 00:03:24,394 - INFO - Batch: 90/141, Loss: 1.7105, Perplexity: 5.5317, Time: 0.25s
2025-04-26 00:03:24,655 - INFO - Batch: 100/141, Loss: 1.7035, Perplexity: 5.4929, Time: 0.26s
2025-04-26 00:03:24,912 - INFO - Batch: 110/141, Loss: 1.7020, Perplexity: 5.4850, Time: 0.26s
2025-04-26 00:03:25,167 - INFO - Batch: 120/141, Loss: 1.6952, Perplexity: 5.4476, Time: 0.25s
2025-04-26 00:03:25,493 - INFO - Batch: 130/141, Loss: 1.6929, Perplexity: 5.4352, Time: 0.33s
2025-04-26 00:03:25,751 - INFO - Batch: 140/141, Loss: 1.6918, Perplexity: 5.4294, Time: 0.26s
2025-04-26 00:03:26,587 - INFO - Validation  Loss: 1.8760, Perplexity: 6.5272, Time: 0.79s
2025-04-26 00:03:26,587 - INFO - Epoch 227/300 - Train Loss: 1.7033, Val Loss: 1.8760
2025-04-26 00:03:26,996 - INFO - Batch: 10/141, Loss: 1.7031, Perplexity: 5.4907, Time: 0.41s
2025-04-26 00:03:27,596 - INFO - Batch: 20/141, Loss: 1.7120, Perplexity: 5.5399, Time: 0.60s
2025-04-26 00:03:27,935 - INFO - Batch: 30/141, Loss: 1.7072, Perplexity: 5.5133, Time: 0.34s
2025-04-26 00:03:28,298 - INFO - Batch: 40/141, Loss: 1.6951, Perplexity: 5.4470, Time: 0.36s
2025-04-26 00:03:28,654 - INFO - Batch: 50/141, Loss: 1.7033, Perplexity: 5.4922, Time: 0.36s
2025-04-26 00:03:29,036 - INFO - Batch: 60/141, Loss: 1.6985, Perplexity: 5.4660, Time: 0.38s
2025-04-26 00:03:29,376 - INFO - Batch: 70/141, Loss: 1.7105, Perplexity: 5.5317, Time: 0.34s
2025-04-26 00:03:29,658 - INFO - Batch: 80/141, Loss: 1.7060, Perplexity: 5.5066, Time: 0.28s
2025-04-26 00:03:30,061 - INFO - Batch: 90/141, Loss: 1.7074, Perplexity: 5.5147, Time: 0.40s
2025-04-26 00:03:30,448 - INFO - Batch: 100/141, Loss: 1.6835, Perplexity: 5.3843, Time: 0.39s
2025-04-26 00:03:30,707 - INFO - Batch: 110/141, Loss: 1.7099, Perplexity: 5.5282, Time: 0.26s
2025-04-26 00:03:31,022 - INFO - Batch: 120/141, Loss: 1.6863, Perplexity: 5.3994, Time: 0.31s
2025-04-26 00:03:31,269 - INFO - Batch: 130/141, Loss: 1.6970, Perplexity: 5.4576, Time: 0.25s
2025-04-26 00:03:31,604 - INFO - Batch: 140/141, Loss: 1.7097, Perplexity: 5.5271, Time: 0.34s
2025-04-26 00:03:32,531 - INFO - Validation  Loss: 1.8767, Perplexity: 6.5319, Time: 0.90s
2025-04-26 00:03:32,531 - INFO - Epoch 228/300 - Train Loss: 1.7032, Val Loss: 1.8767
2025-04-26 00:03:32,821 - INFO - Batch: 10/141, Loss: 1.7142, Perplexity: 5.5522, Time: 0.29s
2025-04-26 00:03:33,207 - INFO - Batch: 20/141, Loss: 1.6997, Perplexity: 5.4721, Time: 0.39s
2025-04-26 00:03:33,617 - INFO - Batch: 30/141, Loss: 1.6954, Perplexity: 5.4488, Time: 0.41s
2025-04-26 00:03:33,905 - INFO - Batch: 40/141, Loss: 1.6992, Perplexity: 5.4695, Time: 0.29s
2025-04-26 00:03:34,181 - INFO - Batch: 50/141, Loss: 1.7018, Perplexity: 5.4841, Time: 0.28s
2025-04-26 00:03:34,459 - INFO - Batch: 60/141, Loss: 1.7111, Perplexity: 5.5348, Time: 0.28s
2025-04-26 00:03:34,815 - INFO - Batch: 70/141, Loss: 1.7061, Perplexity: 5.5073, Time: 0.36s
2025-04-26 00:03:35,075 - INFO - Batch: 80/141, Loss: 1.6893, Perplexity: 5.4159, Time: 0.26s
2025-04-26 00:03:35,415 - INFO - Batch: 90/141, Loss: 1.6957, Perplexity: 5.4504, Time: 0.34s
2025-04-26 00:03:35,722 - INFO - Batch: 100/141, Loss: 1.7126, Perplexity: 5.5433, Time: 0.31s
2025-04-26 00:03:36,049 - INFO - Batch: 110/141, Loss: 1.7023, Perplexity: 5.4866, Time: 0.33s
2025-04-26 00:03:36,490 - INFO - Batch: 120/141, Loss: 1.6932, Perplexity: 5.4368, Time: 0.44s
2025-04-26 00:03:36,913 - INFO - Batch: 130/141, Loss: 1.7112, Perplexity: 5.5358, Time: 0.42s
2025-04-26 00:03:37,374 - INFO - Batch: 140/141, Loss: 1.6825, Perplexity: 5.3790, Time: 0.46s
2025-04-26 00:03:38,425 - INFO - Validation  Loss: 1.8763, Perplexity: 6.5295, Time: 1.01s
2025-04-26 00:03:38,425 - INFO - Epoch 229/300 - Train Loss: 1.7031, Val Loss: 1.8763
2025-04-26 00:03:38,819 - INFO - Batch: 10/141, Loss: 1.7079, Perplexity: 5.5173, Time: 0.39s
2025-04-26 00:03:39,165 - INFO - Batch: 20/141, Loss: 1.7092, Perplexity: 5.5244, Time: 0.35s
2025-04-26 00:03:39,688 - INFO - Batch: 30/141, Loss: 1.7073, Perplexity: 5.5141, Time: 0.52s
2025-04-26 00:03:40,032 - INFO - Batch: 40/141, Loss: 1.7074, Perplexity: 5.5143, Time: 0.34s
2025-04-26 00:03:40,418 - INFO - Batch: 50/141, Loss: 1.6990, Perplexity: 5.4685, Time: 0.39s
2025-04-26 00:03:40,809 - INFO - Batch: 60/141, Loss: 1.7135, Perplexity: 5.5483, Time: 0.39s
2025-04-26 00:03:41,227 - INFO - Batch: 70/141, Loss: 1.7179, Perplexity: 5.5729, Time: 0.42s
2025-04-26 00:03:41,611 - INFO - Batch: 80/141, Loss: 1.7071, Perplexity: 5.5128, Time: 0.38s
2025-04-26 00:03:41,945 - INFO - Batch: 90/141, Loss: 1.7082, Perplexity: 5.5189, Time: 0.33s
2025-04-26 00:03:42,293 - INFO - Batch: 100/141, Loss: 1.7018, Perplexity: 5.4839, Time: 0.35s
2025-04-26 00:03:42,661 - INFO - Batch: 110/141, Loss: 1.6933, Perplexity: 5.4372, Time: 0.37s
2025-04-26 00:03:42,932 - INFO - Batch: 120/141, Loss: 1.7164, Perplexity: 5.5646, Time: 0.27s
2025-04-26 00:03:43,186 - INFO - Batch: 130/141, Loss: 1.6889, Perplexity: 5.4136, Time: 0.25s
2025-04-26 00:03:43,444 - INFO - Batch: 140/141, Loss: 1.7039, Perplexity: 5.4954, Time: 0.26s
2025-04-26 00:03:44,234 - INFO - Validation  Loss: 1.8774, Perplexity: 6.5367, Time: 0.76s
2025-04-26 00:03:44,234 - INFO - Epoch 230/300 - Train Loss: 1.7031, Val Loss: 1.8774
2025-04-26 00:03:44,669 - INFO - Batch: 10/141, Loss: 1.6982, Perplexity: 5.4640, Time: 0.43s
2025-04-26 00:03:44,948 - INFO - Batch: 20/141, Loss: 1.7117, Perplexity: 5.5385, Time: 0.28s
2025-04-26 00:03:45,276 - INFO - Batch: 30/141, Loss: 1.7078, Perplexity: 5.5169, Time: 0.33s
2025-04-26 00:03:45,592 - INFO - Batch: 40/141, Loss: 1.7040, Perplexity: 5.4959, Time: 0.32s
2025-04-26 00:03:45,933 - INFO - Batch: 50/141, Loss: 1.7037, Perplexity: 5.4945, Time: 0.34s
2025-04-26 00:03:46,214 - INFO - Batch: 60/141, Loss: 1.7042, Perplexity: 5.4971, Time: 0.28s
2025-04-26 00:03:46,510 - INFO - Batch: 70/141, Loss: 1.7011, Perplexity: 5.4802, Time: 0.30s
2025-04-26 00:03:46,857 - INFO - Batch: 80/141, Loss: 1.6946, Perplexity: 5.4442, Time: 0.35s
2025-04-26 00:03:47,143 - INFO - Batch: 90/141, Loss: 1.6967, Perplexity: 5.4558, Time: 0.29s
2025-04-26 00:03:47,529 - INFO - Batch: 100/141, Loss: 1.7267, Perplexity: 5.6219, Time: 0.39s
2025-04-26 00:03:47,900 - INFO - Batch: 110/141, Loss: 1.7115, Perplexity: 5.5373, Time: 0.37s
2025-04-26 00:03:48,212 - INFO - Batch: 120/141, Loss: 1.7010, Perplexity: 5.4793, Time: 0.31s
2025-04-26 00:03:48,550 - INFO - Batch: 130/141, Loss: 1.6857, Perplexity: 5.3962, Time: 0.34s
2025-04-26 00:03:48,873 - INFO - Batch: 140/141, Loss: 1.7029, Perplexity: 5.4897, Time: 0.32s
2025-04-26 00:03:49,962 - INFO - Validation  Loss: 1.8764, Perplexity: 6.5300, Time: 1.06s
2025-04-26 00:03:49,962 - INFO - Epoch 231/300 - Train Loss: 1.7030, Val Loss: 1.8764
2025-04-26 00:03:50,422 - INFO - Batch: 10/141, Loss: 1.7213, Perplexity: 5.5916, Time: 0.46s
2025-04-26 00:03:50,845 - INFO - Batch: 20/141, Loss: 1.7092, Perplexity: 5.5247, Time: 0.42s
2025-04-26 00:03:51,143 - INFO - Batch: 30/141, Loss: 1.7015, Perplexity: 5.4823, Time: 0.30s
2025-04-26 00:03:51,623 - INFO - Batch: 40/141, Loss: 1.7121, Perplexity: 5.5406, Time: 0.48s
2025-04-26 00:03:52,000 - INFO - Batch: 50/141, Loss: 1.7066, Perplexity: 5.5101, Time: 0.38s
2025-04-26 00:03:52,373 - INFO - Batch: 60/141, Loss: 1.7032, Perplexity: 5.4915, Time: 0.37s
2025-04-26 00:03:52,721 - INFO - Batch: 70/141, Loss: 1.7015, Perplexity: 5.4821, Time: 0.35s
2025-04-26 00:03:53,125 - INFO - Batch: 80/141, Loss: 1.7060, Perplexity: 5.5067, Time: 0.40s
2025-04-26 00:03:53,457 - INFO - Batch: 90/141, Loss: 1.7129, Perplexity: 5.5452, Time: 0.33s
2025-04-26 00:03:53,808 - INFO - Batch: 100/141, Loss: 1.7029, Perplexity: 5.4898, Time: 0.35s
2025-04-26 00:03:54,231 - INFO - Batch: 110/141, Loss: 1.7110, Perplexity: 5.5343, Time: 0.42s
2025-04-26 00:03:54,790 - INFO - Batch: 120/141, Loss: 1.7135, Perplexity: 5.5483, Time: 0.56s
2025-04-26 00:03:55,268 - INFO - Batch: 130/141, Loss: 1.7024, Perplexity: 5.4874, Time: 0.48s
2025-04-26 00:03:55,537 - INFO - Batch: 140/141, Loss: 1.6965, Perplexity: 5.4550, Time: 0.27s
2025-04-26 00:03:56,516 - INFO - Validation  Loss: 1.8761, Perplexity: 6.5278, Time: 0.93s
2025-04-26 00:03:56,516 - INFO - Epoch 232/300 - Train Loss: 1.7030, Val Loss: 1.8761
2025-04-26 00:03:56,920 - INFO - Batch: 10/141, Loss: 1.7120, Perplexity: 5.5401, Time: 0.40s
2025-04-26 00:03:57,158 - INFO - Batch: 20/141, Loss: 1.7160, Perplexity: 5.5621, Time: 0.24s
2025-04-26 00:03:57,400 - INFO - Batch: 30/141, Loss: 1.7015, Perplexity: 5.4820, Time: 0.24s
2025-04-26 00:03:57,621 - INFO - Batch: 40/141, Loss: 1.7141, Perplexity: 5.5515, Time: 0.22s
2025-04-26 00:03:57,859 - INFO - Batch: 50/141, Loss: 1.6987, Perplexity: 5.4669, Time: 0.24s
2025-04-26 00:03:58,108 - INFO - Batch: 60/141, Loss: 1.7076, Perplexity: 5.5157, Time: 0.25s
2025-04-26 00:03:58,329 - INFO - Batch: 70/141, Loss: 1.7219, Perplexity: 5.5951, Time: 0.22s
2025-04-26 00:03:58,653 - INFO - Batch: 80/141, Loss: 1.6982, Perplexity: 5.4643, Time: 0.32s
2025-04-26 00:03:58,949 - INFO - Batch: 90/141, Loss: 1.6957, Perplexity: 5.4503, Time: 0.30s
2025-04-26 00:03:59,316 - INFO - Batch: 100/141, Loss: 1.7023, Perplexity: 5.4866, Time: 0.37s
2025-04-26 00:03:59,571 - INFO - Batch: 110/141, Loss: 1.6970, Perplexity: 5.4578, Time: 0.25s
2025-04-26 00:03:59,823 - INFO - Batch: 120/141, Loss: 1.7007, Perplexity: 5.4775, Time: 0.25s
2025-04-26 00:04:00,053 - INFO - Batch: 130/141, Loss: 1.6987, Perplexity: 5.4666, Time: 0.23s
2025-04-26 00:04:00,286 - INFO - Batch: 140/141, Loss: 1.7124, Perplexity: 5.5424, Time: 0.23s
2025-04-26 00:04:00,995 - INFO - Validation  Loss: 1.8768, Perplexity: 6.5323, Time: 0.68s
2025-04-26 00:04:00,995 - INFO - Epoch 233/300 - Train Loss: 1.7030, Val Loss: 1.8768
2025-04-26 00:04:01,234 - INFO - Batch: 10/141, Loss: 1.7208, Perplexity: 5.5889, Time: 0.24s
2025-04-26 00:04:01,483 - INFO - Batch: 20/141, Loss: 1.7011, Perplexity: 5.4800, Time: 0.25s
2025-04-26 00:04:01,731 - INFO - Batch: 30/141, Loss: 1.7067, Perplexity: 5.5109, Time: 0.25s
2025-04-26 00:04:01,998 - INFO - Batch: 40/141, Loss: 1.7043, Perplexity: 5.4976, Time: 0.27s
2025-04-26 00:04:02,333 - INFO - Batch: 50/141, Loss: 1.7029, Perplexity: 5.4899, Time: 0.33s
2025-04-26 00:04:02,606 - INFO - Batch: 60/141, Loss: 1.7028, Perplexity: 5.4894, Time: 0.27s
2025-04-26 00:04:02,865 - INFO - Batch: 70/141, Loss: 1.6896, Perplexity: 5.4172, Time: 0.26s
2025-04-26 00:04:03,147 - INFO - Batch: 80/141, Loss: 1.6948, Perplexity: 5.4455, Time: 0.28s
2025-04-26 00:04:03,427 - INFO - Batch: 90/141, Loss: 1.7070, Perplexity: 5.5125, Time: 0.28s
2025-04-26 00:04:03,699 - INFO - Batch: 100/141, Loss: 1.6965, Perplexity: 5.4547, Time: 0.27s
2025-04-26 00:04:04,071 - INFO - Batch: 110/141, Loss: 1.7065, Perplexity: 5.5098, Time: 0.37s
2025-04-26 00:04:04,381 - INFO - Batch: 120/141, Loss: 1.6979, Perplexity: 5.4625, Time: 0.31s
2025-04-26 00:04:04,724 - INFO - Batch: 130/141, Loss: 1.7078, Perplexity: 5.5170, Time: 0.34s
2025-04-26 00:04:05,000 - INFO - Batch: 140/141, Loss: 1.7109, Perplexity: 5.5341, Time: 0.28s
2025-04-26 00:04:05,713 - INFO - Validation  Loss: 1.8775, Perplexity: 6.5370, Time: 0.67s
2025-04-26 00:04:05,713 - INFO - Epoch 234/300 - Train Loss: 1.7029, Val Loss: 1.8775
2025-04-26 00:04:05,966 - INFO - Batch: 10/141, Loss: 1.7009, Perplexity: 5.4788, Time: 0.25s
2025-04-26 00:04:06,267 - INFO - Batch: 20/141, Loss: 1.7100, Perplexity: 5.5290, Time: 0.30s
2025-04-26 00:04:06,520 - INFO - Batch: 30/141, Loss: 1.6939, Perplexity: 5.4406, Time: 0.25s
2025-04-26 00:04:06,801 - INFO - Batch: 40/141, Loss: 1.7050, Perplexity: 5.5011, Time: 0.28s
2025-04-26 00:04:07,037 - INFO - Batch: 50/141, Loss: 1.7140, Perplexity: 5.5509, Time: 0.24s
2025-04-26 00:04:07,274 - INFO - Batch: 60/141, Loss: 1.7109, Perplexity: 5.5340, Time: 0.24s
2025-04-26 00:04:07,537 - INFO - Batch: 70/141, Loss: 1.7062, Perplexity: 5.5077, Time: 0.26s
2025-04-26 00:04:07,863 - INFO - Batch: 80/141, Loss: 1.7072, Perplexity: 5.5137, Time: 0.33s
2025-04-26 00:04:08,176 - INFO - Batch: 90/141, Loss: 1.6898, Perplexity: 5.4183, Time: 0.31s
2025-04-26 00:04:08,457 - INFO - Batch: 100/141, Loss: 1.7024, Perplexity: 5.4872, Time: 0.28s
2025-04-26 00:04:08,703 - INFO - Batch: 110/141, Loss: 1.6934, Perplexity: 5.4380, Time: 0.25s
2025-04-26 00:04:08,949 - INFO - Batch: 120/141, Loss: 1.7051, Perplexity: 5.5022, Time: 0.25s
2025-04-26 00:04:09,211 - INFO - Batch: 130/141, Loss: 1.6865, Perplexity: 5.4005, Time: 0.26s
2025-04-26 00:04:09,443 - INFO - Batch: 140/141, Loss: 1.7017, Perplexity: 5.4830, Time: 0.23s
2025-04-26 00:04:10,156 - INFO - Validation  Loss: 1.8767, Perplexity: 6.5321, Time: 0.68s
2025-04-26 00:04:10,156 - INFO - Epoch 235/300 - Train Loss: 1.7029, Val Loss: 1.8767
2025-04-26 00:04:10,392 - INFO - Batch: 10/141, Loss: 1.6929, Perplexity: 5.4351, Time: 0.24s
2025-04-26 00:04:10,619 - INFO - Batch: 20/141, Loss: 1.7180, Perplexity: 5.5733, Time: 0.23s
2025-04-26 00:04:10,850 - INFO - Batch: 30/141, Loss: 1.7024, Perplexity: 5.4869, Time: 0.23s
2025-04-26 00:04:11,092 - INFO - Batch: 40/141, Loss: 1.6916, Perplexity: 5.4282, Time: 0.24s
2025-04-26 00:04:11,336 - INFO - Batch: 50/141, Loss: 1.7037, Perplexity: 5.4944, Time: 0.24s
2025-04-26 00:04:11,633 - INFO - Batch: 60/141, Loss: 1.6921, Perplexity: 5.4311, Time: 0.30s
2025-04-26 00:04:11,894 - INFO - Batch: 70/141, Loss: 1.7144, Perplexity: 5.5536, Time: 0.26s
2025-04-26 00:04:12,179 - INFO - Batch: 80/141, Loss: 1.7054, Perplexity: 5.5035, Time: 0.28s
2025-04-26 00:04:12,413 - INFO - Batch: 90/141, Loss: 1.7090, Perplexity: 5.5232, Time: 0.23s
2025-04-26 00:04:12,644 - INFO - Batch: 100/141, Loss: 1.7086, Perplexity: 5.5210, Time: 0.23s
2025-04-26 00:04:12,874 - INFO - Batch: 110/141, Loss: 1.7093, Perplexity: 5.5250, Time: 0.23s
2025-04-26 00:04:13,107 - INFO - Batch: 120/141, Loss: 1.7032, Perplexity: 5.4914, Time: 0.23s
2025-04-26 00:04:13,357 - INFO - Batch: 130/141, Loss: 1.7087, Perplexity: 5.5216, Time: 0.25s
2025-04-26 00:04:13,644 - INFO - Batch: 140/141, Loss: 1.7099, Perplexity: 5.5283, Time: 0.29s
2025-04-26 00:04:14,304 - INFO - Validation  Loss: 1.8767, Perplexity: 6.5318, Time: 0.63s
2025-04-26 00:04:14,304 - INFO - Epoch 236/300 - Train Loss: 1.7028, Val Loss: 1.8767
2025-04-26 00:04:14,538 - INFO - Batch: 10/141, Loss: 1.6983, Perplexity: 5.4649, Time: 0.23s
2025-04-26 00:04:14,765 - INFO - Batch: 20/141, Loss: 1.7121, Perplexity: 5.5407, Time: 0.23s
2025-04-26 00:04:15,056 - INFO - Batch: 30/141, Loss: 1.7024, Perplexity: 5.4869, Time: 0.29s
2025-04-26 00:04:15,302 - INFO - Batch: 40/141, Loss: 1.6904, Perplexity: 5.4216, Time: 0.25s
2025-04-26 00:04:15,532 - INFO - Batch: 50/141, Loss: 1.7053, Perplexity: 5.5032, Time: 0.23s
2025-04-26 00:04:15,762 - INFO - Batch: 60/141, Loss: 1.7000, Perplexity: 5.4738, Time: 0.23s
2025-04-26 00:04:15,994 - INFO - Batch: 70/141, Loss: 1.6901, Perplexity: 5.4198, Time: 0.23s
2025-04-26 00:04:16,224 - INFO - Batch: 80/141, Loss: 1.7068, Perplexity: 5.5111, Time: 0.23s
2025-04-26 00:04:16,500 - INFO - Batch: 90/141, Loss: 1.6997, Perplexity: 5.4723, Time: 0.28s
2025-04-26 00:04:16,793 - INFO - Batch: 100/141, Loss: 1.7085, Perplexity: 5.5209, Time: 0.29s
2025-04-26 00:04:17,064 - INFO - Batch: 110/141, Loss: 1.7065, Perplexity: 5.5096, Time: 0.27s
2025-04-26 00:04:17,328 - INFO - Batch: 120/141, Loss: 1.7170, Perplexity: 5.5678, Time: 0.26s
2025-04-26 00:04:17,560 - INFO - Batch: 130/141, Loss: 1.6896, Perplexity: 5.4172, Time: 0.23s
2025-04-26 00:04:17,846 - INFO - Batch: 140/141, Loss: 1.7163, Perplexity: 5.5636, Time: 0.29s
2025-04-26 00:04:18,542 - INFO - Validation  Loss: 1.8761, Perplexity: 6.5277, Time: 0.66s
2025-04-26 00:04:18,542 - INFO - Epoch 237/300 - Train Loss: 1.7028, Val Loss: 1.8761
2025-04-26 00:04:18,776 - INFO - Batch: 10/141, Loss: 1.7003, Perplexity: 5.4754, Time: 0.23s
2025-04-26 00:04:19,006 - INFO - Batch: 20/141, Loss: 1.7025, Perplexity: 5.4877, Time: 0.23s
2025-04-26 00:04:19,240 - INFO - Batch: 30/141, Loss: 1.6953, Perplexity: 5.4481, Time: 0.23s
2025-04-26 00:04:19,471 - INFO - Batch: 40/141, Loss: 1.6979, Perplexity: 5.4624, Time: 0.23s
2025-04-26 00:04:19,695 - INFO - Batch: 50/141, Loss: 1.7016, Perplexity: 5.4827, Time: 0.22s
2025-04-26 00:04:19,927 - INFO - Batch: 60/141, Loss: 1.6915, Perplexity: 5.4278, Time: 0.23s
2025-04-26 00:04:20,219 - INFO - Batch: 70/141, Loss: 1.6936, Perplexity: 5.4390, Time: 0.29s
2025-04-26 00:04:20,447 - INFO - Batch: 80/141, Loss: 1.7011, Perplexity: 5.4802, Time: 0.23s
2025-04-26 00:04:20,683 - INFO - Batch: 90/141, Loss: 1.6842, Perplexity: 5.3882, Time: 0.24s
2025-04-26 00:04:20,919 - INFO - Batch: 100/141, Loss: 1.6893, Perplexity: 5.4158, Time: 0.24s
2025-04-26 00:04:21,171 - INFO - Batch: 110/141, Loss: 1.6950, Perplexity: 5.4469, Time: 0.25s
2025-04-26 00:04:21,407 - INFO - Batch: 120/141, Loss: 1.7002, Perplexity: 5.4752, Time: 0.24s
2025-04-26 00:04:21,635 - INFO - Batch: 130/141, Loss: 1.7079, Perplexity: 5.5171, Time: 0.23s
2025-04-26 00:04:21,874 - INFO - Batch: 140/141, Loss: 1.7022, Perplexity: 5.4862, Time: 0.24s
2025-04-26 00:04:22,576 - INFO - Validation  Loss: 1.8759, Perplexity: 6.5269, Time: 0.61s
2025-04-26 00:04:22,577 - INFO - Epoch 238/300 - Train Loss: 1.7027, Val Loss: 1.8759
2025-04-26 00:04:22,877 - INFO - Batch: 10/141, Loss: 1.7003, Perplexity: 5.4756, Time: 0.30s
2025-04-26 00:04:23,113 - INFO - Batch: 20/141, Loss: 1.7068, Perplexity: 5.5111, Time: 0.24s
2025-04-26 00:04:23,416 - INFO - Batch: 30/141, Loss: 1.7023, Perplexity: 5.4866, Time: 0.30s
2025-04-26 00:04:23,685 - INFO - Batch: 40/141, Loss: 1.6862, Perplexity: 5.3988, Time: 0.27s
2025-04-26 00:04:23,908 - INFO - Batch: 50/141, Loss: 1.6874, Perplexity: 5.4057, Time: 0.22s
2025-04-26 00:04:24,138 - INFO - Batch: 60/141, Loss: 1.7066, Perplexity: 5.5102, Time: 0.23s
2025-04-26 00:04:24,370 - INFO - Batch: 70/141, Loss: 1.6929, Perplexity: 5.4351, Time: 0.23s
2025-04-26 00:04:24,599 - INFO - Batch: 80/141, Loss: 1.7063, Perplexity: 5.5087, Time: 0.23s
2025-04-26 00:04:24,835 - INFO - Batch: 90/141, Loss: 1.6818, Perplexity: 5.3751, Time: 0.23s
2025-04-26 00:04:25,101 - INFO - Batch: 100/141, Loss: 1.6982, Perplexity: 5.4640, Time: 0.27s
2025-04-26 00:04:25,403 - INFO - Batch: 110/141, Loss: 1.6996, Perplexity: 5.4720, Time: 0.30s
2025-04-26 00:04:25,632 - INFO - Batch: 120/141, Loss: 1.7029, Perplexity: 5.4900, Time: 0.23s
2025-04-26 00:04:25,865 - INFO - Batch: 130/141, Loss: 1.6949, Perplexity: 5.4464, Time: 0.23s
2025-04-26 00:04:26,098 - INFO - Batch: 140/141, Loss: 1.6944, Perplexity: 5.4435, Time: 0.23s
2025-04-26 00:04:26,798 - INFO - Validation  Loss: 1.8767, Perplexity: 6.5321, Time: 0.67s
2025-04-26 00:04:26,798 - INFO - Epoch 239/300 - Train Loss: 1.7027, Val Loss: 1.8767
2025-04-26 00:04:27,031 - INFO - Batch: 10/141, Loss: 1.6920, Perplexity: 5.4305, Time: 0.23s
2025-04-26 00:04:27,265 - INFO - Batch: 20/141, Loss: 1.7106, Perplexity: 5.5323, Time: 0.23s
2025-04-26 00:04:27,504 - INFO - Batch: 30/141, Loss: 1.7017, Perplexity: 5.4834, Time: 0.24s
2025-04-26 00:04:27,736 - INFO - Batch: 40/141, Loss: 1.6995, Perplexity: 5.4711, Time: 0.23s
2025-04-26 00:04:27,967 - INFO - Batch: 50/141, Loss: 1.6967, Perplexity: 5.4561, Time: 0.23s
2025-04-26 00:04:28,193 - INFO - Batch: 60/141, Loss: 1.7040, Perplexity: 5.4958, Time: 0.23s
2025-04-26 00:04:28,424 - INFO - Batch: 70/141, Loss: 1.6969, Perplexity: 5.4568, Time: 0.23s
2025-04-26 00:04:28,709 - INFO - Batch: 80/141, Loss: 1.7108, Perplexity: 5.5333, Time: 0.28s
2025-04-26 00:04:28,937 - INFO - Batch: 90/141, Loss: 1.6885, Perplexity: 5.4115, Time: 0.23s
2025-04-26 00:04:29,171 - INFO - Batch: 100/141, Loss: 1.7169, Perplexity: 5.5674, Time: 0.23s
2025-04-26 00:04:29,401 - INFO - Batch: 110/141, Loss: 1.7030, Perplexity: 5.4903, Time: 0.23s
2025-04-26 00:04:29,631 - INFO - Batch: 120/141, Loss: 1.6955, Perplexity: 5.4494, Time: 0.23s
2025-04-26 00:04:29,855 - INFO - Batch: 130/141, Loss: 1.7019, Perplexity: 5.4841, Time: 0.22s
2025-04-26 00:04:30,092 - INFO - Batch: 140/141, Loss: 1.7031, Perplexity: 5.4908, Time: 0.24s
2025-04-26 00:04:30,756 - INFO - Validation  Loss: 1.8762, Perplexity: 6.5285, Time: 0.64s
2025-04-26 00:04:30,756 - INFO - Epoch 240/300 - Train Loss: 1.7027, Val Loss: 1.8762
2025-04-26 00:04:30,985 - INFO - Batch: 10/141, Loss: 1.7112, Perplexity: 5.5355, Time: 0.23s
2025-04-26 00:04:31,227 - INFO - Batch: 20/141, Loss: 1.7050, Perplexity: 5.5015, Time: 0.24s
2025-04-26 00:04:31,466 - INFO - Batch: 30/141, Loss: 1.7034, Perplexity: 5.4925, Time: 0.24s
2025-04-26 00:04:31,755 - INFO - Batch: 40/141, Loss: 1.6953, Perplexity: 5.4483, Time: 0.29s
2025-04-26 00:04:31,981 - INFO - Batch: 50/141, Loss: 1.7030, Perplexity: 5.4904, Time: 0.23s
2025-04-26 00:04:32,211 - INFO - Batch: 60/141, Loss: 1.7136, Perplexity: 5.5488, Time: 0.23s
2025-04-26 00:04:32,440 - INFO - Batch: 70/141, Loss: 1.6929, Perplexity: 5.4353, Time: 0.23s
2025-04-26 00:04:32,671 - INFO - Batch: 80/141, Loss: 1.7119, Perplexity: 5.5397, Time: 0.23s
2025-04-26 00:04:32,900 - INFO - Batch: 90/141, Loss: 1.6983, Perplexity: 5.4646, Time: 0.23s
2025-04-26 00:04:33,175 - INFO - Batch: 100/141, Loss: 1.7040, Perplexity: 5.4958, Time: 0.27s
2025-04-26 00:04:33,412 - INFO - Batch: 110/141, Loss: 1.6971, Perplexity: 5.4582, Time: 0.24s
2025-04-26 00:04:33,713 - INFO - Batch: 120/141, Loss: 1.7055, Perplexity: 5.5044, Time: 0.30s
2025-04-26 00:04:33,945 - INFO - Batch: 130/141, Loss: 1.6970, Perplexity: 5.4576, Time: 0.23s
2025-04-26 00:04:34,179 - INFO - Batch: 140/141, Loss: 1.7132, Perplexity: 5.5466, Time: 0.23s
2025-04-26 00:04:34,779 - INFO - Validation  Loss: 1.8771, Perplexity: 6.5343, Time: 0.57s
2025-04-26 00:04:34,779 - INFO - Epoch 241/300 - Train Loss: 1.7026, Val Loss: 1.8771
2025-04-26 00:04:35,075 - INFO - Batch: 10/141, Loss: 1.7021, Perplexity: 5.4854, Time: 0.30s
2025-04-26 00:04:35,283 - INFO - Batch: 20/141, Loss: 1.7105, Perplexity: 5.5315, Time: 0.21s
2025-04-26 00:04:35,504 - INFO - Batch: 30/141, Loss: 1.7067, Perplexity: 5.5109, Time: 0.22s
2025-04-26 00:04:35,718 - INFO - Batch: 40/141, Loss: 1.6864, Perplexity: 5.4001, Time: 0.21s
2025-04-26 00:04:35,988 - INFO - Batch: 50/141, Loss: 1.6828, Perplexity: 5.3808, Time: 0.27s
2025-04-26 00:04:36,210 - INFO - Batch: 60/141, Loss: 1.7186, Perplexity: 5.5766, Time: 0.22s
2025-04-26 00:04:36,423 - INFO - Batch: 70/141, Loss: 1.6998, Perplexity: 5.4727, Time: 0.21s
2025-04-26 00:04:36,632 - INFO - Batch: 80/141, Loss: 1.7148, Perplexity: 5.5553, Time: 0.21s
2025-04-26 00:04:36,958 - INFO - Batch: 90/141, Loss: 1.7101, Perplexity: 5.5294, Time: 0.33s
2025-04-26 00:04:37,172 - INFO - Batch: 100/141, Loss: 1.7059, Perplexity: 5.5064, Time: 0.21s
2025-04-26 00:04:37,388 - INFO - Batch: 110/141, Loss: 1.6989, Perplexity: 5.4681, Time: 0.22s
2025-04-26 00:04:37,613 - INFO - Batch: 120/141, Loss: 1.7035, Perplexity: 5.4932, Time: 0.22s
2025-04-26 00:04:37,821 - INFO - Batch: 130/141, Loss: 1.6974, Perplexity: 5.4596, Time: 0.21s
2025-04-26 00:04:38,038 - INFO - Batch: 140/141, Loss: 1.6949, Perplexity: 5.4461, Time: 0.22s
2025-04-26 00:04:38,663 - INFO - Validation  Loss: 1.8770, Perplexity: 6.5336, Time: 0.60s
2025-04-26 00:04:38,663 - INFO - Epoch 242/300 - Train Loss: 1.7026, Val Loss: 1.8770
2025-04-26 00:04:38,875 - INFO - Batch: 10/141, Loss: 1.7128, Perplexity: 5.5445, Time: 0.21s
2025-04-26 00:04:39,092 - INFO - Batch: 20/141, Loss: 1.7019, Perplexity: 5.4842, Time: 0.22s
2025-04-26 00:04:39,301 - INFO - Batch: 30/141, Loss: 1.7080, Perplexity: 5.5178, Time: 0.21s
2025-04-26 00:04:39,516 - INFO - Batch: 40/141, Loss: 1.7059, Perplexity: 5.5061, Time: 0.22s
2025-04-26 00:04:39,840 - INFO - Batch: 50/141, Loss: 1.7094, Perplexity: 5.5258, Time: 0.32s
2025-04-26 00:04:40,054 - INFO - Batch: 60/141, Loss: 1.7090, Perplexity: 5.5237, Time: 0.21s
2025-04-26 00:04:40,274 - INFO - Batch: 70/141, Loss: 1.7126, Perplexity: 5.5435, Time: 0.22s
2025-04-26 00:04:40,486 - INFO - Batch: 80/141, Loss: 1.6979, Perplexity: 5.4624, Time: 0.21s
2025-04-26 00:04:40,709 - INFO - Batch: 90/141, Loss: 1.7100, Perplexity: 5.5290, Time: 0.22s
2025-04-26 00:04:40,932 - INFO - Batch: 100/141, Loss: 1.6995, Perplexity: 5.4711, Time: 0.22s
2025-04-26 00:04:41,262 - INFO - Batch: 110/141, Loss: 1.7097, Perplexity: 5.5272, Time: 0.33s
2025-04-26 00:04:41,490 - INFO - Batch: 120/141, Loss: 1.6912, Perplexity: 5.4259, Time: 0.23s
2025-04-26 00:04:41,779 - INFO - Batch: 130/141, Loss: 1.6995, Perplexity: 5.4715, Time: 0.29s
2025-04-26 00:04:41,999 - INFO - Batch: 140/141, Loss: 1.7105, Perplexity: 5.5318, Time: 0.22s
2025-04-26 00:04:42,582 - INFO - Validation  Loss: 1.8777, Perplexity: 6.5386, Time: 0.56s
2025-04-26 00:04:42,582 - INFO - Epoch 243/300 - Train Loss: 1.7026, Val Loss: 1.8777
2025-04-26 00:04:42,794 - INFO - Batch: 10/141, Loss: 1.6947, Perplexity: 5.4448, Time: 0.21s
2025-04-26 00:04:43,066 - INFO - Batch: 20/141, Loss: 1.7047, Perplexity: 5.4995, Time: 0.27s
2025-04-26 00:04:43,282 - INFO - Batch: 30/141, Loss: 1.7127, Perplexity: 5.5440, Time: 0.22s
2025-04-26 00:04:43,499 - INFO - Batch: 40/141, Loss: 1.7013, Perplexity: 5.4813, Time: 0.22s
2025-04-26 00:04:43,720 - INFO - Batch: 50/141, Loss: 1.7058, Perplexity: 5.5057, Time: 0.22s
2025-04-26 00:04:43,930 - INFO - Batch: 60/141, Loss: 1.7142, Perplexity: 5.5522, Time: 0.21s
2025-04-26 00:04:44,136 - INFO - Batch: 70/141, Loss: 1.7014, Perplexity: 5.4818, Time: 0.21s
2025-04-26 00:04:44,356 - INFO - Batch: 80/141, Loss: 1.7026, Perplexity: 5.4880, Time: 0.22s
2025-04-26 00:04:44,566 - INFO - Batch: 90/141, Loss: 1.7018, Perplexity: 5.4837, Time: 0.21s
2025-04-26 00:04:44,837 - INFO - Batch: 100/141, Loss: 1.7051, Perplexity: 5.5019, Time: 0.27s
2025-04-26 00:04:45,056 - INFO - Batch: 110/141, Loss: 1.7055, Perplexity: 5.5042, Time: 0.22s
2025-04-26 00:04:45,380 - INFO - Batch: 120/141, Loss: 1.6939, Perplexity: 5.4408, Time: 0.32s
2025-04-26 00:04:45,605 - INFO - Batch: 130/141, Loss: 1.7108, Perplexity: 5.5334, Time: 0.22s
2025-04-26 00:04:45,843 - INFO - Batch: 140/141, Loss: 1.7040, Perplexity: 5.4959, Time: 0.24s
2025-04-26 00:04:46,719 - INFO - Validation  Loss: 1.8756, Perplexity: 6.5250, Time: 0.84s
2025-04-26 00:04:46,720 - INFO - Epoch 244/300 - Train Loss: 1.7025, Val Loss: 1.8756
2025-04-26 00:04:47,026 - INFO - Batch: 10/141, Loss: 1.7065, Perplexity: 5.5097, Time: 0.31s
2025-04-26 00:04:47,304 - INFO - Batch: 20/141, Loss: 1.6929, Perplexity: 5.4353, Time: 0.28s
2025-04-26 00:04:47,582 - INFO - Batch: 30/141, Loss: 1.7065, Perplexity: 5.5097, Time: 0.28s
2025-04-26 00:04:47,848 - INFO - Batch: 40/141, Loss: 1.6922, Perplexity: 5.4313, Time: 0.27s
2025-04-26 00:04:48,100 - INFO - Batch: 50/141, Loss: 1.7124, Perplexity: 5.5420, Time: 0.25s
2025-04-26 00:04:48,408 - INFO - Batch: 60/141, Loss: 1.7060, Perplexity: 5.5069, Time: 0.31s
2025-04-26 00:04:48,646 - INFO - Batch: 70/141, Loss: 1.7154, Perplexity: 5.5589, Time: 0.24s
2025-04-26 00:04:48,919 - INFO - Batch: 80/141, Loss: 1.6947, Perplexity: 5.4449, Time: 0.27s
2025-04-26 00:04:49,156 - INFO - Batch: 90/141, Loss: 1.7062, Perplexity: 5.5082, Time: 0.24s
2025-04-26 00:04:49,411 - INFO - Batch: 100/141, Loss: 1.7059, Perplexity: 5.5065, Time: 0.25s
2025-04-26 00:04:49,654 - INFO - Batch: 110/141, Loss: 1.7057, Perplexity: 5.5051, Time: 0.24s
2025-04-26 00:04:49,914 - INFO - Batch: 120/141, Loss: 1.6922, Perplexity: 5.4314, Time: 0.26s
2025-04-26 00:04:50,160 - INFO - Batch: 130/141, Loss: 1.7058, Perplexity: 5.5057, Time: 0.25s
2025-04-26 00:04:50,459 - INFO - Batch: 140/141, Loss: 1.6970, Perplexity: 5.4575, Time: 0.30s
2025-04-26 00:04:51,112 - INFO - Validation  Loss: 1.8780, Perplexity: 6.5402, Time: 0.62s
2025-04-26 00:04:51,112 - INFO - Epoch 245/300 - Train Loss: 1.7025, Val Loss: 1.8780
2025-04-26 00:04:51,361 - INFO - Batch: 10/141, Loss: 1.7039, Perplexity: 5.4955, Time: 0.25s
2025-04-26 00:04:51,646 - INFO - Batch: 20/141, Loss: 1.6911, Perplexity: 5.4255, Time: 0.28s
2025-04-26 00:04:51,963 - INFO - Batch: 30/141, Loss: 1.7084, Perplexity: 5.5200, Time: 0.32s
2025-04-26 00:04:52,202 - INFO - Batch: 40/141, Loss: 1.7101, Perplexity: 5.5295, Time: 0.24s
2025-04-26 00:04:52,459 - INFO - Batch: 50/141, Loss: 1.7059, Perplexity: 5.5062, Time: 0.26s
2025-04-26 00:04:52,695 - INFO - Batch: 60/141, Loss: 1.7136, Perplexity: 5.5486, Time: 0.24s
2025-04-26 00:04:52,939 - INFO - Batch: 70/141, Loss: 1.7006, Perplexity: 5.4770, Time: 0.24s
2025-04-26 00:04:53,187 - INFO - Batch: 80/141, Loss: 1.6999, Perplexity: 5.4733, Time: 0.25s
2025-04-26 00:04:53,430 - INFO - Batch: 90/141, Loss: 1.6970, Perplexity: 5.4574, Time: 0.24s
2025-04-26 00:04:53,743 - INFO - Batch: 100/141, Loss: 1.7100, Perplexity: 5.5290, Time: 0.31s
2025-04-26 00:04:54,010 - INFO - Batch: 110/141, Loss: 1.6997, Perplexity: 5.4721, Time: 0.27s
2025-04-26 00:04:54,261 - INFO - Batch: 120/141, Loss: 1.7048, Perplexity: 5.5000, Time: 0.25s
2025-04-26 00:04:54,518 - INFO - Batch: 130/141, Loss: 1.7156, Perplexity: 5.5602, Time: 0.26s
2025-04-26 00:04:54,753 - INFO - Batch: 140/141, Loss: 1.7036, Perplexity: 5.4938, Time: 0.23s
2025-04-26 00:04:55,436 - INFO - Validation  Loss: 1.8760, Perplexity: 6.5273, Time: 0.65s
2025-04-26 00:04:55,437 - INFO - Epoch 246/300 - Train Loss: 1.7024, Val Loss: 1.8760
2025-04-26 00:04:55,692 - INFO - Batch: 10/141, Loss: 1.7133, Perplexity: 5.5474, Time: 0.26s
2025-04-26 00:04:55,950 - INFO - Batch: 20/141, Loss: 1.6824, Perplexity: 5.3786, Time: 0.26s
2025-04-26 00:04:56,174 - INFO - Batch: 30/141, Loss: 1.7002, Perplexity: 5.4750, Time: 0.22s
2025-04-26 00:04:56,415 - INFO - Batch: 40/141, Loss: 1.7201, Perplexity: 5.5850, Time: 0.24s
2025-04-26 00:04:56,650 - INFO - Batch: 50/141, Loss: 1.6970, Perplexity: 5.4576, Time: 0.24s
2025-04-26 00:04:56,893 - INFO - Batch: 60/141, Loss: 1.6930, Perplexity: 5.4356, Time: 0.24s
2025-04-26 00:04:57,170 - INFO - Batch: 70/141, Loss: 1.7080, Perplexity: 5.5179, Time: 0.28s
2025-04-26 00:04:57,392 - INFO - Batch: 80/141, Loss: 1.7127, Perplexity: 5.5437, Time: 0.22s
2025-04-26 00:04:57,609 - INFO - Batch: 90/141, Loss: 1.7053, Perplexity: 5.5028, Time: 0.22s
2025-04-26 00:04:57,853 - INFO - Batch: 100/141, Loss: 1.7091, Perplexity: 5.5241, Time: 0.24s
2025-04-26 00:04:58,075 - INFO - Batch: 110/141, Loss: 1.7090, Perplexity: 5.5235, Time: 0.22s
2025-04-26 00:04:58,293 - INFO - Batch: 120/141, Loss: 1.6898, Perplexity: 5.4185, Time: 0.22s
2025-04-26 00:04:58,523 - INFO - Batch: 130/141, Loss: 1.7091, Perplexity: 5.5242, Time: 0.23s
2025-04-26 00:04:58,746 - INFO - Batch: 140/141, Loss: 1.7014, Perplexity: 5.4817, Time: 0.22s
2025-04-26 00:04:59,399 - INFO - Validation  Loss: 1.8771, Perplexity: 6.5346, Time: 0.62s
2025-04-26 00:04:59,399 - INFO - Epoch 247/300 - Train Loss: 1.7024, Val Loss: 1.8771
2025-04-26 00:04:59,637 - INFO - Batch: 10/141, Loss: 1.7232, Perplexity: 5.6026, Time: 0.24s
2025-04-26 00:04:59,870 - INFO - Batch: 20/141, Loss: 1.7039, Perplexity: 5.4952, Time: 0.23s
2025-04-26 00:05:00,093 - INFO - Batch: 30/141, Loss: 1.7073, Perplexity: 5.5138, Time: 0.22s
2025-04-26 00:05:00,392 - INFO - Batch: 40/141, Loss: 1.7081, Perplexity: 5.5185, Time: 0.30s
2025-04-26 00:05:00,615 - INFO - Batch: 50/141, Loss: 1.6847, Perplexity: 5.3908, Time: 0.22s
2025-04-26 00:05:00,841 - INFO - Batch: 60/141, Loss: 1.6956, Perplexity: 5.4498, Time: 0.23s
2025-04-26 00:05:01,081 - INFO - Batch: 70/141, Loss: 1.6967, Perplexity: 5.4561, Time: 0.24s
2025-04-26 00:05:01,303 - INFO - Batch: 80/141, Loss: 1.7092, Perplexity: 5.5247, Time: 0.22s
2025-04-26 00:05:01,544 - INFO - Batch: 90/141, Loss: 1.6933, Perplexity: 5.4375, Time: 0.24s
2025-04-26 00:05:01,768 - INFO - Batch: 100/141, Loss: 1.7172, Perplexity: 5.5688, Time: 0.22s
2025-04-26 00:05:02,071 - INFO - Batch: 110/141, Loss: 1.7035, Perplexity: 5.4929, Time: 0.30s
2025-04-26 00:05:02,300 - INFO - Batch: 120/141, Loss: 1.7124, Perplexity: 5.5421, Time: 0.23s
2025-04-26 00:05:02,576 - INFO - Batch: 130/141, Loss: 1.7065, Perplexity: 5.5096, Time: 0.28s
2025-04-26 00:05:02,846 - INFO - Batch: 140/141, Loss: 1.7059, Perplexity: 5.5064, Time: 0.27s
2025-04-26 00:05:03,574 - INFO - Validation  Loss: 1.8772, Perplexity: 6.5353, Time: 0.70s
2025-04-26 00:05:03,574 - INFO - Epoch 248/300 - Train Loss: 1.7023, Val Loss: 1.8772
2025-04-26 00:05:03,837 - INFO - Batch: 10/141, Loss: 1.6991, Perplexity: 5.4688, Time: 0.26s
2025-04-26 00:05:04,085 - INFO - Batch: 20/141, Loss: 1.7025, Perplexity: 5.4875, Time: 0.25s
2025-04-26 00:05:04,317 - INFO - Batch: 30/141, Loss: 1.6915, Perplexity: 5.4276, Time: 0.23s
2025-04-26 00:05:04,552 - INFO - Batch: 40/141, Loss: 1.6983, Perplexity: 5.4648, Time: 0.24s
2025-04-26 00:05:04,788 - INFO - Batch: 50/141, Loss: 1.6972, Perplexity: 5.4588, Time: 0.24s
2025-04-26 00:05:05,019 - INFO - Batch: 60/141, Loss: 1.7041, Perplexity: 5.4964, Time: 0.23s
2025-04-26 00:05:05,258 - INFO - Batch: 70/141, Loss: 1.6952, Perplexity: 5.4480, Time: 0.24s
2025-04-26 00:05:05,561 - INFO - Batch: 80/141, Loss: 1.6933, Perplexity: 5.4375, Time: 0.30s
2025-04-26 00:05:05,786 - INFO - Batch: 90/141, Loss: 1.7033, Perplexity: 5.4918, Time: 0.22s
2025-04-26 00:05:06,024 - INFO - Batch: 100/141, Loss: 1.6956, Perplexity: 5.4498, Time: 0.24s
2025-04-26 00:05:06,263 - INFO - Batch: 110/141, Loss: 1.7004, Perplexity: 5.4760, Time: 0.24s
2025-04-26 00:05:06,501 - INFO - Batch: 120/141, Loss: 1.7113, Perplexity: 5.5361, Time: 0.24s
2025-04-26 00:05:06,729 - INFO - Batch: 130/141, Loss: 1.6915, Perplexity: 5.4277, Time: 0.23s
2025-04-26 00:05:06,956 - INFO - Batch: 140/141, Loss: 1.7101, Perplexity: 5.5296, Time: 0.23s
2025-04-26 00:05:07,646 - INFO - Validation  Loss: 1.8770, Perplexity: 6.5338, Time: 0.66s
2025-04-26 00:05:07,647 - INFO - Epoch 249/300 - Train Loss: 1.7023, Val Loss: 1.8770
2025-04-26 00:05:07,896 - INFO - Batch: 10/141, Loss: 1.7023, Perplexity: 5.4865, Time: 0.25s
2025-04-26 00:05:08,141 - INFO - Batch: 20/141, Loss: 1.7153, Perplexity: 5.5581, Time: 0.25s
2025-04-26 00:05:08,392 - INFO - Batch: 30/141, Loss: 1.7006, Perplexity: 5.4775, Time: 0.25s
2025-04-26 00:05:08,618 - INFO - Batch: 40/141, Loss: 1.7127, Perplexity: 5.5438, Time: 0.23s
2025-04-26 00:05:08,921 - INFO - Batch: 50/141, Loss: 1.7183, Perplexity: 5.5752, Time: 0.30s
2025-04-26 00:05:09,160 - INFO - Batch: 60/141, Loss: 1.6934, Perplexity: 5.4378, Time: 0.24s
2025-04-26 00:05:09,469 - INFO - Batch: 70/141, Loss: 1.7018, Perplexity: 5.4838, Time: 0.31s
2025-04-26 00:05:09,827 - INFO - Batch: 80/141, Loss: 1.6973, Perplexity: 5.4591, Time: 0.36s
2025-04-26 00:05:10,158 - INFO - Batch: 90/141, Loss: 1.7181, Perplexity: 5.5740, Time: 0.33s
2025-04-26 00:05:10,435 - INFO - Batch: 100/141, Loss: 1.7001, Perplexity: 5.4745, Time: 0.28s
2025-04-26 00:05:10,677 - INFO - Batch: 110/141, Loss: 1.7106, Perplexity: 5.5322, Time: 0.24s
2025-04-26 00:05:10,994 - INFO - Batch: 120/141, Loss: 1.7108, Perplexity: 5.5333, Time: 0.32s
2025-04-26 00:05:11,230 - INFO - Batch: 130/141, Loss: 1.6968, Perplexity: 5.4564, Time: 0.24s
2025-04-26 00:05:11,458 - INFO - Batch: 140/141, Loss: 1.7044, Perplexity: 5.4981, Time: 0.23s
2025-04-26 00:05:12,097 - INFO - Validation  Loss: 1.8767, Perplexity: 6.5320, Time: 0.61s
2025-04-26 00:05:12,098 - INFO - Epoch 250/300 - Train Loss: 1.7022, Val Loss: 1.8767
2025-04-26 00:05:12,465 - INFO - Batch: 10/141, Loss: 1.6979, Perplexity: 5.4624, Time: 0.37s
2025-04-26 00:05:12,722 - INFO - Batch: 20/141, Loss: 1.7158, Perplexity: 5.5612, Time: 0.25s
2025-04-26 00:05:12,955 - INFO - Batch: 30/141, Loss: 1.7061, Perplexity: 5.5077, Time: 0.23s
2025-04-26 00:05:13,193 - INFO - Batch: 40/141, Loss: 1.7144, Perplexity: 5.5534, Time: 0.24s
2025-04-26 00:05:13,432 - INFO - Batch: 50/141, Loss: 1.6795, Perplexity: 5.3631, Time: 0.24s
2025-04-26 00:05:13,671 - INFO - Batch: 60/141, Loss: 1.7119, Perplexity: 5.5395, Time: 0.24s
2025-04-26 00:05:13,915 - INFO - Batch: 70/141, Loss: 1.6966, Perplexity: 5.4555, Time: 0.24s
2025-04-26 00:05:14,157 - INFO - Batch: 80/141, Loss: 1.6936, Perplexity: 5.4389, Time: 0.24s
2025-04-26 00:05:14,448 - INFO - Batch: 90/141, Loss: 1.7037, Perplexity: 5.4942, Time: 0.29s
2025-04-26 00:05:14,690 - INFO - Batch: 100/141, Loss: 1.6949, Perplexity: 5.4463, Time: 0.24s
2025-04-26 00:05:14,921 - INFO - Batch: 110/141, Loss: 1.6997, Perplexity: 5.4724, Time: 0.23s
2025-04-26 00:05:15,170 - INFO - Batch: 120/141, Loss: 1.6878, Perplexity: 5.4076, Time: 0.25s
2025-04-26 00:05:15,409 - INFO - Batch: 130/141, Loss: 1.7044, Perplexity: 5.4978, Time: 0.24s
2025-04-26 00:05:15,643 - INFO - Batch: 140/141, Loss: 1.7002, Perplexity: 5.4751, Time: 0.23s
2025-04-26 00:05:16,321 - INFO - Validation  Loss: 1.8769, Perplexity: 6.5334, Time: 0.64s
2025-04-26 00:05:16,321 - INFO - Epoch 251/300 - Train Loss: 1.7022, Val Loss: 1.8769
2025-04-26 00:05:16,548 - INFO - Batch: 10/141, Loss: 1.6870, Perplexity: 5.4034, Time: 0.23s
2025-04-26 00:05:16,783 - INFO - Batch: 20/141, Loss: 1.6918, Perplexity: 5.4291, Time: 0.23s
2025-04-26 00:05:17,020 - INFO - Batch: 30/141, Loss: 1.6909, Perplexity: 5.4246, Time: 0.24s
2025-04-26 00:05:17,268 - INFO - Batch: 40/141, Loss: 1.7119, Perplexity: 5.5393, Time: 0.25s
2025-04-26 00:05:17,709 - INFO - Batch: 50/141, Loss: 1.6974, Perplexity: 5.4595, Time: 0.44s
2025-04-26 00:05:18,005 - INFO - Batch: 60/141, Loss: 1.7121, Perplexity: 5.5407, Time: 0.30s
2025-04-26 00:05:18,264 - INFO - Batch: 70/141, Loss: 1.7023, Perplexity: 5.4868, Time: 0.26s
2025-04-26 00:05:18,498 - INFO - Batch: 80/141, Loss: 1.7025, Perplexity: 5.4875, Time: 0.23s
2025-04-26 00:05:18,760 - INFO - Batch: 90/141, Loss: 1.7048, Perplexity: 5.5002, Time: 0.26s
2025-04-26 00:05:19,002 - INFO - Batch: 100/141, Loss: 1.7119, Perplexity: 5.5395, Time: 0.24s
2025-04-26 00:05:19,248 - INFO - Batch: 110/141, Loss: 1.6966, Perplexity: 5.4555, Time: 0.25s
2025-04-26 00:05:19,486 - INFO - Batch: 120/141, Loss: 1.7104, Perplexity: 5.5311, Time: 0.24s
2025-04-26 00:05:19,823 - INFO - Batch: 130/141, Loss: 1.6920, Perplexity: 5.4302, Time: 0.34s
2025-04-26 00:05:20,118 - INFO - Batch: 140/141, Loss: 1.7085, Perplexity: 5.5205, Time: 0.29s
2025-04-26 00:05:20,830 - INFO - Validation  Loss: 1.8766, Perplexity: 6.5311, Time: 0.67s
2025-04-26 00:05:20,831 - INFO - Epoch 252/300 - Train Loss: 1.7022, Val Loss: 1.8766
2025-04-26 00:05:21,062 - INFO - Batch: 10/141, Loss: 1.7086, Perplexity: 5.5210, Time: 0.23s
2025-04-26 00:05:21,356 - INFO - Batch: 20/141, Loss: 1.6979, Perplexity: 5.4626, Time: 0.29s
2025-04-26 00:05:21,584 - INFO - Batch: 30/141, Loss: 1.7122, Perplexity: 5.5413, Time: 0.23s
2025-04-26 00:05:21,819 - INFO - Batch: 40/141, Loss: 1.6917, Perplexity: 5.4287, Time: 0.23s
2025-04-26 00:05:22,057 - INFO - Batch: 50/141, Loss: 1.6883, Perplexity: 5.4101, Time: 0.24s
2025-04-26 00:05:22,293 - INFO - Batch: 60/141, Loss: 1.6926, Perplexity: 5.4336, Time: 0.24s
2025-04-26 00:05:22,522 - INFO - Batch: 70/141, Loss: 1.7068, Perplexity: 5.5115, Time: 0.23s
2025-04-26 00:05:22,764 - INFO - Batch: 80/141, Loss: 1.7038, Perplexity: 5.4950, Time: 0.24s
2025-04-26 00:05:22,998 - INFO - Batch: 90/141, Loss: 1.6826, Perplexity: 5.3796, Time: 0.23s
2025-04-26 00:05:23,293 - INFO - Batch: 100/141, Loss: 1.7130, Perplexity: 5.5456, Time: 0.29s
2025-04-26 00:05:23,525 - INFO - Batch: 110/141, Loss: 1.7005, Perplexity: 5.4766, Time: 0.23s
2025-04-26 00:05:23,760 - INFO - Batch: 120/141, Loss: 1.7015, Perplexity: 5.4822, Time: 0.23s
2025-04-26 00:05:23,983 - INFO - Batch: 130/141, Loss: 1.6981, Perplexity: 5.4636, Time: 0.22s
2025-04-26 00:05:24,222 - INFO - Batch: 140/141, Loss: 1.7099, Perplexity: 5.5282, Time: 0.24s
2025-04-26 00:05:25,055 - INFO - Validation  Loss: 1.8769, Perplexity: 6.5332, Time: 0.80s
2025-04-26 00:05:25,055 - INFO - Epoch 253/300 - Train Loss: 1.7021, Val Loss: 1.8769
2025-04-26 00:05:25,308 - INFO - Batch: 10/141, Loss: 1.7087, Perplexity: 5.5215, Time: 0.25s
2025-04-26 00:05:25,567 - INFO - Batch: 20/141, Loss: 1.7002, Perplexity: 5.4752, Time: 0.26s
2025-04-26 00:05:25,872 - INFO - Batch: 30/141, Loss: 1.7185, Perplexity: 5.5762, Time: 0.30s
2025-04-26 00:05:26,190 - INFO - Batch: 40/141, Loss: 1.7088, Perplexity: 5.5223, Time: 0.32s
2025-04-26 00:05:26,491 - INFO - Batch: 50/141, Loss: 1.6965, Perplexity: 5.4549, Time: 0.30s
2025-04-26 00:05:26,857 - INFO - Batch: 60/141, Loss: 1.6993, Perplexity: 5.4703, Time: 0.37s
2025-04-26 00:05:27,146 - INFO - Batch: 70/141, Loss: 1.7200, Perplexity: 5.5845, Time: 0.29s
2025-04-26 00:05:27,492 - INFO - Batch: 80/141, Loss: 1.6905, Perplexity: 5.4219, Time: 0.35s
2025-04-26 00:05:27,744 - INFO - Batch: 90/141, Loss: 1.7035, Perplexity: 5.4929, Time: 0.25s
2025-04-26 00:05:27,995 - INFO - Batch: 100/141, Loss: 1.6977, Perplexity: 5.4612, Time: 0.25s
2025-04-26 00:05:28,268 - INFO - Batch: 110/141, Loss: 1.7126, Perplexity: 5.5431, Time: 0.27s
2025-04-26 00:05:28,499 - INFO - Batch: 120/141, Loss: 1.7175, Perplexity: 5.5705, Time: 0.23s
2025-04-26 00:05:28,735 - INFO - Batch: 130/141, Loss: 1.6896, Perplexity: 5.4173, Time: 0.24s
2025-04-26 00:05:29,058 - INFO - Batch: 140/141, Loss: 1.7062, Perplexity: 5.5078, Time: 0.32s
2025-04-26 00:05:29,760 - INFO - Validation  Loss: 1.8769, Perplexity: 6.5334, Time: 0.67s
2025-04-26 00:05:29,760 - INFO - Epoch 254/300 - Train Loss: 1.7021, Val Loss: 1.8769
2025-04-26 00:05:29,999 - INFO - Batch: 10/141, Loss: 1.7075, Perplexity: 5.5151, Time: 0.24s
2025-04-26 00:05:30,225 - INFO - Batch: 20/141, Loss: 1.6881, Perplexity: 5.4091, Time: 0.23s
2025-04-26 00:05:30,527 - INFO - Batch: 30/141, Loss: 1.7170, Perplexity: 5.5675, Time: 0.30s
2025-04-26 00:05:30,758 - INFO - Batch: 40/141, Loss: 1.7029, Perplexity: 5.4900, Time: 0.23s
2025-04-26 00:05:31,005 - INFO - Batch: 50/141, Loss: 1.7067, Perplexity: 5.5106, Time: 0.25s
2025-04-26 00:05:31,238 - INFO - Batch: 60/141, Loss: 1.7033, Perplexity: 5.4919, Time: 0.23s
2025-04-26 00:05:31,472 - INFO - Batch: 70/141, Loss: 1.6922, Perplexity: 5.4312, Time: 0.23s
2025-04-26 00:05:31,714 - INFO - Batch: 80/141, Loss: 1.7036, Perplexity: 5.4936, Time: 0.24s
2025-04-26 00:05:31,949 - INFO - Batch: 90/141, Loss: 1.6958, Perplexity: 5.4508, Time: 0.23s
2025-04-26 00:05:32,186 - INFO - Batch: 100/141, Loss: 1.7152, Perplexity: 5.5578, Time: 0.24s
2025-04-26 00:05:32,479 - INFO - Batch: 110/141, Loss: 1.6981, Perplexity: 5.4638, Time: 0.29s
2025-04-26 00:05:32,719 - INFO - Batch: 120/141, Loss: 1.7090, Perplexity: 5.5233, Time: 0.24s
2025-04-26 00:05:33,003 - INFO - Batch: 130/141, Loss: 1.7152, Perplexity: 5.5576, Time: 0.28s
2025-04-26 00:05:33,378 - INFO - Batch: 140/141, Loss: 1.7057, Perplexity: 5.5054, Time: 0.38s
2025-04-26 00:05:34,255 - INFO - Validation  Loss: 1.8775, Perplexity: 6.5369, Time: 0.85s
2025-04-26 00:05:34,255 - INFO - Epoch 255/300 - Train Loss: 1.7021, Val Loss: 1.8775
2025-04-26 00:05:34,584 - INFO - Batch: 10/141, Loss: 1.6972, Perplexity: 5.4584, Time: 0.33s
2025-04-26 00:05:34,828 - INFO - Batch: 20/141, Loss: 1.6864, Perplexity: 5.3998, Time: 0.24s
2025-04-26 00:05:35,101 - INFO - Batch: 30/141, Loss: 1.6978, Perplexity: 5.4619, Time: 0.27s
2025-04-26 00:05:35,344 - INFO - Batch: 40/141, Loss: 1.7018, Perplexity: 5.4839, Time: 0.24s
2025-04-26 00:05:35,597 - INFO - Batch: 50/141, Loss: 1.7016, Perplexity: 5.4829, Time: 0.25s
2025-04-26 00:05:35,836 - INFO - Batch: 60/141, Loss: 1.6964, Perplexity: 5.4543, Time: 0.24s
2025-04-26 00:05:36,151 - INFO - Batch: 70/141, Loss: 1.7043, Perplexity: 5.4977, Time: 0.31s
2025-04-26 00:05:36,406 - INFO - Batch: 80/141, Loss: 1.6954, Perplexity: 5.4490, Time: 0.26s
2025-04-26 00:05:36,652 - INFO - Batch: 90/141, Loss: 1.7132, Perplexity: 5.5464, Time: 0.25s
2025-04-26 00:05:36,928 - INFO - Batch: 100/141, Loss: 1.7034, Perplexity: 5.4928, Time: 0.28s
2025-04-26 00:05:37,169 - INFO - Batch: 110/141, Loss: 1.7064, Perplexity: 5.5094, Time: 0.24s
2025-04-26 00:05:37,405 - INFO - Batch: 120/141, Loss: 1.6989, Perplexity: 5.4678, Time: 0.24s
2025-04-26 00:05:37,630 - INFO - Batch: 130/141, Loss: 1.7035, Perplexity: 5.4934, Time: 0.22s
2025-04-26 00:05:37,859 - INFO - Batch: 140/141, Loss: 1.6965, Perplexity: 5.4547, Time: 0.23s
2025-04-26 00:05:38,574 - INFO - Validation  Loss: 1.8766, Perplexity: 6.5311, Time: 0.69s
2025-04-26 00:05:38,574 - INFO - Epoch 256/300 - Train Loss: 1.7020, Val Loss: 1.8766
2025-04-26 00:05:38,853 - INFO - Batch: 10/141, Loss: 1.6947, Perplexity: 5.4451, Time: 0.28s
2025-04-26 00:05:39,164 - INFO - Batch: 20/141, Loss: 1.7029, Perplexity: 5.4901, Time: 0.31s
2025-04-26 00:05:39,538 - INFO - Batch: 30/141, Loss: 1.7000, Perplexity: 5.4737, Time: 0.37s
2025-04-26 00:05:39,887 - INFO - Batch: 40/141, Loss: 1.6949, Perplexity: 5.4464, Time: 0.35s
2025-04-26 00:05:40,113 - INFO - Batch: 50/141, Loss: 1.7113, Perplexity: 5.5363, Time: 0.23s
2025-04-26 00:05:40,341 - INFO - Batch: 60/141, Loss: 1.7138, Perplexity: 5.5500, Time: 0.23s
2025-04-26 00:05:40,564 - INFO - Batch: 70/141, Loss: 1.6980, Perplexity: 5.4629, Time: 0.22s
2025-04-26 00:05:40,800 - INFO - Batch: 80/141, Loss: 1.7013, Perplexity: 5.4812, Time: 0.24s
2025-04-26 00:05:41,099 - INFO - Batch: 90/141, Loss: 1.7187, Perplexity: 5.5771, Time: 0.30s
2025-04-26 00:05:41,387 - INFO - Batch: 100/141, Loss: 1.6956, Perplexity: 5.4498, Time: 0.29s
2025-04-26 00:05:41,670 - INFO - Batch: 110/141, Loss: 1.6978, Perplexity: 5.4621, Time: 0.28s
2025-04-26 00:05:42,023 - INFO - Batch: 120/141, Loss: 1.7063, Perplexity: 5.5086, Time: 0.35s
2025-04-26 00:05:42,315 - INFO - Batch: 130/141, Loss: 1.7040, Perplexity: 5.4960, Time: 0.29s
2025-04-26 00:05:42,616 - INFO - Batch: 140/141, Loss: 1.6850, Perplexity: 5.3924, Time: 0.30s
2025-04-26 00:05:43,740 - INFO - Validation  Loss: 1.8759, Perplexity: 6.5268, Time: 1.07s
2025-04-26 00:05:43,741 - INFO - Epoch 257/300 - Train Loss: 1.7020, Val Loss: 1.8759
2025-04-26 00:05:43,977 - INFO - Batch: 10/141, Loss: 1.7175, Perplexity: 5.5705, Time: 0.24s
2025-04-26 00:05:44,214 - INFO - Batch: 20/141, Loss: 1.7005, Perplexity: 5.4769, Time: 0.24s
2025-04-26 00:05:44,450 - INFO - Batch: 30/141, Loss: 1.7071, Perplexity: 5.5128, Time: 0.24s
2025-04-26 00:05:44,694 - INFO - Batch: 40/141, Loss: 1.7135, Perplexity: 5.5483, Time: 0.24s
2025-04-26 00:05:44,930 - INFO - Batch: 50/141, Loss: 1.7007, Perplexity: 5.4776, Time: 0.24s
2025-04-26 00:05:45,179 - INFO - Batch: 60/141, Loss: 1.7117, Perplexity: 5.5383, Time: 0.25s
2025-04-26 00:05:45,415 - INFO - Batch: 70/141, Loss: 1.6995, Perplexity: 5.4711, Time: 0.24s
2025-04-26 00:05:45,711 - INFO - Batch: 80/141, Loss: 1.6962, Perplexity: 5.4533, Time: 0.30s
2025-04-26 00:05:45,956 - INFO - Batch: 90/141, Loss: 1.6968, Perplexity: 5.4566, Time: 0.24s
2025-04-26 00:05:46,236 - INFO - Batch: 100/141, Loss: 1.6989, Perplexity: 5.4680, Time: 0.28s
2025-04-26 00:05:46,481 - INFO - Batch: 110/141, Loss: 1.7100, Perplexity: 5.5287, Time: 0.25s
2025-04-26 00:05:46,719 - INFO - Batch: 120/141, Loss: 1.7126, Perplexity: 5.5435, Time: 0.24s
2025-04-26 00:05:46,950 - INFO - Batch: 130/141, Loss: 1.7007, Perplexity: 5.4780, Time: 0.23s
2025-04-26 00:05:47,201 - INFO - Batch: 140/141, Loss: 1.7119, Perplexity: 5.5396, Time: 0.25s
2025-04-26 00:05:47,918 - INFO - Validation  Loss: 1.8770, Perplexity: 6.5336, Time: 0.68s
2025-04-26 00:05:47,918 - INFO - Epoch 258/300 - Train Loss: 1.7019, Val Loss: 1.8770
2025-04-26 00:05:48,179 - INFO - Batch: 10/141, Loss: 1.6931, Perplexity: 5.4362, Time: 0.26s
2025-04-26 00:05:48,441 - INFO - Batch: 20/141, Loss: 1.7082, Perplexity: 5.5190, Time: 0.26s
2025-04-26 00:05:48,714 - INFO - Batch: 30/141, Loss: 1.7008, Perplexity: 5.4781, Time: 0.27s
2025-04-26 00:05:48,972 - INFO - Batch: 40/141, Loss: 1.6991, Perplexity: 5.4690, Time: 0.26s
2025-04-26 00:05:49,281 - INFO - Batch: 50/141, Loss: 1.6993, Perplexity: 5.4703, Time: 0.31s
2025-04-26 00:05:49,533 - INFO - Batch: 60/141, Loss: 1.7005, Perplexity: 5.4766, Time: 0.25s
2025-04-26 00:05:49,796 - INFO - Batch: 70/141, Loss: 1.7156, Perplexity: 5.5601, Time: 0.26s
2025-04-26 00:05:50,025 - INFO - Batch: 80/141, Loss: 1.6978, Perplexity: 5.4619, Time: 0.23s
2025-04-26 00:05:50,255 - INFO - Batch: 90/141, Loss: 1.7065, Perplexity: 5.5097, Time: 0.23s
2025-04-26 00:05:50,487 - INFO - Batch: 100/141, Loss: 1.6999, Perplexity: 5.4732, Time: 0.23s
2025-04-26 00:05:50,732 - INFO - Batch: 110/141, Loss: 1.7118, Perplexity: 5.5390, Time: 0.25s
2025-04-26 00:05:51,035 - INFO - Batch: 120/141, Loss: 1.7022, Perplexity: 5.4857, Time: 0.30s
2025-04-26 00:05:51,273 - INFO - Batch: 130/141, Loss: 1.7070, Perplexity: 5.5126, Time: 0.24s
2025-04-26 00:05:51,502 - INFO - Batch: 140/141, Loss: 1.6885, Perplexity: 5.4111, Time: 0.23s
2025-04-26 00:05:52,134 - INFO - Validation  Loss: 1.8763, Perplexity: 6.5294, Time: 0.60s
2025-04-26 00:05:52,134 - INFO - Epoch 259/300 - Train Loss: 1.7019, Val Loss: 1.8763
2025-04-26 00:05:52,472 - INFO - Batch: 10/141, Loss: 1.6979, Perplexity: 5.4622, Time: 0.34s
2025-04-26 00:05:52,738 - INFO - Batch: 20/141, Loss: 1.6907, Perplexity: 5.4234, Time: 0.27s
2025-04-26 00:05:52,996 - INFO - Batch: 30/141, Loss: 1.7112, Perplexity: 5.5357, Time: 0.26s
2025-04-26 00:05:53,276 - INFO - Batch: 40/141, Loss: 1.7078, Perplexity: 5.5168, Time: 0.28s
2025-04-26 00:05:53,503 - INFO - Batch: 50/141, Loss: 1.7111, Perplexity: 5.5353, Time: 0.23s
2025-04-26 00:05:53,792 - INFO - Batch: 60/141, Loss: 1.6968, Perplexity: 5.4564, Time: 0.29s
2025-04-26 00:05:54,032 - INFO - Batch: 70/141, Loss: 1.6820, Perplexity: 5.3764, Time: 0.24s
2025-04-26 00:05:54,362 - INFO - Batch: 80/141, Loss: 1.7071, Perplexity: 5.5132, Time: 0.33s
2025-04-26 00:05:54,702 - INFO - Batch: 90/141, Loss: 1.7076, Perplexity: 5.5159, Time: 0.34s
2025-04-26 00:05:54,940 - INFO - Batch: 100/141, Loss: 1.6968, Perplexity: 5.4566, Time: 0.24s
2025-04-26 00:05:55,167 - INFO - Batch: 110/141, Loss: 1.6970, Perplexity: 5.4577, Time: 0.23s
2025-04-26 00:05:55,402 - INFO - Batch: 120/141, Loss: 1.6874, Perplexity: 5.4053, Time: 0.23s
2025-04-26 00:05:55,630 - INFO - Batch: 130/141, Loss: 1.7034, Perplexity: 5.4926, Time: 0.23s
2025-04-26 00:05:55,859 - INFO - Batch: 140/141, Loss: 1.7084, Perplexity: 5.5199, Time: 0.23s
2025-04-26 00:05:56,528 - INFO - Validation  Loss: 1.8765, Perplexity: 6.5306, Time: 0.64s
2025-04-26 00:05:56,528 - INFO - Epoch 260/300 - Train Loss: 1.7018, Val Loss: 1.8765
2025-04-26 00:05:56,759 - INFO - Batch: 10/141, Loss: 1.7066, Perplexity: 5.5102, Time: 0.23s
2025-04-26 00:05:56,987 - INFO - Batch: 20/141, Loss: 1.6869, Perplexity: 5.4026, Time: 0.23s
2025-04-26 00:05:57,225 - INFO - Batch: 30/141, Loss: 1.6994, Perplexity: 5.4706, Time: 0.24s
2025-04-26 00:05:57,448 - INFO - Batch: 40/141, Loss: 1.7012, Perplexity: 5.4806, Time: 0.22s
2025-04-26 00:05:57,674 - INFO - Batch: 50/141, Loss: 1.6881, Perplexity: 5.4089, Time: 0.23s
2025-04-26 00:05:57,961 - INFO - Batch: 60/141, Loss: 1.7037, Perplexity: 5.4943, Time: 0.29s
2025-04-26 00:05:58,186 - INFO - Batch: 70/141, Loss: 1.7067, Perplexity: 5.5106, Time: 0.22s
2025-04-26 00:05:58,411 - INFO - Batch: 80/141, Loss: 1.7149, Perplexity: 5.5559, Time: 0.22s
2025-04-26 00:05:58,634 - INFO - Batch: 90/141, Loss: 1.7172, Perplexity: 5.5690, Time: 0.22s
2025-04-26 00:05:58,880 - INFO - Batch: 100/141, Loss: 1.6938, Perplexity: 5.4403, Time: 0.25s
2025-04-26 00:05:59,103 - INFO - Batch: 110/141, Loss: 1.7122, Perplexity: 5.5412, Time: 0.22s
2025-04-26 00:05:59,338 - INFO - Batch: 120/141, Loss: 1.7024, Perplexity: 5.4872, Time: 0.23s
2025-04-26 00:05:59,623 - INFO - Batch: 130/141, Loss: 1.7033, Perplexity: 5.4919, Time: 0.28s
2025-04-26 00:05:59,851 - INFO - Batch: 140/141, Loss: 1.7094, Perplexity: 5.5258, Time: 0.23s
2025-04-26 00:06:00,455 - INFO - Validation  Loss: 1.8765, Perplexity: 6.5309, Time: 0.57s
2025-04-26 00:06:00,455 - INFO - Epoch 261/300 - Train Loss: 1.7018, Val Loss: 1.8765
2025-04-26 00:06:00,681 - INFO - Batch: 10/141, Loss: 1.6944, Perplexity: 5.4432, Time: 0.23s
2025-04-26 00:06:00,978 - INFO - Batch: 20/141, Loss: 1.6975, Perplexity: 5.4604, Time: 0.30s
2025-04-26 00:06:01,203 - INFO - Batch: 30/141, Loss: 1.7015, Perplexity: 5.4824, Time: 0.23s
2025-04-26 00:06:01,438 - INFO - Batch: 40/141, Loss: 1.6844, Perplexity: 5.3895, Time: 0.23s
2025-04-26 00:06:01,661 - INFO - Batch: 50/141, Loss: 1.7059, Perplexity: 5.5062, Time: 0.22s
2025-04-26 00:06:01,891 - INFO - Batch: 60/141, Loss: 1.7041, Perplexity: 5.4966, Time: 0.23s
2025-04-26 00:06:02,120 - INFO - Batch: 70/141, Loss: 1.6839, Perplexity: 5.3867, Time: 0.23s
2025-04-26 00:06:02,357 - INFO - Batch: 80/141, Loss: 1.7085, Perplexity: 5.5209, Time: 0.24s
2025-04-26 00:06:02,586 - INFO - Batch: 90/141, Loss: 1.7085, Perplexity: 5.5206, Time: 0.23s
2025-04-26 00:06:02,922 - INFO - Batch: 100/141, Loss: 1.7160, Perplexity: 5.5624, Time: 0.34s
2025-04-26 00:06:03,155 - INFO - Batch: 110/141, Loss: 1.7031, Perplexity: 5.4910, Time: 0.23s
2025-04-26 00:06:03,401 - INFO - Batch: 120/141, Loss: 1.6946, Perplexity: 5.4443, Time: 0.24s
2025-04-26 00:06:03,630 - INFO - Batch: 130/141, Loss: 1.7020, Perplexity: 5.4847, Time: 0.23s
2025-04-26 00:06:03,892 - INFO - Batch: 140/141, Loss: 1.7027, Perplexity: 5.4890, Time: 0.26s
2025-04-26 00:06:04,627 - INFO - Validation  Loss: 1.8761, Perplexity: 6.5279, Time: 0.70s
2025-04-26 00:06:04,627 - INFO - Epoch 262/300 - Train Loss: 1.7018, Val Loss: 1.8761
2025-04-26 00:06:04,875 - INFO - Batch: 10/141, Loss: 1.7037, Perplexity: 5.4943, Time: 0.25s
2025-04-26 00:06:05,145 - INFO - Batch: 20/141, Loss: 1.6980, Perplexity: 5.4630, Time: 0.27s
2025-04-26 00:06:05,428 - INFO - Batch: 30/141, Loss: 1.7065, Perplexity: 5.5096, Time: 0.28s
2025-04-26 00:06:05,658 - INFO - Batch: 40/141, Loss: 1.7032, Perplexity: 5.4913, Time: 0.23s
2025-04-26 00:06:05,924 - INFO - Batch: 50/141, Loss: 1.7145, Perplexity: 5.5541, Time: 0.27s
2025-04-26 00:06:06,168 - INFO - Batch: 60/141, Loss: 1.6931, Perplexity: 5.4363, Time: 0.24s
2025-04-26 00:06:06,504 - INFO - Batch: 70/141, Loss: 1.6919, Perplexity: 5.4296, Time: 0.34s
2025-04-26 00:06:06,767 - INFO - Batch: 80/141, Loss: 1.6912, Perplexity: 5.4262, Time: 0.26s
2025-04-26 00:06:07,055 - INFO - Batch: 90/141, Loss: 1.7010, Perplexity: 5.4793, Time: 0.29s
2025-04-26 00:06:07,295 - INFO - Batch: 100/141, Loss: 1.7110, Perplexity: 5.5344, Time: 0.24s
2025-04-26 00:06:07,559 - INFO - Batch: 110/141, Loss: 1.7096, Perplexity: 5.5267, Time: 0.26s
2025-04-26 00:06:07,879 - INFO - Batch: 120/141, Loss: 1.6979, Perplexity: 5.4626, Time: 0.32s
2025-04-26 00:06:08,188 - INFO - Batch: 130/141, Loss: 1.7039, Perplexity: 5.4952, Time: 0.31s
2025-04-26 00:06:08,514 - INFO - Batch: 140/141, Loss: 1.6789, Perplexity: 5.3597, Time: 0.33s
2025-04-26 00:06:09,166 - INFO - Validation  Loss: 1.8762, Perplexity: 6.5289, Time: 0.62s
2025-04-26 00:06:09,166 - INFO - Epoch 263/300 - Train Loss: 1.7017, Val Loss: 1.8762
2025-04-26 00:06:09,424 - INFO - Batch: 10/141, Loss: 1.6937, Perplexity: 5.4397, Time: 0.26s
2025-04-26 00:06:09,766 - INFO - Batch: 20/141, Loss: 1.6945, Perplexity: 5.4440, Time: 0.34s
2025-04-26 00:06:10,153 - INFO - Batch: 30/141, Loss: 1.6877, Perplexity: 5.4068, Time: 0.39s
2025-04-26 00:06:10,434 - INFO - Batch: 40/141, Loss: 1.6959, Perplexity: 5.4517, Time: 0.28s
2025-04-26 00:06:10,740 - INFO - Batch: 50/141, Loss: 1.6941, Perplexity: 5.4419, Time: 0.31s
2025-04-26 00:06:11,006 - INFO - Batch: 60/141, Loss: 1.6993, Perplexity: 5.4700, Time: 0.27s
2025-04-26 00:06:11,244 - INFO - Batch: 70/141, Loss: 1.7070, Perplexity: 5.5121, Time: 0.24s
2025-04-26 00:06:11,480 - INFO - Batch: 80/141, Loss: 1.6986, Perplexity: 5.4663, Time: 0.24s
2025-04-26 00:06:11,709 - INFO - Batch: 90/141, Loss: 1.7062, Perplexity: 5.5081, Time: 0.23s
2025-04-26 00:06:11,949 - INFO - Batch: 100/141, Loss: 1.7024, Perplexity: 5.4873, Time: 0.24s
2025-04-26 00:06:12,251 - INFO - Batch: 110/141, Loss: 1.7048, Perplexity: 5.5004, Time: 0.30s
2025-04-26 00:06:12,489 - INFO - Batch: 120/141, Loss: 1.6974, Perplexity: 5.4596, Time: 0.24s
2025-04-26 00:06:12,759 - INFO - Batch: 130/141, Loss: 1.7035, Perplexity: 5.4930, Time: 0.27s
2025-04-26 00:06:13,011 - INFO - Batch: 140/141, Loss: 1.7148, Perplexity: 5.5553, Time: 0.25s
2025-04-26 00:06:13,792 - INFO - Validation  Loss: 1.8774, Perplexity: 6.5363, Time: 0.73s
2025-04-26 00:06:13,792 - INFO - Epoch 264/300 - Train Loss: 1.7017, Val Loss: 1.8774
2025-04-26 00:06:14,035 - INFO - Batch: 10/141, Loss: 1.6873, Perplexity: 5.4048, Time: 0.24s
2025-04-26 00:06:14,258 - INFO - Batch: 20/141, Loss: 1.6970, Perplexity: 5.4577, Time: 0.22s
2025-04-26 00:06:14,479 - INFO - Batch: 30/141, Loss: 1.6962, Perplexity: 5.4532, Time: 0.22s
2025-04-26 00:06:14,713 - INFO - Batch: 40/141, Loss: 1.6913, Perplexity: 5.4266, Time: 0.23s
2025-04-26 00:06:14,948 - INFO - Batch: 50/141, Loss: 1.7255, Perplexity: 5.6151, Time: 0.23s
2025-04-26 00:06:15,249 - INFO - Batch: 60/141, Loss: 1.6918, Perplexity: 5.4295, Time: 0.30s
2025-04-26 00:06:15,555 - INFO - Batch: 70/141, Loss: 1.6856, Perplexity: 5.3959, Time: 0.30s
2025-04-26 00:06:15,786 - INFO - Batch: 80/141, Loss: 1.7027, Perplexity: 5.4885, Time: 0.23s
2025-04-26 00:06:16,026 - INFO - Batch: 90/141, Loss: 1.7046, Perplexity: 5.4992, Time: 0.24s
2025-04-26 00:06:16,268 - INFO - Batch: 100/141, Loss: 1.7036, Perplexity: 5.4938, Time: 0.24s
2025-04-26 00:06:16,500 - INFO - Batch: 110/141, Loss: 1.7081, Perplexity: 5.5186, Time: 0.23s
2025-04-26 00:06:16,730 - INFO - Batch: 120/141, Loss: 1.6929, Perplexity: 5.4350, Time: 0.23s
2025-04-26 00:06:16,973 - INFO - Batch: 130/141, Loss: 1.7024, Perplexity: 5.4874, Time: 0.24s
2025-04-26 00:06:17,204 - INFO - Batch: 140/141, Loss: 1.6992, Perplexity: 5.4698, Time: 0.23s
2025-04-26 00:06:17,905 - INFO - Validation  Loss: 1.8764, Perplexity: 6.5297, Time: 0.67s
2025-04-26 00:06:17,905 - INFO - Epoch 265/300 - Train Loss: 1.7017, Val Loss: 1.8764
2025-04-26 00:06:18,135 - INFO - Batch: 10/141, Loss: 1.6958, Perplexity: 5.4512, Time: 0.23s
2025-04-26 00:06:18,361 - INFO - Batch: 20/141, Loss: 1.6840, Perplexity: 5.3870, Time: 0.23s
2025-04-26 00:06:18,598 - INFO - Batch: 30/141, Loss: 1.7084, Perplexity: 5.5201, Time: 0.24s
2025-04-26 00:06:18,890 - INFO - Batch: 40/141, Loss: 1.7049, Perplexity: 5.5009, Time: 0.29s
2025-04-26 00:06:19,128 - INFO - Batch: 50/141, Loss: 1.7023, Perplexity: 5.4865, Time: 0.24s
2025-04-26 00:06:19,361 - INFO - Batch: 60/141, Loss: 1.7040, Perplexity: 5.4958, Time: 0.23s
2025-04-26 00:06:19,635 - INFO - Batch: 70/141, Loss: 1.7022, Perplexity: 5.4861, Time: 0.27s
2025-04-26 00:06:19,885 - INFO - Batch: 80/141, Loss: 1.7077, Perplexity: 5.5163, Time: 0.25s
2025-04-26 00:06:20,120 - INFO - Batch: 90/141, Loss: 1.7161, Perplexity: 5.5628, Time: 0.23s
2025-04-26 00:06:20,346 - INFO - Batch: 100/141, Loss: 1.7071, Perplexity: 5.5128, Time: 0.23s
2025-04-26 00:06:20,570 - INFO - Batch: 110/141, Loss: 1.7024, Perplexity: 5.4869, Time: 0.22s
2025-04-26 00:06:20,883 - INFO - Batch: 120/141, Loss: 1.6991, Perplexity: 5.4688, Time: 0.31s
2025-04-26 00:06:21,128 - INFO - Batch: 130/141, Loss: 1.6893, Perplexity: 5.4155, Time: 0.24s
2025-04-26 00:06:21,410 - INFO - Batch: 140/141, Loss: 1.6786, Perplexity: 5.3580, Time: 0.28s
2025-04-26 00:06:22,052 - INFO - Validation  Loss: 1.8773, Perplexity: 6.5358, Time: 0.61s
2025-04-26 00:06:22,052 - INFO - Epoch 266/300 - Train Loss: 1.7016, Val Loss: 1.8773
2025-04-26 00:06:22,418 - INFO - Batch: 10/141, Loss: 1.6942, Perplexity: 5.4424, Time: 0.37s
2025-04-26 00:06:22,648 - INFO - Batch: 20/141, Loss: 1.7003, Perplexity: 5.4753, Time: 0.23s
2025-04-26 00:06:22,884 - INFO - Batch: 30/141, Loss: 1.7200, Perplexity: 5.5844, Time: 0.24s
2025-04-26 00:06:23,143 - INFO - Batch: 40/141, Loss: 1.6996, Perplexity: 5.4716, Time: 0.26s
2025-04-26 00:06:23,364 - INFO - Batch: 50/141, Loss: 1.7011, Perplexity: 5.4800, Time: 0.22s
2025-04-26 00:06:23,596 - INFO - Batch: 60/141, Loss: 1.6950, Perplexity: 5.4469, Time: 0.23s
2025-04-26 00:06:23,819 - INFO - Batch: 70/141, Loss: 1.6847, Perplexity: 5.3908, Time: 0.22s
2025-04-26 00:06:24,132 - INFO - Batch: 80/141, Loss: 1.7050, Perplexity: 5.5016, Time: 0.31s
2025-04-26 00:06:24,355 - INFO - Batch: 90/141, Loss: 1.7031, Perplexity: 5.4909, Time: 0.22s
2025-04-26 00:06:24,581 - INFO - Batch: 100/141, Loss: 1.6944, Perplexity: 5.4434, Time: 0.23s
2025-04-26 00:06:24,812 - INFO - Batch: 110/141, Loss: 1.6991, Perplexity: 5.4691, Time: 0.23s
2025-04-26 00:06:25,058 - INFO - Batch: 120/141, Loss: 1.7035, Perplexity: 5.4929, Time: 0.25s
2025-04-26 00:06:25,284 - INFO - Batch: 130/141, Loss: 1.7024, Perplexity: 5.4872, Time: 0.23s
2025-04-26 00:06:25,509 - INFO - Batch: 140/141, Loss: 1.6874, Perplexity: 5.4054, Time: 0.22s
2025-04-26 00:06:26,241 - INFO - Validation  Loss: 1.8766, Perplexity: 6.5314, Time: 0.70s
2025-04-26 00:06:26,241 - INFO - Epoch 267/300 - Train Loss: 1.7016, Val Loss: 1.8766
2025-04-26 00:06:26,549 - INFO - Batch: 10/141, Loss: 1.6985, Perplexity: 5.4659, Time: 0.31s
2025-04-26 00:06:26,848 - INFO - Batch: 20/141, Loss: 1.6940, Perplexity: 5.4411, Time: 0.30s
2025-04-26 00:06:27,125 - INFO - Batch: 30/141, Loss: 1.6866, Perplexity: 5.4010, Time: 0.28s
2025-04-26 00:06:27,363 - INFO - Batch: 40/141, Loss: 1.7064, Perplexity: 5.5089, Time: 0.24s
2025-04-26 00:06:27,696 - INFO - Batch: 50/141, Loss: 1.7041, Perplexity: 5.4966, Time: 0.33s
2025-04-26 00:06:27,941 - INFO - Batch: 60/141, Loss: 1.6828, Perplexity: 5.3804, Time: 0.24s
2025-04-26 00:06:28,193 - INFO - Batch: 70/141, Loss: 1.6998, Perplexity: 5.4731, Time: 0.25s
2025-04-26 00:06:28,440 - INFO - Batch: 80/141, Loss: 1.7092, Perplexity: 5.5243, Time: 0.25s
2025-04-26 00:06:28,669 - INFO - Batch: 90/141, Loss: 1.6987, Perplexity: 5.4668, Time: 0.23s
2025-04-26 00:06:28,938 - INFO - Batch: 100/141, Loss: 1.7128, Perplexity: 5.5445, Time: 0.27s
2025-04-26 00:06:29,186 - INFO - Batch: 110/141, Loss: 1.7036, Perplexity: 5.4936, Time: 0.25s
2025-04-26 00:06:29,583 - INFO - Batch: 120/141, Loss: 1.6969, Perplexity: 5.4573, Time: 0.40s
2025-04-26 00:06:29,895 - INFO - Batch: 130/141, Loss: 1.6939, Perplexity: 5.4408, Time: 0.31s
2025-04-26 00:06:30,180 - INFO - Batch: 140/141, Loss: 1.6949, Perplexity: 5.4461, Time: 0.28s
2025-04-26 00:06:30,840 - INFO - Validation  Loss: 1.8759, Perplexity: 6.5269, Time: 0.63s
2025-04-26 00:06:30,840 - INFO - Epoch 268/300 - Train Loss: 1.7015, Val Loss: 1.8759
2025-04-26 00:06:31,093 - INFO - Batch: 10/141, Loss: 1.7037, Perplexity: 5.4940, Time: 0.25s
2025-04-26 00:06:31,396 - INFO - Batch: 20/141, Loss: 1.6934, Perplexity: 5.4377, Time: 0.30s
2025-04-26 00:06:31,647 - INFO - Batch: 30/141, Loss: 1.7135, Perplexity: 5.5482, Time: 0.25s
2025-04-26 00:06:31,882 - INFO - Batch: 40/141, Loss: 1.7060, Perplexity: 5.5069, Time: 0.24s
2025-04-26 00:06:32,116 - INFO - Batch: 50/141, Loss: 1.7049, Perplexity: 5.5011, Time: 0.23s
2025-04-26 00:06:32,362 - INFO - Batch: 60/141, Loss: 1.6842, Perplexity: 5.3880, Time: 0.25s
2025-04-26 00:06:32,597 - INFO - Batch: 70/141, Loss: 1.7108, Perplexity: 5.5335, Time: 0.24s
2025-04-26 00:06:32,836 - INFO - Batch: 80/141, Loss: 1.7056, Perplexity: 5.5049, Time: 0.24s
2025-04-26 00:06:33,136 - INFO - Batch: 90/141, Loss: 1.6839, Perplexity: 5.3864, Time: 0.30s
2025-04-26 00:06:33,370 - INFO - Batch: 100/141, Loss: 1.6984, Perplexity: 5.4653, Time: 0.23s
2025-04-26 00:06:33,601 - INFO - Batch: 110/141, Loss: 1.7018, Perplexity: 5.4836, Time: 0.23s
2025-04-26 00:06:33,835 - INFO - Batch: 120/141, Loss: 1.6993, Perplexity: 5.4699, Time: 0.23s
2025-04-26 00:06:34,066 - INFO - Batch: 130/141, Loss: 1.6949, Perplexity: 5.4461, Time: 0.23s
2025-04-26 00:06:34,304 - INFO - Batch: 140/141, Loss: 1.6922, Perplexity: 5.4314, Time: 0.24s
2025-04-26 00:06:34,967 - INFO - Validation  Loss: 1.8763, Perplexity: 6.5296, Time: 0.63s
2025-04-26 00:06:34,967 - INFO - Epoch 269/300 - Train Loss: 1.7015, Val Loss: 1.8763
2025-04-26 00:06:35,208 - INFO - Batch: 10/141, Loss: 1.6984, Perplexity: 5.4649, Time: 0.24s
2025-04-26 00:06:35,432 - INFO - Batch: 20/141, Loss: 1.7089, Perplexity: 5.5231, Time: 0.22s
2025-04-26 00:06:35,669 - INFO - Batch: 30/141, Loss: 1.7028, Perplexity: 5.4893, Time: 0.24s
2025-04-26 00:06:35,908 - INFO - Batch: 40/141, Loss: 1.7023, Perplexity: 5.4864, Time: 0.24s
2025-04-26 00:06:36,155 - INFO - Batch: 50/141, Loss: 1.7092, Perplexity: 5.5244, Time: 0.25s
2025-04-26 00:06:36,454 - INFO - Batch: 60/141, Loss: 1.7057, Perplexity: 5.5053, Time: 0.30s
2025-04-26 00:06:36,693 - INFO - Batch: 70/141, Loss: 1.7139, Perplexity: 5.5507, Time: 0.24s
2025-04-26 00:06:36,923 - INFO - Batch: 80/141, Loss: 1.7038, Perplexity: 5.4949, Time: 0.23s
2025-04-26 00:06:37,166 - INFO - Batch: 90/141, Loss: 1.7082, Perplexity: 5.5190, Time: 0.24s
2025-04-26 00:06:37,406 - INFO - Batch: 100/141, Loss: 1.7033, Perplexity: 5.4919, Time: 0.24s
2025-04-26 00:06:37,645 - INFO - Batch: 110/141, Loss: 1.7045, Perplexity: 5.4984, Time: 0.24s
2025-04-26 00:06:37,886 - INFO - Batch: 120/141, Loss: 1.7004, Perplexity: 5.4759, Time: 0.24s
2025-04-26 00:06:38,148 - INFO - Batch: 130/141, Loss: 1.7012, Perplexity: 5.4805, Time: 0.26s
2025-04-26 00:06:38,438 - INFO - Batch: 140/141, Loss: 1.6875, Perplexity: 5.4060, Time: 0.29s
2025-04-26 00:06:39,076 - INFO - Validation  Loss: 1.8770, Perplexity: 6.5338, Time: 0.60s
2025-04-26 00:06:39,076 - INFO - Epoch 270/300 - Train Loss: 1.7014, Val Loss: 1.8770
2025-04-26 00:06:39,364 - INFO - Batch: 10/141, Loss: 1.7028, Perplexity: 5.4894, Time: 0.29s
2025-04-26 00:06:39,665 - INFO - Batch: 20/141, Loss: 1.6814, Perplexity: 5.3728, Time: 0.30s
2025-04-26 00:06:39,891 - INFO - Batch: 30/141, Loss: 1.6942, Perplexity: 5.4425, Time: 0.23s
2025-04-26 00:06:40,128 - INFO - Batch: 40/141, Loss: 1.7099, Perplexity: 5.5283, Time: 0.24s
2025-04-26 00:06:40,353 - INFO - Batch: 50/141, Loss: 1.7011, Perplexity: 5.4802, Time: 0.22s
2025-04-26 00:06:40,574 - INFO - Batch: 60/141, Loss: 1.7071, Perplexity: 5.5132, Time: 0.22s
2025-04-26 00:06:40,802 - INFO - Batch: 70/141, Loss: 1.7032, Perplexity: 5.4914, Time: 0.23s
2025-04-26 00:06:41,022 - INFO - Batch: 80/141, Loss: 1.7129, Perplexity: 5.5452, Time: 0.22s
2025-04-26 00:06:41,244 - INFO - Batch: 90/141, Loss: 1.6974, Perplexity: 5.4599, Time: 0.22s
2025-04-26 00:06:41,525 - INFO - Batch: 100/141, Loss: 1.7049, Perplexity: 5.5007, Time: 0.28s
2025-04-26 00:06:41,748 - INFO - Batch: 110/141, Loss: 1.6982, Perplexity: 5.4643, Time: 0.22s
2025-04-26 00:06:41,972 - INFO - Batch: 120/141, Loss: 1.7202, Perplexity: 5.5859, Time: 0.22s
2025-04-26 00:06:42,203 - INFO - Batch: 130/141, Loss: 1.7072, Perplexity: 5.5135, Time: 0.23s
2025-04-26 00:06:42,487 - INFO - Batch: 140/141, Loss: 1.7019, Perplexity: 5.4844, Time: 0.28s
2025-04-26 00:06:43,319 - INFO - Validation  Loss: 1.8768, Perplexity: 6.5326, Time: 0.79s
2025-04-26 00:06:43,319 - INFO - Epoch 271/300 - Train Loss: 1.7014, Val Loss: 1.8768
2025-04-26 00:06:43,619 - INFO - Batch: 10/141, Loss: 1.6976, Perplexity: 5.4608, Time: 0.30s
2025-04-26 00:06:43,918 - INFO - Batch: 20/141, Loss: 1.6958, Perplexity: 5.4512, Time: 0.30s
2025-04-26 00:06:44,198 - INFO - Batch: 30/141, Loss: 1.7046, Perplexity: 5.4994, Time: 0.28s
2025-04-26 00:06:44,447 - INFO - Batch: 40/141, Loss: 1.7018, Perplexity: 5.4837, Time: 0.25s
2025-04-26 00:06:44,720 - INFO - Batch: 50/141, Loss: 1.7133, Perplexity: 5.5470, Time: 0.27s
2025-04-26 00:06:44,956 - INFO - Batch: 60/141, Loss: 1.7059, Perplexity: 5.5065, Time: 0.24s
2025-04-26 00:06:45,277 - INFO - Batch: 70/141, Loss: 1.6913, Perplexity: 5.4264, Time: 0.32s
2025-04-26 00:06:45,523 - INFO - Batch: 80/141, Loss: 1.7029, Perplexity: 5.4897, Time: 0.25s
2025-04-26 00:06:45,829 - INFO - Batch: 90/141, Loss: 1.7025, Perplexity: 5.4875, Time: 0.31s
2025-04-26 00:06:46,129 - INFO - Batch: 100/141, Loss: 1.6957, Perplexity: 5.4505, Time: 0.30s
2025-04-26 00:06:46,411 - INFO - Batch: 110/141, Loss: 1.7016, Perplexity: 5.4825, Time: 0.28s
2025-04-26 00:06:46,728 - INFO - Batch: 120/141, Loss: 1.6912, Perplexity: 5.4262, Time: 0.32s
2025-04-26 00:06:47,077 - INFO - Batch: 130/141, Loss: 1.7039, Perplexity: 5.4955, Time: 0.35s
2025-04-26 00:06:47,479 - INFO - Batch: 140/141, Loss: 1.7045, Perplexity: 5.4986, Time: 0.40s
2025-04-26 00:06:48,176 - INFO - Validation  Loss: 1.8764, Perplexity: 6.5300, Time: 0.66s
2025-04-26 00:06:48,176 - INFO - Epoch 272/300 - Train Loss: 1.7013, Val Loss: 1.8764
2025-04-26 00:06:48,406 - INFO - Batch: 10/141, Loss: 1.7100, Perplexity: 5.5288, Time: 0.23s
2025-04-26 00:06:48,641 - INFO - Batch: 20/141, Loss: 1.6970, Perplexity: 5.4573, Time: 0.23s
2025-04-26 00:06:48,932 - INFO - Batch: 30/141, Loss: 1.7040, Perplexity: 5.4959, Time: 0.29s
2025-04-26 00:06:49,158 - INFO - Batch: 40/141, Loss: 1.7005, Perplexity: 5.4764, Time: 0.23s
2025-04-26 00:06:49,384 - INFO - Batch: 50/141, Loss: 1.6957, Perplexity: 5.4502, Time: 0.23s
2025-04-26 00:06:49,612 - INFO - Batch: 60/141, Loss: 1.7133, Perplexity: 5.5470, Time: 0.23s
2025-04-26 00:06:49,859 - INFO - Batch: 70/141, Loss: 1.7037, Perplexity: 5.4943, Time: 0.25s
2025-04-26 00:06:50,149 - INFO - Batch: 80/141, Loss: 1.6930, Perplexity: 5.4357, Time: 0.29s
2025-04-26 00:06:50,384 - INFO - Batch: 90/141, Loss: 1.7155, Perplexity: 5.5597, Time: 0.23s
2025-04-26 00:06:50,616 - INFO - Batch: 100/141, Loss: 1.7049, Perplexity: 5.5007, Time: 0.23s
2025-04-26 00:06:50,904 - INFO - Batch: 110/141, Loss: 1.7046, Perplexity: 5.4992, Time: 0.29s
2025-04-26 00:06:51,138 - INFO - Batch: 120/141, Loss: 1.6939, Perplexity: 5.4408, Time: 0.23s
2025-04-26 00:06:51,362 - INFO - Batch: 130/141, Loss: 1.6916, Perplexity: 5.4283, Time: 0.22s
2025-04-26 00:06:51,602 - INFO - Batch: 140/141, Loss: 1.7167, Perplexity: 5.5662, Time: 0.24s
2025-04-26 00:06:52,349 - INFO - Validation  Loss: 1.8763, Perplexity: 6.5291, Time: 0.72s
2025-04-26 00:06:52,350 - INFO - Epoch 273/300 - Train Loss: 1.7013, Val Loss: 1.8763
2025-04-26 00:06:52,594 - INFO - Batch: 10/141, Loss: 1.6978, Perplexity: 5.4617, Time: 0.24s
2025-04-26 00:06:52,832 - INFO - Batch: 20/141, Loss: 1.7127, Perplexity: 5.5438, Time: 0.24s
2025-04-26 00:06:53,059 - INFO - Batch: 30/141, Loss: 1.7172, Perplexity: 5.5687, Time: 0.23s
2025-04-26 00:06:53,285 - INFO - Batch: 40/141, Loss: 1.6833, Perplexity: 5.3834, Time: 0.23s
2025-04-26 00:06:53,515 - INFO - Batch: 50/141, Loss: 1.7140, Perplexity: 5.5511, Time: 0.23s
2025-04-26 00:06:53,742 - INFO - Batch: 60/141, Loss: 1.7130, Perplexity: 5.5457, Time: 0.23s
2025-04-26 00:06:53,970 - INFO - Batch: 70/141, Loss: 1.7002, Perplexity: 5.4748, Time: 0.23s
2025-04-26 00:06:54,256 - INFO - Batch: 80/141, Loss: 1.6949, Perplexity: 5.4460, Time: 0.29s
2025-04-26 00:06:54,479 - INFO - Batch: 90/141, Loss: 1.6987, Perplexity: 5.4667, Time: 0.22s
2025-04-26 00:06:54,707 - INFO - Batch: 100/141, Loss: 1.6929, Perplexity: 5.4354, Time: 0.23s
2025-04-26 00:06:54,937 - INFO - Batch: 110/141, Loss: 1.6904, Perplexity: 5.4216, Time: 0.23s
2025-04-26 00:06:55,164 - INFO - Batch: 120/141, Loss: 1.6982, Perplexity: 5.4639, Time: 0.23s
2025-04-26 00:06:55,393 - INFO - Batch: 130/141, Loss: 1.6928, Perplexity: 5.4345, Time: 0.23s
2025-04-26 00:06:55,621 - INFO - Batch: 140/141, Loss: 1.7026, Perplexity: 5.4884, Time: 0.23s
2025-04-26 00:06:56,297 - INFO - Validation  Loss: 1.8768, Perplexity: 6.5324, Time: 0.65s
2025-04-26 00:06:56,297 - INFO - Epoch 274/300 - Train Loss: 1.7012, Val Loss: 1.8768
2025-04-26 00:06:56,528 - INFO - Batch: 10/141, Loss: 1.6998, Perplexity: 5.4729, Time: 0.23s
2025-04-26 00:06:56,763 - INFO - Batch: 20/141, Loss: 1.6973, Perplexity: 5.4590, Time: 0.23s
2025-04-26 00:06:57,052 - INFO - Batch: 30/141, Loss: 1.7185, Perplexity: 5.5762, Time: 0.29s
2025-04-26 00:06:57,362 - INFO - Batch: 40/141, Loss: 1.7114, Perplexity: 5.5369, Time: 0.31s
2025-04-26 00:06:57,600 - INFO - Batch: 50/141, Loss: 1.6990, Perplexity: 5.4687, Time: 0.24s
2025-04-26 00:06:57,881 - INFO - Batch: 60/141, Loss: 1.7004, Perplexity: 5.4761, Time: 0.28s
2025-04-26 00:06:58,126 - INFO - Batch: 70/141, Loss: 1.7123, Perplexity: 5.5415, Time: 0.24s
2025-04-26 00:06:58,381 - INFO - Batch: 80/141, Loss: 1.6991, Perplexity: 5.4689, Time: 0.26s
2025-04-26 00:06:58,721 - INFO - Batch: 90/141, Loss: 1.7046, Perplexity: 5.4993, Time: 0.34s
2025-04-26 00:06:58,947 - INFO - Batch: 100/141, Loss: 1.6893, Perplexity: 5.4159, Time: 0.23s
2025-04-26 00:06:59,178 - INFO - Batch: 110/141, Loss: 1.6802, Perplexity: 5.3664, Time: 0.23s
2025-04-26 00:06:59,511 - INFO - Batch: 120/141, Loss: 1.7158, Perplexity: 5.5614, Time: 0.33s
2025-04-26 00:06:59,750 - INFO - Batch: 130/141, Loss: 1.6998, Perplexity: 5.4726, Time: 0.24s
2025-04-26 00:06:59,977 - INFO - Batch: 140/141, Loss: 1.7180, Perplexity: 5.5736, Time: 0.23s
2025-04-26 00:07:00,585 - INFO - Validation  Loss: 1.8770, Perplexity: 6.5336, Time: 0.58s
2025-04-26 00:07:00,586 - INFO - Epoch 275/300 - Train Loss: 1.7012, Val Loss: 1.8770
2025-04-26 00:07:00,881 - INFO - Batch: 10/141, Loss: 1.7095, Perplexity: 5.5264, Time: 0.30s
2025-04-26 00:07:01,126 - INFO - Batch: 20/141, Loss: 1.6954, Perplexity: 5.4489, Time: 0.25s
2025-04-26 00:07:01,359 - INFO - Batch: 30/141, Loss: 1.7069, Perplexity: 5.5118, Time: 0.23s
2025-04-26 00:07:01,606 - INFO - Batch: 40/141, Loss: 1.7023, Perplexity: 5.4864, Time: 0.25s
2025-04-26 00:07:01,862 - INFO - Batch: 50/141, Loss: 1.7107, Perplexity: 5.5329, Time: 0.26s
2025-04-26 00:07:02,096 - INFO - Batch: 60/141, Loss: 1.6956, Perplexity: 5.4498, Time: 0.23s
2025-04-26 00:07:02,340 - INFO - Batch: 70/141, Loss: 1.6971, Perplexity: 5.4579, Time: 0.24s
2025-04-26 00:07:02,572 - INFO - Batch: 80/141, Loss: 1.7075, Perplexity: 5.5153, Time: 0.23s
2025-04-26 00:07:02,870 - INFO - Batch: 90/141, Loss: 1.6965, Perplexity: 5.4548, Time: 0.30s
2025-04-26 00:07:03,111 - INFO - Batch: 100/141, Loss: 1.7070, Perplexity: 5.5123, Time: 0.24s
2025-04-26 00:07:03,352 - INFO - Batch: 110/141, Loss: 1.7031, Perplexity: 5.4908, Time: 0.24s
2025-04-26 00:07:03,598 - INFO - Batch: 120/141, Loss: 1.6915, Perplexity: 5.4277, Time: 0.25s
2025-04-26 00:07:03,856 - INFO - Batch: 130/141, Loss: 1.7045, Perplexity: 5.4984, Time: 0.26s
2025-04-26 00:07:04,118 - INFO - Batch: 140/141, Loss: 1.7118, Perplexity: 5.5390, Time: 0.26s
2025-04-26 00:07:04,871 - INFO - Validation  Loss: 1.8770, Perplexity: 6.5338, Time: 0.71s
2025-04-26 00:07:04,871 - INFO - Epoch 276/300 - Train Loss: 1.7012, Val Loss: 1.8770
2025-04-26 00:07:05,191 - INFO - Batch: 10/141, Loss: 1.7055, Perplexity: 5.5040, Time: 0.32s
2025-04-26 00:07:05,419 - INFO - Batch: 20/141, Loss: 1.6993, Perplexity: 5.4702, Time: 0.23s
2025-04-26 00:07:05,699 - INFO - Batch: 30/141, Loss: 1.7032, Perplexity: 5.4918, Time: 0.28s
2025-04-26 00:07:05,925 - INFO - Batch: 40/141, Loss: 1.6918, Perplexity: 5.4291, Time: 0.23s
2025-04-26 00:07:06,231 - INFO - Batch: 50/141, Loss: 1.6920, Perplexity: 5.4302, Time: 0.31s
2025-04-26 00:07:06,511 - INFO - Batch: 60/141, Loss: 1.6925, Perplexity: 5.4331, Time: 0.28s
2025-04-26 00:07:06,745 - INFO - Batch: 70/141, Loss: 1.7096, Perplexity: 5.5269, Time: 0.23s
2025-04-26 00:07:07,025 - INFO - Batch: 80/141, Loss: 1.7043, Perplexity: 5.4974, Time: 0.28s
2025-04-26 00:07:07,265 - INFO - Batch: 90/141, Loss: 1.7227, Perplexity: 5.5997, Time: 0.24s
2025-04-26 00:07:07,549 - INFO - Batch: 100/141, Loss: 1.6985, Perplexity: 5.4656, Time: 0.28s
2025-04-26 00:07:07,783 - INFO - Batch: 110/141, Loss: 1.7004, Perplexity: 5.4759, Time: 0.23s
2025-04-26 00:07:08,009 - INFO - Batch: 120/141, Loss: 1.6998, Perplexity: 5.4727, Time: 0.23s
2025-04-26 00:07:08,302 - INFO - Batch: 130/141, Loss: 1.6980, Perplexity: 5.4632, Time: 0.29s
2025-04-26 00:07:08,528 - INFO - Batch: 140/141, Loss: 1.7055, Perplexity: 5.5041, Time: 0.23s
2025-04-26 00:07:09,168 - INFO - Validation  Loss: 1.8767, Perplexity: 6.5317, Time: 0.61s
2025-04-26 00:07:09,168 - INFO - Epoch 277/300 - Train Loss: 1.7012, Val Loss: 1.8767
2025-04-26 00:07:09,406 - INFO - Batch: 10/141, Loss: 1.6955, Perplexity: 5.4494, Time: 0.24s
2025-04-26 00:07:09,695 - INFO - Batch: 20/141, Loss: 1.7050, Perplexity: 5.5015, Time: 0.29s
2025-04-26 00:07:09,925 - INFO - Batch: 30/141, Loss: 1.7011, Perplexity: 5.4801, Time: 0.23s
2025-04-26 00:07:10,162 - INFO - Batch: 40/141, Loss: 1.7084, Perplexity: 5.5203, Time: 0.24s
2025-04-26 00:07:10,395 - INFO - Batch: 50/141, Loss: 1.6970, Perplexity: 5.4576, Time: 0.23s
2025-04-26 00:07:10,627 - INFO - Batch: 60/141, Loss: 1.7035, Perplexity: 5.4932, Time: 0.23s
2025-04-26 00:07:10,882 - INFO - Batch: 70/141, Loss: 1.7004, Perplexity: 5.4761, Time: 0.26s
2025-04-26 00:07:11,118 - INFO - Batch: 80/141, Loss: 1.7046, Perplexity: 5.4991, Time: 0.24s
2025-04-26 00:07:11,414 - INFO - Batch: 90/141, Loss: 1.6988, Perplexity: 5.4674, Time: 0.30s
2025-04-26 00:07:11,643 - INFO - Batch: 100/141, Loss: 1.7067, Perplexity: 5.5110, Time: 0.23s
2025-04-26 00:07:11,880 - INFO - Batch: 110/141, Loss: 1.6883, Perplexity: 5.4102, Time: 0.24s
2025-04-26 00:07:12,110 - INFO - Batch: 120/141, Loss: 1.7014, Perplexity: 5.4816, Time: 0.23s
2025-04-26 00:07:12,347 - INFO - Batch: 130/141, Loss: 1.6917, Perplexity: 5.4286, Time: 0.24s
2025-04-26 00:07:12,581 - INFO - Batch: 140/141, Loss: 1.6985, Perplexity: 5.4657, Time: 0.23s
2025-04-26 00:07:13,245 - INFO - Validation  Loss: 1.8769, Perplexity: 6.5331, Time: 0.63s
2025-04-26 00:07:13,245 - INFO - Epoch 278/300 - Train Loss: 1.7011, Val Loss: 1.8769
2025-04-26 00:07:13,470 - INFO - Batch: 10/141, Loss: 1.7118, Perplexity: 5.5388, Time: 0.22s
2025-04-26 00:07:13,698 - INFO - Batch: 20/141, Loss: 1.7022, Perplexity: 5.4860, Time: 0.23s
2025-04-26 00:07:13,927 - INFO - Batch: 30/141, Loss: 1.7084, Perplexity: 5.5201, Time: 0.23s
2025-04-26 00:07:14,153 - INFO - Batch: 40/141, Loss: 1.7100, Perplexity: 5.5288, Time: 0.23s
2025-04-26 00:07:14,492 - INFO - Batch: 50/141, Loss: 1.6978, Perplexity: 5.4621, Time: 0.34s
2025-04-26 00:07:14,778 - INFO - Batch: 60/141, Loss: 1.7033, Perplexity: 5.4920, Time: 0.29s
2025-04-26 00:07:14,996 - INFO - Batch: 70/141, Loss: 1.6940, Perplexity: 5.4411, Time: 0.22s
2025-04-26 00:07:15,237 - INFO - Batch: 80/141, Loss: 1.6984, Perplexity: 5.4654, Time: 0.24s
2025-04-26 00:07:15,460 - INFO - Batch: 90/141, Loss: 1.7008, Perplexity: 5.4784, Time: 0.22s
2025-04-26 00:07:15,690 - INFO - Batch: 100/141, Loss: 1.6921, Perplexity: 5.4308, Time: 0.23s
2025-04-26 00:07:15,932 - INFO - Batch: 110/141, Loss: 1.6928, Perplexity: 5.4345, Time: 0.24s
2025-04-26 00:07:16,176 - INFO - Batch: 120/141, Loss: 1.7058, Perplexity: 5.5060, Time: 0.24s
2025-04-26 00:07:16,404 - INFO - Batch: 130/141, Loss: 1.6947, Perplexity: 5.4448, Time: 0.23s
2025-04-26 00:07:16,682 - INFO - Batch: 140/141, Loss: 1.7129, Perplexity: 5.5448, Time: 0.28s
2025-04-26 00:07:17,284 - INFO - Validation  Loss: 1.8770, Perplexity: 6.5341, Time: 0.58s
2025-04-26 00:07:17,284 - INFO - Epoch 279/300 - Train Loss: 1.7010, Val Loss: 1.8770
2025-04-26 00:07:17,516 - INFO - Batch: 10/141, Loss: 1.7085, Perplexity: 5.5207, Time: 0.23s
2025-04-26 00:07:17,737 - INFO - Batch: 20/141, Loss: 1.6994, Perplexity: 5.4708, Time: 0.22s
2025-04-26 00:07:18,021 - INFO - Batch: 30/141, Loss: 1.7138, Perplexity: 5.5501, Time: 0.28s
2025-04-26 00:07:18,240 - INFO - Batch: 40/141, Loss: 1.6904, Perplexity: 5.4217, Time: 0.22s
2025-04-26 00:07:18,463 - INFO - Batch: 50/141, Loss: 1.6915, Perplexity: 5.4274, Time: 0.22s
2025-04-26 00:07:18,682 - INFO - Batch: 60/141, Loss: 1.6950, Perplexity: 5.4465, Time: 0.22s
2025-04-26 00:07:18,907 - INFO - Batch: 70/141, Loss: 1.7047, Perplexity: 5.4995, Time: 0.22s
2025-04-26 00:07:19,123 - INFO - Batch: 80/141, Loss: 1.7151, Perplexity: 5.5572, Time: 0.22s
2025-04-26 00:07:19,351 - INFO - Batch: 90/141, Loss: 1.7105, Perplexity: 5.5315, Time: 0.23s
2025-04-26 00:07:19,697 - INFO - Batch: 100/141, Loss: 1.7065, Perplexity: 5.5095, Time: 0.35s
2025-04-26 00:07:19,996 - INFO - Batch: 110/141, Loss: 1.6993, Perplexity: 5.4701, Time: 0.30s
2025-04-26 00:07:20,288 - INFO - Batch: 120/141, Loss: 1.6971, Perplexity: 5.4579, Time: 0.29s
2025-04-26 00:07:20,515 - INFO - Batch: 130/141, Loss: 1.7076, Perplexity: 5.5156, Time: 0.23s
2025-04-26 00:07:20,756 - INFO - Batch: 140/141, Loss: 1.6987, Perplexity: 5.4670, Time: 0.24s
2025-04-26 00:07:21,440 - INFO - Validation  Loss: 1.8769, Perplexity: 6.5334, Time: 0.65s
2025-04-26 00:07:21,440 - INFO - Epoch 280/300 - Train Loss: 1.7009, Val Loss: 1.8769
2025-04-26 00:07:21,675 - INFO - Batch: 10/141, Loss: 1.7064, Perplexity: 5.5093, Time: 0.24s
2025-04-26 00:07:21,905 - INFO - Batch: 20/141, Loss: 1.7075, Perplexity: 5.5152, Time: 0.23s
2025-04-26 00:07:22,199 - INFO - Batch: 30/141, Loss: 1.6982, Perplexity: 5.4639, Time: 0.29s
2025-04-26 00:07:22,502 - INFO - Batch: 40/141, Loss: 1.6935, Perplexity: 5.4383, Time: 0.30s
2025-04-26 00:07:22,741 - INFO - Batch: 50/141, Loss: 1.7105, Perplexity: 5.5317, Time: 0.24s
2025-04-26 00:07:22,962 - INFO - Batch: 60/141, Loss: 1.7080, Perplexity: 5.5178, Time: 0.22s
2025-04-26 00:07:23,239 - INFO - Batch: 70/141, Loss: 1.6945, Perplexity: 5.4442, Time: 0.28s
2025-04-26 00:07:23,461 - INFO - Batch: 80/141, Loss: 1.6944, Perplexity: 5.4435, Time: 0.22s
2025-04-26 00:07:23,677 - INFO - Batch: 90/141, Loss: 1.6984, Perplexity: 5.4654, Time: 0.22s
2025-04-26 00:07:23,901 - INFO - Batch: 100/141, Loss: 1.7044, Perplexity: 5.4980, Time: 0.22s
2025-04-26 00:07:24,129 - INFO - Batch: 110/141, Loss: 1.7051, Perplexity: 5.5018, Time: 0.23s
2025-04-26 00:07:24,355 - INFO - Batch: 120/141, Loss: 1.7032, Perplexity: 5.4917, Time: 0.23s
2025-04-26 00:07:24,644 - INFO - Batch: 130/141, Loss: 1.7039, Perplexity: 5.4956, Time: 0.29s
2025-04-26 00:07:24,944 - INFO - Batch: 140/141, Loss: 1.7076, Perplexity: 5.5155, Time: 0.30s
2025-04-26 00:07:25,798 - INFO - Validation  Loss: 1.8761, Perplexity: 6.5281, Time: 0.81s
2025-04-26 00:07:25,798 - INFO - Epoch 281/300 - Train Loss: 1.7009, Val Loss: 1.8761
2025-04-26 00:07:26,133 - INFO - Batch: 10/141, Loss: 1.6965, Perplexity: 5.4549, Time: 0.33s
2025-04-26 00:07:26,371 - INFO - Batch: 20/141, Loss: 1.7187, Perplexity: 5.5771, Time: 0.24s
2025-04-26 00:07:26,671 - INFO - Batch: 30/141, Loss: 1.7128, Perplexity: 5.5447, Time: 0.30s
2025-04-26 00:07:27,066 - INFO - Batch: 40/141, Loss: 1.7067, Perplexity: 5.5107, Time: 0.39s
2025-04-26 00:07:27,345 - INFO - Batch: 50/141, Loss: 1.7114, Perplexity: 5.5368, Time: 0.28s
2025-04-26 00:07:27,664 - INFO - Batch: 60/141, Loss: 1.6939, Perplexity: 5.4406, Time: 0.32s
2025-04-26 00:07:27,902 - INFO - Batch: 70/141, Loss: 1.7135, Perplexity: 5.5483, Time: 0.24s
2025-04-26 00:07:28,167 - INFO - Batch: 80/141, Loss: 1.7020, Perplexity: 5.4848, Time: 0.26s
2025-04-26 00:07:28,398 - INFO - Batch: 90/141, Loss: 1.7106, Perplexity: 5.5325, Time: 0.23s
2025-04-26 00:07:28,635 - INFO - Batch: 100/141, Loss: 1.7009, Perplexity: 5.4789, Time: 0.24s
2025-04-26 00:07:28,946 - INFO - Batch: 110/141, Loss: 1.6863, Perplexity: 5.3996, Time: 0.31s
2025-04-26 00:07:29,201 - INFO - Batch: 120/141, Loss: 1.7019, Perplexity: 5.4843, Time: 0.25s
2025-04-26 00:07:29,463 - INFO - Batch: 130/141, Loss: 1.7126, Perplexity: 5.5436, Time: 0.26s
2025-04-26 00:07:29,736 - INFO - Batch: 140/141, Loss: 1.6961, Perplexity: 5.4528, Time: 0.27s
2025-04-26 00:07:30,524 - INFO - Validation  Loss: 1.8758, Perplexity: 6.5259, Time: 0.76s
2025-04-26 00:07:30,524 - INFO - Epoch 282/300 - Train Loss: 1.7009, Val Loss: 1.8758
2025-04-26 00:07:30,835 - INFO - Batch: 10/141, Loss: 1.6971, Perplexity: 5.4583, Time: 0.31s
2025-04-26 00:07:31,113 - INFO - Batch: 20/141, Loss: 1.6992, Perplexity: 5.4698, Time: 0.28s
2025-04-26 00:07:31,350 - INFO - Batch: 30/141, Loss: 1.6958, Perplexity: 5.4511, Time: 0.24s
2025-04-26 00:07:31,593 - INFO - Batch: 40/141, Loss: 1.6987, Perplexity: 5.4670, Time: 0.24s
2025-04-26 00:07:31,848 - INFO - Batch: 50/141, Loss: 1.6936, Perplexity: 5.4390, Time: 0.25s
2025-04-26 00:07:32,118 - INFO - Batch: 60/141, Loss: 1.7133, Perplexity: 5.5473, Time: 0.27s
2025-04-26 00:07:32,359 - INFO - Batch: 70/141, Loss: 1.6998, Perplexity: 5.4729, Time: 0.24s
2025-04-26 00:07:32,773 - INFO - Batch: 80/141, Loss: 1.7105, Perplexity: 5.5319, Time: 0.41s
2025-04-26 00:07:33,143 - INFO - Batch: 90/141, Loss: 1.7064, Perplexity: 5.5092, Time: 0.37s
2025-04-26 00:07:33,399 - INFO - Batch: 100/141, Loss: 1.6941, Perplexity: 5.4420, Time: 0.26s
2025-04-26 00:07:33,695 - INFO - Batch: 110/141, Loss: 1.7137, Perplexity: 5.5496, Time: 0.30s
2025-04-26 00:07:33,931 - INFO - Batch: 120/141, Loss: 1.7002, Perplexity: 5.4752, Time: 0.24s
2025-04-26 00:07:34,176 - INFO - Batch: 130/141, Loss: 1.6962, Perplexity: 5.4529, Time: 0.24s
2025-04-26 00:07:34,416 - INFO - Batch: 140/141, Loss: 1.6922, Perplexity: 5.4313, Time: 0.24s
2025-04-26 00:07:35,075 - INFO - Validation  Loss: 1.8757, Perplexity: 6.5257, Time: 0.62s
2025-04-26 00:07:35,075 - INFO - Epoch 283/300 - Train Loss: 1.7008, Val Loss: 1.8757
2025-04-26 00:07:35,348 - INFO - Batch: 10/141, Loss: 1.7113, Perplexity: 5.5361, Time: 0.27s
2025-04-26 00:07:35,650 - INFO - Batch: 20/141, Loss: 1.7099, Perplexity: 5.5282, Time: 0.30s
2025-04-26 00:07:35,901 - INFO - Batch: 30/141, Loss: 1.7028, Perplexity: 5.4893, Time: 0.25s
2025-04-26 00:07:36,217 - INFO - Batch: 40/141, Loss: 1.6979, Perplexity: 5.4623, Time: 0.32s
2025-04-26 00:07:36,445 - INFO - Batch: 50/141, Loss: 1.6858, Perplexity: 5.3968, Time: 0.23s
2025-04-26 00:07:36,691 - INFO - Batch: 60/141, Loss: 1.6959, Perplexity: 5.4516, Time: 0.25s
2025-04-26 00:07:36,921 - INFO - Batch: 70/141, Loss: 1.6992, Perplexity: 5.4693, Time: 0.23s
2025-04-26 00:07:37,154 - INFO - Batch: 80/141, Loss: 1.7064, Perplexity: 5.5089, Time: 0.23s
2025-04-26 00:07:37,378 - INFO - Batch: 90/141, Loss: 1.7003, Perplexity: 5.4757, Time: 0.22s
2025-04-26 00:07:37,606 - INFO - Batch: 100/141, Loss: 1.6859, Perplexity: 5.3972, Time: 0.23s
2025-04-26 00:07:37,834 - INFO - Batch: 110/141, Loss: 1.7004, Perplexity: 5.4763, Time: 0.23s
2025-04-26 00:07:38,128 - INFO - Batch: 120/141, Loss: 1.7104, Perplexity: 5.5312, Time: 0.29s
2025-04-26 00:07:38,347 - INFO - Batch: 130/141, Loss: 1.6989, Perplexity: 5.4677, Time: 0.22s
2025-04-26 00:07:38,575 - INFO - Batch: 140/141, Loss: 1.6999, Perplexity: 5.4734, Time: 0.23s
2025-04-26 00:07:39,175 - INFO - Validation  Loss: 1.8763, Perplexity: 6.5296, Time: 0.56s
2025-04-26 00:07:39,175 - INFO - Epoch 284/300 - Train Loss: 1.7008, Val Loss: 1.8763
2025-04-26 00:07:39,457 - INFO - Batch: 10/141, Loss: 1.6964, Perplexity: 5.4545, Time: 0.28s
2025-04-26 00:07:39,675 - INFO - Batch: 20/141, Loss: 1.6993, Perplexity: 5.4703, Time: 0.22s
2025-04-26 00:07:39,930 - INFO - Batch: 30/141, Loss: 1.7004, Perplexity: 5.4764, Time: 0.25s
2025-04-26 00:07:40,165 - INFO - Batch: 40/141, Loss: 1.7027, Perplexity: 5.4886, Time: 0.23s
2025-04-26 00:07:40,390 - INFO - Batch: 50/141, Loss: 1.7039, Perplexity: 5.4956, Time: 0.22s
2025-04-26 00:07:40,622 - INFO - Batch: 60/141, Loss: 1.6951, Perplexity: 5.4475, Time: 0.23s
2025-04-26 00:07:40,850 - INFO - Batch: 70/141, Loss: 1.7064, Perplexity: 5.5089, Time: 0.23s
2025-04-26 00:07:41,089 - INFO - Batch: 80/141, Loss: 1.7027, Perplexity: 5.4885, Time: 0.24s
2025-04-26 00:07:41,373 - INFO - Batch: 90/141, Loss: 1.7201, Perplexity: 5.5849, Time: 0.28s
2025-04-26 00:07:41,598 - INFO - Batch: 100/141, Loss: 1.7037, Perplexity: 5.4940, Time: 0.23s
2025-04-26 00:07:41,868 - INFO - Batch: 110/141, Loss: 1.6947, Perplexity: 5.4452, Time: 0.27s
2025-04-26 00:07:42,104 - INFO - Batch: 120/141, Loss: 1.6987, Perplexity: 5.4671, Time: 0.24s
2025-04-26 00:07:42,335 - INFO - Batch: 130/141, Loss: 1.7123, Perplexity: 5.5417, Time: 0.23s
2025-04-26 00:07:42,571 - INFO - Batch: 140/141, Loss: 1.6923, Perplexity: 5.4320, Time: 0.24s
2025-04-26 00:07:43,413 - INFO - Validation  Loss: 1.8762, Perplexity: 6.5287, Time: 0.81s
2025-04-26 00:07:43,413 - INFO - Epoch 285/300 - Train Loss: 1.7007, Val Loss: 1.8762
2025-04-26 00:07:43,720 - INFO - Batch: 10/141, Loss: 1.7119, Perplexity: 5.5393, Time: 0.31s
2025-04-26 00:07:44,006 - INFO - Batch: 20/141, Loss: 1.6944, Perplexity: 5.4432, Time: 0.29s
2025-04-26 00:07:44,266 - INFO - Batch: 30/141, Loss: 1.6990, Perplexity: 5.4686, Time: 0.26s
2025-04-26 00:07:44,580 - INFO - Batch: 40/141, Loss: 1.7006, Perplexity: 5.4770, Time: 0.31s
2025-04-26 00:07:44,966 - INFO - Batch: 50/141, Loss: 1.6901, Perplexity: 5.4202, Time: 0.39s
2025-04-26 00:07:45,247 - INFO - Batch: 60/141, Loss: 1.7040, Perplexity: 5.4960, Time: 0.28s
2025-04-26 00:07:45,478 - INFO - Batch: 70/141, Loss: 1.6884, Perplexity: 5.4106, Time: 0.23s
2025-04-26 00:07:45,704 - INFO - Batch: 80/141, Loss: 1.6956, Perplexity: 5.4500, Time: 0.23s
2025-04-26 00:07:45,926 - INFO - Batch: 90/141, Loss: 1.7141, Perplexity: 5.5515, Time: 0.22s
2025-04-26 00:07:46,166 - INFO - Batch: 100/141, Loss: 1.6957, Perplexity: 5.4504, Time: 0.24s
2025-04-26 00:07:46,399 - INFO - Batch: 110/141, Loss: 1.6976, Perplexity: 5.4609, Time: 0.23s
2025-04-26 00:07:46,637 - INFO - Batch: 120/141, Loss: 1.7009, Perplexity: 5.4788, Time: 0.24s
2025-04-26 00:07:46,949 - INFO - Batch: 130/141, Loss: 1.6988, Perplexity: 5.4674, Time: 0.31s
2025-04-26 00:07:47,182 - INFO - Batch: 140/141, Loss: 1.7032, Perplexity: 5.4917, Time: 0.23s
2025-04-26 00:07:47,816 - INFO - Validation  Loss: 1.8769, Perplexity: 6.5330, Time: 0.60s
2025-04-26 00:07:47,816 - INFO - Epoch 286/300 - Train Loss: 1.7007, Val Loss: 1.8769
2025-04-26 00:07:48,055 - INFO - Batch: 10/141, Loss: 1.7175, Perplexity: 5.5704, Time: 0.24s
2025-04-26 00:07:48,350 - INFO - Batch: 20/141, Loss: 1.7107, Perplexity: 5.5326, Time: 0.29s
2025-04-26 00:07:48,602 - INFO - Batch: 30/141, Loss: 1.6990, Perplexity: 5.4683, Time: 0.25s
2025-04-26 00:07:48,870 - INFO - Batch: 40/141, Loss: 1.7076, Perplexity: 5.5155, Time: 0.27s
2025-04-26 00:07:49,199 - INFO - Batch: 50/141, Loss: 1.6999, Perplexity: 5.4733, Time: 0.33s
2025-04-26 00:07:49,532 - INFO - Batch: 60/141, Loss: 1.7003, Perplexity: 5.4754, Time: 0.33s
2025-04-26 00:07:49,795 - INFO - Batch: 70/141, Loss: 1.7013, Perplexity: 5.4811, Time: 0.26s
2025-04-26 00:07:50,058 - INFO - Batch: 80/141, Loss: 1.7015, Perplexity: 5.4822, Time: 0.26s
2025-04-26 00:07:50,297 - INFO - Batch: 90/141, Loss: 1.7101, Perplexity: 5.5298, Time: 0.24s
2025-04-26 00:07:50,698 - INFO - Batch: 100/141, Loss: 1.6903, Perplexity: 5.4212, Time: 0.40s
2025-04-26 00:07:51,031 - INFO - Batch: 110/141, Loss: 1.7051, Perplexity: 5.5022, Time: 0.33s
2025-04-26 00:07:51,345 - INFO - Batch: 120/141, Loss: 1.6972, Perplexity: 5.4586, Time: 0.31s
2025-04-26 00:07:51,607 - INFO - Batch: 130/141, Loss: 1.7084, Perplexity: 5.5202, Time: 0.26s
2025-04-26 00:07:51,853 - INFO - Batch: 140/141, Loss: 1.6889, Perplexity: 5.4135, Time: 0.25s
2025-04-26 00:07:52,530 - INFO - Validation  Loss: 1.8762, Perplexity: 6.5284, Time: 0.64s
2025-04-26 00:07:52,532 - INFO - Epoch 287/300 - Train Loss: 1.7007, Val Loss: 1.8762
2025-04-26 00:07:52,820 - INFO - Batch: 10/141, Loss: 1.7138, Perplexity: 5.5499, Time: 0.29s
2025-04-26 00:07:53,113 - INFO - Batch: 20/141, Loss: 1.6975, Perplexity: 5.4604, Time: 0.29s
2025-04-26 00:07:53,408 - INFO - Batch: 30/141, Loss: 1.7020, Perplexity: 5.4850, Time: 0.30s
2025-04-26 00:07:53,646 - INFO - Batch: 40/141, Loss: 1.7081, Perplexity: 5.5182, Time: 0.24s
2025-04-26 00:07:53,870 - INFO - Batch: 50/141, Loss: 1.7014, Perplexity: 5.4817, Time: 0.22s
2025-04-26 00:07:54,166 - INFO - Batch: 60/141, Loss: 1.7059, Perplexity: 5.5066, Time: 0.30s
2025-04-26 00:07:54,396 - INFO - Batch: 70/141, Loss: 1.7038, Perplexity: 5.4948, Time: 0.23s
2025-04-26 00:07:54,620 - INFO - Batch: 80/141, Loss: 1.7125, Perplexity: 5.5427, Time: 0.22s
2025-04-26 00:07:54,849 - INFO - Batch: 90/141, Loss: 1.7064, Perplexity: 5.5092, Time: 0.23s
2025-04-26 00:07:55,083 - INFO - Batch: 100/141, Loss: 1.6957, Perplexity: 5.4507, Time: 0.23s
2025-04-26 00:07:55,308 - INFO - Batch: 110/141, Loss: 1.6829, Perplexity: 5.3811, Time: 0.23s
2025-04-26 00:07:55,540 - INFO - Batch: 120/141, Loss: 1.7191, Perplexity: 5.5795, Time: 0.23s
2025-04-26 00:07:55,768 - INFO - Batch: 130/141, Loss: 1.7002, Perplexity: 5.4749, Time: 0.23s
2025-04-26 00:07:56,102 - INFO - Batch: 140/141, Loss: 1.6975, Perplexity: 5.4603, Time: 0.33s
2025-04-26 00:07:56,687 - INFO - Validation  Loss: 1.8773, Perplexity: 6.5360, Time: 0.55s
2025-04-26 00:07:56,687 - INFO - Epoch 288/300 - Train Loss: 1.7007, Val Loss: 1.8773
2025-04-26 00:07:56,918 - INFO - Batch: 10/141, Loss: 1.6910, Perplexity: 5.4249, Time: 0.23s
2025-04-26 00:07:57,143 - INFO - Batch: 20/141, Loss: 1.6965, Perplexity: 5.4550, Time: 0.22s
2025-04-26 00:07:57,439 - INFO - Batch: 30/141, Loss: 1.6967, Perplexity: 5.4557, Time: 0.30s
2025-04-26 00:07:57,663 - INFO - Batch: 40/141, Loss: 1.6978, Perplexity: 5.4621, Time: 0.22s
2025-04-26 00:07:57,886 - INFO - Batch: 50/141, Loss: 1.7031, Perplexity: 5.4907, Time: 0.22s
2025-04-26 00:07:58,115 - INFO - Batch: 60/141, Loss: 1.7059, Perplexity: 5.5061, Time: 0.23s
2025-04-26 00:07:58,344 - INFO - Batch: 70/141, Loss: 1.6917, Perplexity: 5.4285, Time: 0.23s
2025-04-26 00:07:58,571 - INFO - Batch: 80/141, Loss: 1.7079, Perplexity: 5.5176, Time: 0.23s
2025-04-26 00:07:58,801 - INFO - Batch: 90/141, Loss: 1.7035, Perplexity: 5.4930, Time: 0.23s
2025-04-26 00:07:59,033 - INFO - Batch: 100/141, Loss: 1.7045, Perplexity: 5.4985, Time: 0.23s
2025-04-26 00:07:59,319 - INFO - Batch: 110/141, Loss: 1.6958, Perplexity: 5.4510, Time: 0.29s
2025-04-26 00:07:59,542 - INFO - Batch: 120/141, Loss: 1.7228, Perplexity: 5.6001, Time: 0.22s
2025-04-26 00:07:59,765 - INFO - Batch: 130/141, Loss: 1.7028, Perplexity: 5.4894, Time: 0.22s
2025-04-26 00:07:59,994 - INFO - Batch: 140/141, Loss: 1.7110, Perplexity: 5.5344, Time: 0.23s
2025-04-26 00:08:00,683 - INFO - Validation  Loss: 1.8761, Perplexity: 6.5279, Time: 0.65s
2025-04-26 00:08:00,683 - INFO - Epoch 289/300 - Train Loss: 1.7006, Val Loss: 1.8761
2025-04-26 00:08:00,901 - INFO - Batch: 10/141, Loss: 1.7122, Perplexity: 5.5410, Time: 0.22s
2025-04-26 00:08:01,140 - INFO - Batch: 20/141, Loss: 1.7002, Perplexity: 5.4749, Time: 0.24s
2025-04-26 00:08:01,377 - INFO - Batch: 30/141, Loss: 1.6999, Perplexity: 5.4735, Time: 0.24s
2025-04-26 00:08:01,609 - INFO - Batch: 40/141, Loss: 1.6855, Perplexity: 5.3951, Time: 0.23s
2025-04-26 00:08:01,846 - INFO - Batch: 50/141, Loss: 1.6980, Perplexity: 5.4632, Time: 0.24s
2025-04-26 00:08:02,073 - INFO - Batch: 60/141, Loss: 1.7007, Perplexity: 5.4775, Time: 0.23s
2025-04-26 00:08:02,368 - INFO - Batch: 70/141, Loss: 1.6985, Perplexity: 5.4660, Time: 0.29s
2025-04-26 00:08:02,596 - INFO - Batch: 80/141, Loss: 1.6926, Perplexity: 5.4338, Time: 0.23s
2025-04-26 00:08:02,826 - INFO - Batch: 90/141, Loss: 1.6908, Perplexity: 5.4240, Time: 0.23s
2025-04-26 00:08:03,062 - INFO - Batch: 100/141, Loss: 1.6927, Perplexity: 5.4344, Time: 0.24s
2025-04-26 00:08:03,287 - INFO - Batch: 110/141, Loss: 1.7103, Perplexity: 5.5308, Time: 0.23s
2025-04-26 00:08:03,525 - INFO - Batch: 120/141, Loss: 1.7051, Perplexity: 5.5021, Time: 0.24s
2025-04-26 00:08:03,753 - INFO - Batch: 130/141, Loss: 1.6953, Perplexity: 5.4482, Time: 0.23s
2025-04-26 00:08:03,991 - INFO - Batch: 140/141, Loss: 1.6979, Perplexity: 5.4623, Time: 0.24s
2025-04-26 00:08:04,646 - INFO - Validation  Loss: 1.8761, Perplexity: 6.5282, Time: 0.62s
2025-04-26 00:08:04,646 - INFO - Epoch 290/300 - Train Loss: 1.7006, Val Loss: 1.8761
2025-04-26 00:08:04,881 - INFO - Batch: 10/141, Loss: 1.6888, Perplexity: 5.4128, Time: 0.23s
2025-04-26 00:08:05,109 - INFO - Batch: 20/141, Loss: 1.6919, Perplexity: 5.4299, Time: 0.23s
2025-04-26 00:08:05,341 - INFO - Batch: 30/141, Loss: 1.7019, Perplexity: 5.4845, Time: 0.23s
2025-04-26 00:08:05,640 - INFO - Batch: 40/141, Loss: 1.6867, Perplexity: 5.4015, Time: 0.30s
2025-04-26 00:08:05,865 - INFO - Batch: 50/141, Loss: 1.6975, Perplexity: 5.4603, Time: 0.22s
2025-04-26 00:08:06,090 - INFO - Batch: 60/141, Loss: 1.7030, Perplexity: 5.4904, Time: 0.23s
2025-04-26 00:08:06,315 - INFO - Batch: 70/141, Loss: 1.7014, Perplexity: 5.4818, Time: 0.22s
2025-04-26 00:08:06,545 - INFO - Batch: 80/141, Loss: 1.6877, Perplexity: 5.4071, Time: 0.23s
2025-04-26 00:08:06,767 - INFO - Batch: 90/141, Loss: 1.6955, Perplexity: 5.4492, Time: 0.22s
2025-04-26 00:08:07,002 - INFO - Batch: 100/141, Loss: 1.7049, Perplexity: 5.5006, Time: 0.24s
2025-04-26 00:08:07,308 - INFO - Batch: 110/141, Loss: 1.7018, Perplexity: 5.4839, Time: 0.31s
2025-04-26 00:08:07,530 - INFO - Batch: 120/141, Loss: 1.7009, Perplexity: 5.4787, Time: 0.22s
2025-04-26 00:08:07,753 - INFO - Batch: 130/141, Loss: 1.6798, Perplexity: 5.3647, Time: 0.22s
2025-04-26 00:08:08,011 - INFO - Batch: 140/141, Loss: 1.7079, Perplexity: 5.5176, Time: 0.26s
2025-04-26 00:08:08,704 - INFO - Validation  Loss: 1.8761, Perplexity: 6.5283, Time: 0.66s
2025-04-26 00:08:08,704 - INFO - Epoch 291/300 - Train Loss: 1.7005, Val Loss: 1.8761
2025-04-26 00:08:08,931 - INFO - Batch: 10/141, Loss: 1.6992, Perplexity: 5.4696, Time: 0.23s
2025-04-26 00:08:09,207 - INFO - Batch: 20/141, Loss: 1.6907, Perplexity: 5.4232, Time: 0.28s
2025-04-26 00:08:09,532 - INFO - Batch: 30/141, Loss: 1.7039, Perplexity: 5.4955, Time: 0.32s
2025-04-26 00:08:09,843 - INFO - Batch: 40/141, Loss: 1.6982, Perplexity: 5.4640, Time: 0.31s
2025-04-26 00:08:10,112 - INFO - Batch: 50/141, Loss: 1.7040, Perplexity: 5.4960, Time: 0.27s
2025-04-26 00:08:10,360 - INFO - Batch: 60/141, Loss: 1.6927, Perplexity: 5.4342, Time: 0.25s
2025-04-26 00:08:10,639 - INFO - Batch: 70/141, Loss: 1.7027, Perplexity: 5.4886, Time: 0.28s
2025-04-26 00:08:11,063 - INFO - Batch: 80/141, Loss: 1.6987, Perplexity: 5.4666, Time: 0.42s
2025-04-26 00:08:11,383 - INFO - Batch: 90/141, Loss: 1.6934, Perplexity: 5.4382, Time: 0.32s
2025-04-26 00:08:11,663 - INFO - Batch: 100/141, Loss: 1.6992, Perplexity: 5.4697, Time: 0.28s
2025-04-26 00:08:11,966 - INFO - Batch: 110/141, Loss: 1.7024, Perplexity: 5.4872, Time: 0.30s
2025-04-26 00:08:12,231 - INFO - Batch: 120/141, Loss: 1.7170, Perplexity: 5.5676, Time: 0.26s
2025-04-26 00:08:12,478 - INFO - Batch: 130/141, Loss: 1.6998, Perplexity: 5.4726, Time: 0.25s
2025-04-26 00:08:12,715 - INFO - Batch: 140/141, Loss: 1.7029, Perplexity: 5.4898, Time: 0.24s
2025-04-26 00:08:13,506 - INFO - Validation  Loss: 1.8772, Perplexity: 6.5351, Time: 0.76s
2025-04-26 00:08:13,507 - INFO - Epoch 292/300 - Train Loss: 1.7005, Val Loss: 1.8772
2025-04-26 00:08:13,734 - INFO - Batch: 10/141, Loss: 1.6997, Perplexity: 5.4725, Time: 0.23s
2025-04-26 00:08:13,991 - INFO - Batch: 20/141, Loss: 1.6984, Perplexity: 5.4653, Time: 0.26s
2025-04-26 00:08:14,223 - INFO - Batch: 30/141, Loss: 1.7008, Perplexity: 5.4782, Time: 0.23s
2025-04-26 00:08:14,467 - INFO - Batch: 40/141, Loss: 1.6917, Perplexity: 5.4288, Time: 0.24s
2025-04-26 00:08:14,773 - INFO - Batch: 50/141, Loss: 1.6820, Perplexity: 5.3763, Time: 0.31s
2025-04-26 00:08:15,000 - INFO - Batch: 60/141, Loss: 1.7008, Perplexity: 5.4783, Time: 0.23s
2025-04-26 00:08:15,268 - INFO - Batch: 70/141, Loss: 1.7052, Perplexity: 5.5024, Time: 0.27s
2025-04-26 00:08:15,519 - INFO - Batch: 80/141, Loss: 1.6963, Perplexity: 5.4539, Time: 0.25s
2025-04-26 00:08:15,859 - INFO - Batch: 90/141, Loss: 1.7105, Perplexity: 5.5319, Time: 0.34s
2025-04-26 00:08:16,104 - INFO - Batch: 100/141, Loss: 1.6984, Perplexity: 5.4653, Time: 0.24s
2025-04-26 00:08:16,349 - INFO - Batch: 110/141, Loss: 1.6954, Perplexity: 5.4486, Time: 0.24s
2025-04-26 00:08:16,697 - INFO - Batch: 120/141, Loss: 1.6963, Perplexity: 5.4537, Time: 0.35s
2025-04-26 00:08:16,949 - INFO - Batch: 130/141, Loss: 1.6948, Perplexity: 5.4455, Time: 0.25s
2025-04-26 00:08:17,192 - INFO - Batch: 140/141, Loss: 1.6984, Perplexity: 5.4651, Time: 0.24s
2025-04-26 00:08:17,810 - INFO - Validation  Loss: 1.8767, Perplexity: 6.5319, Time: 0.57s
2025-04-26 00:08:17,810 - INFO - Epoch 293/300 - Train Loss: 1.7004, Val Loss: 1.8767
2025-04-26 00:08:18,098 - INFO - Batch: 10/141, Loss: 1.7028, Perplexity: 5.4894, Time: 0.29s
2025-04-26 00:08:18,316 - INFO - Batch: 20/141, Loss: 1.6940, Perplexity: 5.4412, Time: 0.22s
2025-04-26 00:08:18,576 - INFO - Batch: 30/141, Loss: 1.7186, Perplexity: 5.5770, Time: 0.26s
2025-04-26 00:08:18,801 - INFO - Batch: 40/141, Loss: 1.6952, Perplexity: 5.4480, Time: 0.22s
2025-04-26 00:08:19,027 - INFO - Batch: 50/141, Loss: 1.6988, Perplexity: 5.4671, Time: 0.23s
2025-04-26 00:08:19,259 - INFO - Batch: 60/141, Loss: 1.7024, Perplexity: 5.4872, Time: 0.23s
2025-04-26 00:08:19,490 - INFO - Batch: 70/141, Loss: 1.6945, Perplexity: 5.4441, Time: 0.23s
2025-04-26 00:08:19,721 - INFO - Batch: 80/141, Loss: 1.7150, Perplexity: 5.5569, Time: 0.23s
2025-04-26 00:08:20,007 - INFO - Batch: 90/141, Loss: 1.6970, Perplexity: 5.4576, Time: 0.29s
2025-04-26 00:08:20,229 - INFO - Batch: 100/141, Loss: 1.7094, Perplexity: 5.5258, Time: 0.22s
2025-04-26 00:08:20,448 - INFO - Batch: 110/141, Loss: 1.7161, Perplexity: 5.5625, Time: 0.22s
2025-04-26 00:08:20,671 - INFO - Batch: 120/141, Loss: 1.7061, Perplexity: 5.5077, Time: 0.22s
2025-04-26 00:08:20,892 - INFO - Batch: 130/141, Loss: 1.6966, Perplexity: 5.4551, Time: 0.22s
2025-04-26 00:08:21,122 - INFO - Batch: 140/141, Loss: 1.7196, Perplexity: 5.5825, Time: 0.23s
2025-04-26 00:08:21,766 - INFO - Validation  Loss: 1.8767, Perplexity: 6.5318, Time: 0.61s
2025-04-26 00:08:21,766 - INFO - Epoch 294/300 - Train Loss: 1.7003, Val Loss: 1.8767
2025-04-26 00:08:21,987 - INFO - Batch: 10/141, Loss: 1.7144, Perplexity: 5.5532, Time: 0.22s
2025-04-26 00:08:22,202 - INFO - Batch: 20/141, Loss: 1.6984, Perplexity: 5.4652, Time: 0.21s
2025-04-26 00:08:22,418 - INFO - Batch: 30/141, Loss: 1.6916, Perplexity: 5.4283, Time: 0.22s
2025-04-26 00:08:22,642 - INFO - Batch: 40/141, Loss: 1.6865, Perplexity: 5.4007, Time: 0.22s
2025-04-26 00:08:22,883 - INFO - Batch: 50/141, Loss: 1.6994, Perplexity: 5.4705, Time: 0.24s
2025-04-26 00:08:23,193 - INFO - Batch: 60/141, Loss: 1.7007, Perplexity: 5.4777, Time: 0.31s
2025-04-26 00:08:23,418 - INFO - Batch: 70/141, Loss: 1.6892, Perplexity: 5.4153, Time: 0.23s
2025-04-26 00:08:23,673 - INFO - Batch: 80/141, Loss: 1.7060, Perplexity: 5.5069, Time: 0.25s
2025-04-26 00:08:23,994 - INFO - Batch: 90/141, Loss: 1.7119, Perplexity: 5.5393, Time: 0.32s
2025-04-26 00:08:24,307 - INFO - Batch: 100/141, Loss: 1.6952, Perplexity: 5.4477, Time: 0.31s
2025-04-26 00:08:24,583 - INFO - Batch: 110/141, Loss: 1.6912, Perplexity: 5.4260, Time: 0.28s
2025-04-26 00:08:24,832 - INFO - Batch: 120/141, Loss: 1.6822, Perplexity: 5.3773, Time: 0.25s
2025-04-26 00:08:25,118 - INFO - Batch: 130/141, Loss: 1.7024, Perplexity: 5.4869, Time: 0.29s
2025-04-26 00:08:25,351 - INFO - Batch: 140/141, Loss: 1.7015, Perplexity: 5.4822, Time: 0.23s
2025-04-26 00:08:25,979 - INFO - Validation  Loss: 1.8767, Perplexity: 6.5316, Time: 0.60s
2025-04-26 00:08:25,979 - INFO - Epoch 295/300 - Train Loss: 1.7003, Val Loss: 1.8767
2025-04-26 00:08:26,214 - INFO - Batch: 10/141, Loss: 1.6945, Perplexity: 5.4442, Time: 0.24s
2025-04-26 00:08:26,510 - INFO - Batch: 20/141, Loss: 1.6996, Perplexity: 5.4718, Time: 0.30s
2025-04-26 00:08:26,734 - INFO - Batch: 30/141, Loss: 1.6983, Perplexity: 5.4649, Time: 0.22s
2025-04-26 00:08:26,956 - INFO - Batch: 40/141, Loss: 1.7016, Perplexity: 5.4825, Time: 0.22s
2025-04-26 00:08:27,185 - INFO - Batch: 50/141, Loss: 1.7092, Perplexity: 5.5247, Time: 0.23s
2025-04-26 00:08:27,417 - INFO - Batch: 60/141, Loss: 1.7090, Perplexity: 5.5234, Time: 0.23s
2025-04-26 00:08:27,638 - INFO - Batch: 70/141, Loss: 1.7104, Perplexity: 5.5312, Time: 0.22s
2025-04-26 00:08:27,857 - INFO - Batch: 80/141, Loss: 1.7112, Perplexity: 5.5355, Time: 0.22s
2025-04-26 00:08:28,080 - INFO - Batch: 90/141, Loss: 1.6948, Perplexity: 5.4458, Time: 0.22s
2025-04-26 00:08:28,359 - INFO - Batch: 100/141, Loss: 1.6900, Perplexity: 5.4196, Time: 0.28s
2025-04-26 00:08:28,580 - INFO - Batch: 110/141, Loss: 1.6934, Perplexity: 5.4380, Time: 0.22s
2025-04-26 00:08:28,804 - INFO - Batch: 120/141, Loss: 1.6972, Perplexity: 5.4588, Time: 0.22s
2025-04-26 00:08:29,030 - INFO - Batch: 130/141, Loss: 1.6988, Perplexity: 5.4672, Time: 0.23s
2025-04-26 00:08:29,254 - INFO - Batch: 140/141, Loss: 1.6920, Perplexity: 5.4302, Time: 0.22s
2025-04-26 00:08:29,910 - INFO - Validation  Loss: 1.8759, Perplexity: 6.5268, Time: 0.62s
2025-04-26 00:08:29,910 - INFO - Epoch 296/300 - Train Loss: 1.7002, Val Loss: 1.8759
2025-04-26 00:08:30,140 - INFO - Batch: 10/141, Loss: 1.6851, Perplexity: 5.3932, Time: 0.23s
2025-04-26 00:08:30,371 - INFO - Batch: 20/141, Loss: 1.7014, Perplexity: 5.4818, Time: 0.23s
2025-04-26 00:08:30,604 - INFO - Batch: 30/141, Loss: 1.7051, Perplexity: 5.5021, Time: 0.23s
2025-04-26 00:08:30,830 - INFO - Batch: 40/141, Loss: 1.6957, Perplexity: 5.4504, Time: 0.23s
2025-04-26 00:08:31,075 - INFO - Batch: 50/141, Loss: 1.7037, Perplexity: 5.4942, Time: 0.24s
2025-04-26 00:08:31,378 - INFO - Batch: 60/141, Loss: 1.6965, Perplexity: 5.4547, Time: 0.30s
2025-04-26 00:08:31,610 - INFO - Batch: 70/141, Loss: 1.6962, Perplexity: 5.4531, Time: 0.23s
2025-04-26 00:08:31,840 - INFO - Batch: 80/141, Loss: 1.6943, Perplexity: 5.4431, Time: 0.23s
2025-04-26 00:08:32,067 - INFO - Batch: 90/141, Loss: 1.6980, Perplexity: 5.4632, Time: 0.23s
2025-04-26 00:08:32,292 - INFO - Batch: 100/141, Loss: 1.7003, Perplexity: 5.4754, Time: 0.22s
2025-04-26 00:08:32,523 - INFO - Batch: 110/141, Loss: 1.6924, Perplexity: 5.4323, Time: 0.23s
2025-04-26 00:08:32,772 - INFO - Batch: 120/141, Loss: 1.7037, Perplexity: 5.4942, Time: 0.25s
2025-04-26 00:08:33,007 - INFO - Batch: 130/141, Loss: 1.6972, Perplexity: 5.4585, Time: 0.23s
2025-04-26 00:08:33,305 - INFO - Batch: 140/141, Loss: 1.6933, Perplexity: 5.4373, Time: 0.30s
2025-04-26 00:08:33,887 - INFO - Validation  Loss: 1.8757, Perplexity: 6.5254, Time: 0.55s
2025-04-26 00:08:33,887 - INFO - Epoch 297/300 - Train Loss: 1.7002, Val Loss: 1.8757
2025-04-26 00:08:34,109 - INFO - Batch: 10/141, Loss: 1.7072, Perplexity: 5.5132, Time: 0.22s
2025-04-26 00:08:34,334 - INFO - Batch: 20/141, Loss: 1.7013, Perplexity: 5.4810, Time: 0.22s
2025-04-26 00:08:34,620 - INFO - Batch: 30/141, Loss: 1.7002, Perplexity: 5.4751, Time: 0.29s
2025-04-26 00:08:34,843 - INFO - Batch: 40/141, Loss: 1.7022, Perplexity: 5.4861, Time: 0.22s
2025-04-26 00:08:35,065 - INFO - Batch: 50/141, Loss: 1.6977, Perplexity: 5.4612, Time: 0.22s
2025-04-26 00:08:35,291 - INFO - Batch: 60/141, Loss: 1.6993, Perplexity: 5.4701, Time: 0.23s
2025-04-26 00:08:35,557 - INFO - Batch: 70/141, Loss: 1.6986, Perplexity: 5.4660, Time: 0.27s
2025-04-26 00:08:35,799 - INFO - Batch: 80/141, Loss: 1.6986, Perplexity: 5.4662, Time: 0.24s
2025-04-26 00:08:36,119 - INFO - Batch: 90/141, Loss: 1.7202, Perplexity: 5.5858, Time: 0.32s
2025-04-26 00:08:36,411 - INFO - Batch: 100/141, Loss: 1.6929, Perplexity: 5.4350, Time: 0.29s
2025-04-26 00:08:36,759 - INFO - Batch: 110/141, Loss: 1.7065, Perplexity: 5.5096, Time: 0.35s
2025-04-26 00:08:36,987 - INFO - Batch: 120/141, Loss: 1.6913, Perplexity: 5.4265, Time: 0.23s
2025-04-26 00:08:37,222 - INFO - Batch: 130/141, Loss: 1.7137, Perplexity: 5.5494, Time: 0.24s
2025-04-26 00:08:37,493 - INFO - Batch: 140/141, Loss: 1.6876, Perplexity: 5.4066, Time: 0.27s
2025-04-26 00:08:38,180 - INFO - Validation  Loss: 1.8752, Perplexity: 6.5224, Time: 0.66s
2025-04-26 00:08:38,180 - INFO - Epoch 298/300 - Train Loss: 1.7001, Val Loss: 1.8752
2025-04-26 00:08:38,559 - INFO - Batch: 10/141, Loss: 1.6969, Perplexity: 5.4570, Time: 0.38s
2025-04-26 00:08:38,883 - INFO - Batch: 20/141, Loss: 1.7039, Perplexity: 5.4956, Time: 0.32s
2025-04-26 00:08:39,128 - INFO - Batch: 30/141, Loss: 1.6969, Perplexity: 5.4570, Time: 0.25s
2025-04-26 00:08:39,363 - INFO - Batch: 40/141, Loss: 1.7021, Perplexity: 5.4856, Time: 0.23s
2025-04-26 00:08:39,642 - INFO - Batch: 50/141, Loss: 1.6989, Perplexity: 5.4680, Time: 0.28s
2025-04-26 00:08:39,946 - INFO - Batch: 60/141, Loss: 1.6903, Perplexity: 5.4209, Time: 0.30s
2025-04-26 00:08:40,317 - INFO - Batch: 70/141, Loss: 1.6981, Perplexity: 5.4634, Time: 0.37s
2025-04-26 00:08:40,545 - INFO - Batch: 80/141, Loss: 1.7053, Perplexity: 5.5028, Time: 0.23s
2025-04-26 00:08:40,769 - INFO - Batch: 90/141, Loss: 1.7142, Perplexity: 5.5523, Time: 0.22s
2025-04-26 00:08:41,026 - INFO - Batch: 100/141, Loss: 1.6926, Perplexity: 5.4334, Time: 0.26s
2025-04-26 00:08:41,269 - INFO - Batch: 110/141, Loss: 1.7010, Perplexity: 5.4792, Time: 0.24s
2025-04-26 00:08:41,502 - INFO - Batch: 120/141, Loss: 1.7019, Perplexity: 5.4841, Time: 0.23s
2025-04-26 00:08:41,752 - INFO - Batch: 130/141, Loss: 1.6968, Perplexity: 5.4566, Time: 0.25s
2025-04-26 00:08:41,994 - INFO - Batch: 140/141, Loss: 1.6995, Perplexity: 5.4713, Time: 0.24s
2025-04-26 00:08:42,668 - INFO - Validation  Loss: 1.8756, Perplexity: 6.5248, Time: 0.64s
2025-04-26 00:08:42,668 - INFO - Epoch 299/300 - Train Loss: 1.7001, Val Loss: 1.8756
2025-04-26 00:08:42,922 - INFO - Batch: 10/141, Loss: 1.6905, Perplexity: 5.4220, Time: 0.25s
2025-04-26 00:08:43,163 - INFO - Batch: 20/141, Loss: 1.6933, Perplexity: 5.4371, Time: 0.24s
2025-04-26 00:08:43,394 - INFO - Batch: 30/141, Loss: 1.7047, Perplexity: 5.4996, Time: 0.23s
2025-04-26 00:08:43,822 - INFO - Batch: 40/141, Loss: 1.7076, Perplexity: 5.5155, Time: 0.43s
2025-04-26 00:08:44,174 - INFO - Batch: 50/141, Loss: 1.7085, Perplexity: 5.5206, Time: 0.35s
2025-04-26 00:08:44,472 - INFO - Batch: 60/141, Loss: 1.7079, Perplexity: 5.5173, Time: 0.30s
2025-04-26 00:08:44,737 - INFO - Batch: 70/141, Loss: 1.6879, Perplexity: 5.4081, Time: 0.26s
2025-04-26 00:08:44,985 - INFO - Batch: 80/141, Loss: 1.6887, Perplexity: 5.4126, Time: 0.25s
2025-04-26 00:08:45,269 - INFO - Batch: 90/141, Loss: 1.7039, Perplexity: 5.4956, Time: 0.28s
2025-04-26 00:08:45,504 - INFO - Batch: 100/141, Loss: 1.7133, Perplexity: 5.5474, Time: 0.23s
2025-04-26 00:08:45,750 - INFO - Batch: 110/141, Loss: 1.6967, Perplexity: 5.4556, Time: 0.25s
2025-04-26 00:08:46,040 - INFO - Batch: 120/141, Loss: 1.6877, Perplexity: 5.4069, Time: 0.29s
2025-04-26 00:08:46,284 - INFO - Batch: 130/141, Loss: 1.7011, Perplexity: 5.4799, Time: 0.24s
2025-04-26 00:08:46,575 - INFO - Batch: 140/141, Loss: 1.7073, Perplexity: 5.5141, Time: 0.29s
2025-04-26 00:08:47,359 - INFO - Validation  Loss: 1.8756, Perplexity: 6.5250, Time: 0.75s
2025-04-26 00:08:47,359 - INFO - Epoch 300/300 - Train Loss: 1.7001, Val Loss: 1.8756
